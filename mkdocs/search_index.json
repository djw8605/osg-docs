{
    "docs": [
        {
            "location": "/", 
            "text": "Installation Guides\n\n\nThe Open Science Grid consists primarily of a fabric of services at\nparticipating sites.\n\n\nOne of the most common ways of participating in the OSG is to install\none of our software services at your site and provide computational\npower opportunistically to the grid.\n\n\nOur most common software products include:\n\n\n\n\nHTCondor CE Installation\n: Provides a \ngateway\n\n  between the grid and your batch system.\n\n\nHTTP Proxy\n: Caches the most commonly-used files at your\n  site to preserve bandwidth (a custom packaging of the venerable \nsquid2\n software).", 
            "title": "Home"
        }, 
        {
            "location": "/#installation-guides", 
            "text": "The Open Science Grid consists primarily of a fabric of services at\nparticipating sites.  One of the most common ways of participating in the OSG is to install\none of our software services at your site and provide computational\npower opportunistically to the grid.  Our most common software products include:   HTCondor CE Installation : Provides a  gateway \n  between the grid and your batch system.  HTTP Proxy : Caches the most commonly-used files at your\n  site to preserve bandwidth (a custom packaging of the venerable  squid2  software).", 
            "title": "Installation Guides"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/", 
            "text": "current_menu: ce\n\n\nInstalling and Maintaining HTCondor-CE\n\n\nAbout This Guide\n\n\nThe HTCondor-CE software is a \njob gateway\n for an OSG Compute Element\n(CE). As such, HTCondor-CE is the entry point for jobs coming from the\nOSG \u2014 it handles authorization and delegation of jobs to your local\nbatch system. In OSG today, most CEs accept \npilot jobs\n from a factory,\nwhich in turn are able to accept and run end-user jobs.\n\n\nUse this page to learn how to install, configure, run, test, and\ntroubleshoot HTCondor-CE from the OSG software repositories.\n\n\nBefore Starting\n\n\nBefore starting the installation process, consider the following points\n(consulting \nthe Reference section below\n as needed):\n\n\n\n\nUser IDs:\n If they do not exist already, the installation will\n    create the Linux user IDs \ncondor\n (UID 4716), \ntomcat\n (UID 91) and\n    \ngratia\n (UID 42401)\n\n\nService certificate:\n The HTCondor-CE service uses a host\n    certificate at \n/etc/grid-security/host*.pem\n\n\nNetwork ports:\n The pilot factories must be able to contact your\n    HTCondor-CE service on ports 9619 and 9620 (TCP)\n\n\nHost choice:\n HTCondor-CE should be installed on a host that\n    already has the ability to submit jobs into your local cluster\n\n\n\n\nAs with all OSG software installations, there are some one-time (per\nhost) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating\n    system\n\n\nObtain root access to the host\n\n\nPrepare \nthe required Yum repositories\n\n\nInstall \nCA certificates\n\n\n\n\nInstalling HTCondor-CE\n\n\nAn HTCondor-CE installation consists of the job gateway (i.e., the\nHTCondor CE job router) and other support software (e.g., GridFTP, a\nGratia probe, authorization software). To simplify installation, OSG\nprovides convenience RPMs that install all required software with a\nsingle command.\n\n\n\n\nIf your batch system is already installed via non-RPM means and is\n   in the following list, install the appropriate 'empty' RPM. Otherwise,\n   skip to the next step.\n\n\n\n\n\n\n\n\n\n\nIf your batch system is\u2026\n\n\nThen run the following command\u2026\n\n\n\n\n\n\n\n\n\n\nHTCondor\n\n\nyum install empty-condor --enablerepo=osg-empty\n\n\n\n\n\n\nPBS\n\n\nyum install empty-torque --enablerepo=osg-empty\n\n\n\n\n\n\nSGE\n\n\nyum install empty-gridengine --enablerepo=osg-empty\n\n\n\n\n\n\n\n\n\n\nSelect the appropriate convenience RPM(s):\n\n\n\n\n\n\n\n\n\n\nIf your batch system is\u2026\n\n\nThen use the following package(s)\u2026\n\n\n\n\n\n\n\n\n\n\nHTCondor\n\n\nosg-ce-condor\n\n\n\n\n\n\nLSF\n\n\nosg-ce-lsf\n\n\n\n\n\n\nPBS\n\n\nosg-ce-pbs\n\n\n\n\n\n\nSGE\n\n\nosg-ce-sge\n\n\n\n\n\n\nSLURM\n\n\nosg-ce-slurm\n\n\n\n\n\n\n\n\n\n\nInstall the CE software: \n $yum install \nPACKAGE(S)\n\n\n\n\nTo ease the transition from GRAM\nto HTCondor-CEs, the convenience RPMs install both types of job gateway\nsoftware. By default, the HTCondor gateway is enabled and the GRAM\ngateway is disabled, which is the correct configuration for most\nHTCondor-CE-based sites (but see the gateway configuration section below\nfor more options).\n\n\nNOTE\n: HTCondor CE version 1.6 or later\nis required to send site resource information to OSG for matching jobs\nto resources.\n\n\nConfiguring HTCondor-CE\n\n\nThere are a few required configuration steps to connect HTCondor CE with\nyour batch system and authorization method. For more advanced\nconfiguration, see the section on \noptional\nconfigurations\n.\n\n\nEnabling HTCondor-CE\n\n\nIf you are installing HTCondor CE on a new host, the default\nconfiguration is correct and you can \nskip\n this step!\nHowever, if you are updating a host that used a Globus GRAM job gateway\n(aka the Globus gatekeeper), you must enable the HTCondor job gateway.\n\n\n\n\nDecide whether to disable GRAM (the preferred option) or run\n    both HTCondor and GRAM CEs\n\n\n\n\nEdit the gateway configuration file\n    \n/etc/osg/config.d/10-gateway.ini\n to reflect your choice\n    To enable HTCondor CE and disable GRAM CE:\n\n\ngram_gateway_enabled = False\nhtcondor_gateway_enabled = True\n\nTo enable both HTCondor and GRAM CEs:\n\n\ngram_gateway_enabled = True\nhtcondor_gateway_enabled = True\n\n\n\n\n\n\nMore information about the Globus GRAM CE can be found \nhere\n.\n\n\nBatch System\n\n\nConfiguring the batch system\n\n\nEnable your batch system by editing the \nenabled\n field in the\n\n/etc/osg/config.d/20-YOUR-BATCH-SYSTEM.ini\n\nfile:\n\n\nenabled = True\n\n\n\n\nBatch systems other than HTCondor\n\n\nIf you are using HTCondor as your \nlocal batch system\n (i.e., in\naddition to your HTCondor CE), skip to the \nconfiguring\nauthorization\n section. For other batch\nsystems (e.g., PBS, LSF, SGE, SLURM), keep reading.\n\n\nSharing the spool directory\n\n\nTo transfer files between the CE and the batch system, HTCondor CE\nrequires a shared file system. The current recommendation is to run a\ndedicated NFS server (whose installation is beyond the scope of this\ndocument) on the \nCE host\n. In this setup, HTCondor-CE writes to the\nlocal spool directory, the NFS server exports the it, and the NFS server\nshares the it with all of the worker nodes.\n\n\nNOTE\n: If you choose not to host the NFS\nserver on your CE, you will need to turn off root squash so that the\nHTCondor-CE daemons can write to the spool directory.\n\n\nBy default, the spool directory is \n/var/lib/condor-ce\n but you can\ncontrol this by setting \nSPOOL\n in\n\n/etc/condor-ce/config.d/99-local.conf\n. For example, the following sets\nthe \nSPOOL\n directory to \n/home/condor\n:\n\n\nSPOOL=/home/condor\n\n\n\n\nNOTE\n: The shared spool directory must\nbe readable and writeable by the \ncondor\n user for HTCondor CE to\nfunction correctly.\n\n\nDisable worker node proxy renewal\n\n\nWorker node proxy renewal is not used by HTCondor-CE and leaving it on\nwill cause some jobs to be held. Edit \n/etc/blah.config\n on the\nHTCondor CE host and set the following two values:\n\n\nblah_disable_wn_proxy_renewal=yes\nblah_delegate_renewed_proxies=no\n\n\n\n\nNOTE\n: This configuration file uses bash syntax rules; there should be no whitespace around the \n=\n.\n\n\nConfiguring authorization\n\n\nThere are two methods to manage authorization for incoming jobs,\n\nedg-mkgridmap\n and GUMS. \nedg-mkgridmap\n is easy to set up and maintain,\nand GUMS has more features and capabilities. We recommend using\n\nedg-mkgridmap\n unless you have specific needs that require the use of\nGUMS. Some examples of these specific requirements are:\n\n\n\n\nYou want to map users based on rules\n\n\nYou need to support multiple VO roles\n\n\nYou need to support gLExec for pilot jobs\n\n\n\n\nAuthorization with edg-mkgridmap\n\n\nTo configure your CE to use \nedg-mkgridmap\n:\n\n\n\n\nFollow the configuration instructions in \nthe edg-mkgridmap\n    document\n to define the VOs that your site\n    accepts\n\n\n\n\nSet some critical gridmap attributes by editing the\n    \n/etc/osg/config.d/10-misc.ini\n file on the HTCondor CE\n    host:\n\n\nauthorization_method = gridmap\n\n\n\n\n\n\nEnable edg-mkgridmap and disable GUMS in the \n/etc/lcmaps.db\n\n    file.\n\n\nIn the \nauthorize_only\n section, comment out the\n\ngumsclient\n line and uncomment the \ngridmapfile\n line. The result\nshould be as follows:\n\n\n```\nauthorize_only:\n\n\ngumsclient -\n good | bad\n\n\ngridmapfile -\n good | bad\n```\n\n\n\n\n\n\nSpecify the location of your grid mapfile in\n    \n/etc/condor-ce/config.d/01-common-auth.conf\n:\n\n\nGRIDMAP = /etc/grid-security/grid-mapfile\n\n\nNote:\n The standard location for the grid mapfile is shown\nabove. Use that location unless you have specific reasons to put the\nfile somewhere else.\n\n\n\n\n\n\nAuthorization with GUMS\n\n\n\n\nFollow the instructions in \nthe GUMS installation and\n    configuration document\n to prepare GUMS\n\n\n\n\nSet some critical GUMS attributes by editing the\n    \n/etc/osg/config.d/10-misc.ini\n file on the HTCondor CE\n    host:\n\n\nauthorization_method = xacml\ngums_host = YOUR GUMS HOSTNAME\n\n\n\n\n\n\nNOTE\n Once \ngsi-authz.conf\n is in place,\nyour local HTCondor will attempt to utilize the LCMAPS callouts if\nenabled in the \ncondor_mapfile\n. If this is not the desired behavior, set\n\nGSI_AUTHZ_CONF=/dev/null\n in the local HTCondor configuration.\n\n\nConfiguring information systems\n\n\nTo split jobs between the various sites of the OSG, information about\neach site\u2019s availability is uploaded to a central collector. The job\nfactories then query the central collector for idle resources and submit\npilot jobs to the available sites. To advertise your site, you will need\nto run the Generic Information Provider and OSG Info Services.\n\n\nGeneric Information Provider (GIP)\n\n\nThe \nGIP\n is a service that discovers information about your site\nresources like the number of available cores and what VO's are allowed\nto run on your site. Consult the \nGIP configuration\ndocument\n for instructions on\nhow to set up your \nGIP\n service.\n\n\nNOTE\n: If you have \ngip-1.3.11-4\n\ninstalled, manual intervention is required for correct reporting to\nBDII. See \n3.2.20 known\nissues\n.\n\n\nOSG Info Services\n\n\nosg-info-services\n takes the information collected from \nGIP\n and\nuploads it to OSG's central collector. For \nosg-info-services\n to\ncommunicate with the appropriate servers, it needs a service certificate\nand key located at \n/etc/grid-security/http/httpcert.pem\n and\n\n/etc/grid-security/http/httpkey.pem\n, respectively. Additionally, the\nservice runs as either the \ntomcat\n user or the account specified by the\n\nuser\n option in \n/etc/osg/config.d/30-gip.ini\n, thus your service\ncertificates need to be owned by the appropriate user.\n\n\n\n\n\n\nEnable osg-info-services in\n    \n/etc/osg/config.d/30-infoservices.ini\n:\n\n\nenabled = *True*\n\n2.  Generate a \nuser-vo-map\n file with your authorization set\nup:\ni.  If you're using edg-mkgridmap, run the following:\n\n\nedg-mkgridmap\n\nii. If you're using GUMS, run the following:\n\n\ngums-host-cron\n\n\n\n\n\n\nApplying configuration settings\n\n\nMaking changes to the OSG configuration files in the \n/etc/osg/config.d\n\ndirectory does not apply those settings to software automatically.\nSettings that are made outside of the OSG directory take effect\nimmediately or at least when the relevant service is restarted. For the\nOSG settings, use the \nosg-configure\n tool to\nvalidate (to a limited extent) and apply the settings to the relevant\nsoftware components. The \nosg-configure\n software is included\nautomatically in an HTCondor CE installation.\n\n\n\n\n\n\nMake all changes to \n.ini\n files in the \n/etc/osg/config.d\n\n    directory\n\n\nNote:\n This document describes the\ncritical settings for HTCondor CE and related software. You may need\nto configure other software that is installed on your HTCondor CE\nhost, too.\n2.  Validate the configuration settings\n\n\nosg-configure -v\n\nFix any errors (at least) that \nosg-configure\n reports.\n3.  Once the validation command succeeds without errors, apply the\nconfiguration settings:\n\n\nosg-configure -c\n\n\n\n\n\n\nOptional configuration\n\n\nThe following configuration steps are optional and will likely not be\nrequired for setting up a small site. If you do not need any of the\nfollowing special configurations, skip to \nthe section on using\nHTCondor CE\n.\n\n\n\n\nTransforming and filtering jobs\n\n\nConfiguring for multiple network interfaces\n\n\nLimiting or disabling locally running jobs on the CE\n\n\nHTCondor accounting groups\n\n\nInstalling the HTCondor-CE View\n\n\n\n\nTransforming and filtering jobs\n\n\nIf you need to modify or filter jobs, more information can be found in\nthe \nJob Router Recipes\n\ndocument.\n\n\nNOTE\n: If you need to assign jobs to\nHTCondor accounting groups, refer to \nthis\n section.\n\n\nConfiguring for multiple network interfaces\n\n\nIf you have multiple network interfaces with different hostnames, the\nHTCondor CE daemons need to know which hostname to use when\ncommunicating to each other. Generally, you will want to set\n\nNETWORK_HOSTNAME\n to the hostname of your public interface in\n\n/etc/condor-ce/config.d/99-local.conf\n directory with the line:\n\n\nNETWORK_HOSTNAME=condorce.example.com\n\n\n\n\nReplacing \ncondorce.example.com\n text with your public\ninterface\u2019s hostname.\n\n\nLimiting or disabling locally jobs running on the CE\n\n\nIf you want to limit or disable jobs running locally on your CE, you\nwill need to configure HTCondor-CE's local and scheduler universes.\nLocal and scheduler universes are HTCondor CE\u2019s analogue to GRAM\u2019s\nmanaged fork: they allow jobs to be run on the CE itself. The two\nuniverses are effectively the same (scheduler universe launches a\nstarter process for each job), so we will be configuring them in unison.\n\n\n\n\n\n\nTo change the default limit\n on the number of locally run jobs\n    (the current default is 20), add the following to\n    \n/etc/condor-ce/config.d/99-local.conf\n:\n\n\nLOCAL_JOB_LIMIT = 20\nSTART_LOCAL_UNIVERSE = TotalLocalJobsRunning + TotalSchedulerJobsRunning \n $(LOCAL_JOB_LIMIT)\nSTART_SCHEDULER_UNIVERSE = $(START_LOCAL_UNIVERSE)\n\n(updating \nLOCAL_JOB_LIMIT\n as appropriate.)\n\n\n\n\n\n\nTo only allow a specific user\n to start locally run jobs, add the\n    following to \n/etc/condor-ce/config.d/99-local.conf\n:\n\n\nALLOWED_LOCAL_USER=alice\nSTART_LOCAL_UNIVERSE = target.Owner =?= \"$(ALLOWED_LOCAL_USER)\"\nSTART_SCHEDULER_UNIVERSE = $(START_LOCAL_UNIVERSE)\n\n\n\n\n\n\nTo disable\n locally run jobs, add the following to\n    \n/etc/condor-ce/config.d/99-local.conf\n:\n\n\nSTART_LOCAL_UNIVERSE = False\nSTART_SCHEDULER_UNIVERSE = $(START_LOCAL_UNIVERSE)\n\n\n\n\n\n\nNOTE\n: RSV requires the ability to start\nlocal universe jobs so if you are using RSV, you need to allow local\nuniverse jobs from the \nrsv\n user.\n\n\nHTCondor accounting groups\n\n\nNOTE\n: For HTCondor batch systems only\n\n\nIf you want to provide fairshare on a group basis, as opposed to a Unix\nuser basis, you can use HTCondor accounting groups. They are independent\nof the Unix groups the user may already be in, and are \ndocumented in the HTCondor manual\n.\nIf you are using HTCondor accounting groups, you can map jobs from the\nCE into HTCondor accounting groups based on their numeric user id, their\nDN, or their VOMS attributes.\n\n\nMapping by UID\n\n\nTo map UID\u2019s to an accounting group, use \n/etc/osg/uid_table.txt\n. It is\nconsulted first and contains lines of the form:\n\n\nuid GroupName\n\n\n\n\nExample \nuid_table.txt\n:\n\n\nuscms02 TestGroup\nosg     other.osgedu\n\n\n\n\nMapping by DN or VOMS attribute\n\n\nTo map DN\u2019s or VOMS attributes to an accounting group, use\n\n/etc/osg/extattr_table.txt\n. This file is only consulted if the user is\nnot found in the UID file and it contains lines of the form:\n\n\nSubjectOrAttribute GroupName\n\n\n\n\nThe SubjectOrAttribute can be a Perl regular\nexpression.\n\n\nExample \nextattr_table.txt\n:\n\n\ncmsprio cms.other.prio\ncms\\/Role=production cms.prod\n.* other\n\n\n\n\nInstall and run the HTCondor-CE-View\n\n\nThe HTCondor-CE-View is an optional web interface to the status of your\nCE. To run the View,\n\n\n\n\n\n\nBegin by installing the package htcondor-ce-view:\n\n\nyum install htcondor-ce-view\n\n2.  Next, uncomment the \nDAEMON_LIST\n configuration located at\n\n/etc/condor-ce/config.d/05-ce-view.conf\n:\n\n\nDAEMON_LIST = $(DAEMON_LIST), CEVIEW, GANGLIAD\n\n3.  Restart the CE service.\n\n\nservice condor-ce restart\n\n\n\n\n\n\nBy default, the website is served from port 80. This may be configured\nin \n/etc/condor-ce/config.d/05-ce-view.conf\n as well.\n\n\nUsing HTCondor-CE\n\n\nAs a site administrator, there are a few ways in which you might use the\nHTCondor CE:\n\n\n\n\nManaging the HTCondor CE and associated services\n\n\nUsing HTCondor CE administrative tools to monitor and maintain the\n    job gateway\n\n\nUsing HTCondor CE user tools to test gateway operations\n\n\n\n\nManaging HTCondor CE and associated services\n\n\nIn addition to the HTCondor CE job gateway service itself, there are a\nnumber of supporting services in your installation. The specific\nservices are:\n\n\n\n\n\n\n\n\nSoftware\n\n\nService name\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nFetch CRL\n\n\nOn EL 6: \nfetch-crl-boot\n and \nfetch-crl-cron\n  \n   On EL 5: \nfetch-crl3-boot\n and \nfetch-crl3-cron\n\n\nSee \nCA documentation\n for more info\n\n\n\n\n\n\nGratia\n\n\ngratia-probes-cron\n\n\nAccounting software\n\n\n\n\n\n\nYour batch system\n\n\ncondor\n or \npbs_server\n or \u2026\n\n\n\n\n\n\n\n\nOSG Info Services\n\n\nosg-info-services\n\n\n\n\n\n\n\n\nHTCondor-CE\n\n\ncondor-ce\n\n\n\n\n\n\n\n\n\n\nStart the services in the order listed and stop them in reverse order.\nAs a reminder, here are common service commands (all run as \nroot\n):\n\n\n\n\n\n\n\n\nTo \u2026\n\n\nRun the command \u2026\n\n\n\n\n\n\n\n\n\n\nStart a service\n\n\nservice \nem\nSERVICE-NAME\n/em\n start\n\n\n\n\n\n\nStop a service\n\n\nservice \nem\nSERVICE-NAME\n/em\n stop\n\n\n\n\n\n\nEnable a service to start during boot\n\n\nchkconfig \nem\nSERVICE-NAME\n/em\n on\n\n\n\n\n\n\nDisable a service from starting during boot\n\n\nchkconfig \nem\nSERVICE-NAME\n/em\n off\n\n\n\n\n\n\n\n\nUsing HTCondor-CE tools\n\n\nSome of the HTCondor CE administrative and user tools are documented in\n\nthe HTCondor CE troubleshooting guide\n.\n\n\nValidating HTCondor-CE\n\n\nThere are different ways to make sure that your HTCondor CE host is\nworking well:\n\n\n\n\nPerform automated validation by running \nRSV\n\n\nManually verify your HTCondor CE using \nthe HTCondor CE\n    troubleshooting guide\n; useful tools\n    include:\n\n\ncondor_ce_run\n\n\ncondor_ce_trace\n\n\ncondor_submit\n\n\n\n\n\n\n\n\nTroubleshooting HTCondor-CE\n\n\nFor information on how to troubleshoot your HTCondor CE, please refer to\n\nthe HTCondor CE troubleshooting guide\n.\n\n\nRegistering the CE\n\n\nTo be part of the OSG Production Grid, your CE must be registered in the\n\nhttps://oim.grid.iu.edu/ OSG Information Management System\n\n(OIM). To register your resource:\n\n\n\n\nObtain, install, and verify your user\n    certificate\n (which you may have\n    done already)\n\n\nRegister your site and CE in\n    OIM\n\n\n\n\nGetting Help\n\n\nTo get assistance, please use the \nthis\npage\n.\n\n\nReference\n\n\nHere are some other HTCondor-CE documents that might be helpful:\n\n\n\n\nHTCondor-CE overview and architecture\n\n\nConfiguring HTCondor-CE job routes\n\n\nThe HTCondor-CE troubleshooting guide\n\n\nSubmitting Jobs to HTCondor-CE\n\n\n\n\nConfiguration\n\n\nThe following directories contain the configuration for HTCondor-CE. The\ndirectories are parsed in the order presented and thus configuration\nwithin the final directory will override configuration specified in the\nprevious directories.\n\n\n\n\n\n\n\n\nLocation\n\n\nComment\n\n\n\n\n\n\n\n\n\n\n/usr/share/condor-ce/config.d/\n\n\nConfiguration defaults (overwritten on package updates)\n\n\n\n\n\n\n/etc/condor-ce/config.d/\n\n\nFiles in this directory are parsed in alphanumeric order (i.e., \n99-local.conf\n will override values in \n01-ce-auth.conf\n)\n\n\n\n\n\n\n\n\nFor a detailed order of the way configuration files are parsed, run the\nfollowing command:\n\n\n# condor_ce_config_val -config\n\n\n\n\nUsers\n\n\nThe following users are needed by HTCondor-CE at all sites:\n\n\n\n\n\n\n\n\nUser\n\n\nComment\n\n\n\n\n\n\n\n\n\n\ncondor\n\n\nThe HTCondor-CE will be run as root, but perform most of its operations as the \ncondor\n user.\n\n\n\n\n\n\ngratia\n\n\nRuns the Gratia probes to collect accounting data\n\n\n\n\n\n\ntomcat\n\n\nDefault user that runs GIP\n\n\n\n\n\n\n\n\nCertificates\n\n\n\n\n\n\n\n\nCertificate\n\n\nUser that owns certificate\n\n\nPath to certificate\n\n\n\n\n\n\n\n\n\n\nHost certificate\n\n\nroot\n\n\n/etc/grid-security/hostcert.pem\n \\\nbr> \n/etc/grid-security/hostkey.pem\n\n\n\n\n\n\n\n\nFind instructions to request a host certificate \nhere\n.", 
            "title": "HTCondor-CE Install"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#current_menu-ce", 
            "text": "", 
            "title": "current_menu: ce"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#installing-and-maintaining-htcondor-ce", 
            "text": "", 
            "title": "Installing and Maintaining HTCondor-CE"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#about-this-guide", 
            "text": "The HTCondor-CE software is a  job gateway  for an OSG Compute Element\n(CE). As such, HTCondor-CE is the entry point for jobs coming from the\nOSG \u2014 it handles authorization and delegation of jobs to your local\nbatch system. In OSG today, most CEs accept  pilot jobs  from a factory,\nwhich in turn are able to accept and run end-user jobs.  Use this page to learn how to install, configure, run, test, and\ntroubleshoot HTCondor-CE from the OSG software repositories.", 
            "title": "About This Guide"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#before-starting", 
            "text": "Before starting the installation process, consider the following points\n(consulting  the Reference section below  as needed):   User IDs:  If they do not exist already, the installation will\n    create the Linux user IDs  condor  (UID 4716),  tomcat  (UID 91) and\n     gratia  (UID 42401)  Service certificate:  The HTCondor-CE service uses a host\n    certificate at  /etc/grid-security/host*.pem  Network ports:  The pilot factories must be able to contact your\n    HTCondor-CE service on ports 9619 and 9620 (TCP)  Host choice:  HTCondor-CE should be installed on a host that\n    already has the ability to submit jobs into your local cluster   As with all OSG software installations, there are some one-time (per\nhost) steps to prepare in advance:   Ensure the host has  a supported operating\n    system  Obtain root access to the host  Prepare  the required Yum repositories  Install  CA certificates", 
            "title": "Before Starting"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#installing-htcondor-ce", 
            "text": "An HTCondor-CE installation consists of the job gateway (i.e., the\nHTCondor CE job router) and other support software (e.g., GridFTP, a\nGratia probe, authorization software). To simplify installation, OSG\nprovides convenience RPMs that install all required software with a\nsingle command.   If your batch system is already installed via non-RPM means and is\n   in the following list, install the appropriate 'empty' RPM. Otherwise,\n   skip to the next step.      If your batch system is\u2026  Then run the following command\u2026      HTCondor  yum install empty-condor --enablerepo=osg-empty    PBS  yum install empty-torque --enablerepo=osg-empty    SGE  yum install empty-gridengine --enablerepo=osg-empty      Select the appropriate convenience RPM(s):      If your batch system is\u2026  Then use the following package(s)\u2026      HTCondor  osg-ce-condor    LSF  osg-ce-lsf    PBS  osg-ce-pbs    SGE  osg-ce-sge    SLURM  osg-ce-slurm      Install the CE software:   $yum install  PACKAGE(S)   To ease the transition from GRAM\nto HTCondor-CEs, the convenience RPMs install both types of job gateway\nsoftware. By default, the HTCondor gateway is enabled and the GRAM\ngateway is disabled, which is the correct configuration for most\nHTCondor-CE-based sites (but see the gateway configuration section below\nfor more options).  NOTE : HTCondor CE version 1.6 or later\nis required to send site resource information to OSG for matching jobs\nto resources.", 
            "title": "Installing HTCondor-CE"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#configuring-htcondor-ce", 
            "text": "There are a few required configuration steps to connect HTCondor CE with\nyour batch system and authorization method. For more advanced\nconfiguration, see the section on  optional\nconfigurations .  Enabling HTCondor-CE  If you are installing HTCondor CE on a new host, the default\nconfiguration is correct and you can  skip  this step!\nHowever, if you are updating a host that used a Globus GRAM job gateway\n(aka the Globus gatekeeper), you must enable the HTCondor job gateway.   Decide whether to disable GRAM (the preferred option) or run\n    both HTCondor and GRAM CEs   Edit the gateway configuration file\n     /etc/osg/config.d/10-gateway.ini  to reflect your choice\n    To enable HTCondor CE and disable GRAM CE:  gram_gateway_enabled = False\nhtcondor_gateway_enabled = True \nTo enable both HTCondor and GRAM CEs:  gram_gateway_enabled = True\nhtcondor_gateway_enabled = True    More information about the Globus GRAM CE can be found  here .", 
            "title": "Configuring HTCondor-CE"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#batch-system", 
            "text": "Configuring the batch system  Enable your batch system by editing the  enabled  field in the /etc/osg/config.d/20-YOUR-BATCH-SYSTEM.ini \nfile:  enabled = True  Batch systems other than HTCondor  If you are using HTCondor as your  local batch system  (i.e., in\naddition to your HTCondor CE), skip to the  configuring\nauthorization  section. For other batch\nsystems (e.g., PBS, LSF, SGE, SLURM), keep reading.  Sharing the spool directory  To transfer files between the CE and the batch system, HTCondor CE\nrequires a shared file system. The current recommendation is to run a\ndedicated NFS server (whose installation is beyond the scope of this\ndocument) on the  CE host . In this setup, HTCondor-CE writes to the\nlocal spool directory, the NFS server exports the it, and the NFS server\nshares the it with all of the worker nodes.  NOTE : If you choose not to host the NFS\nserver on your CE, you will need to turn off root squash so that the\nHTCondor-CE daemons can write to the spool directory.  By default, the spool directory is  /var/lib/condor-ce  but you can\ncontrol this by setting  SPOOL  in /etc/condor-ce/config.d/99-local.conf . For example, the following sets\nthe  SPOOL  directory to  /home/condor :  SPOOL=/home/condor  NOTE : The shared spool directory must\nbe readable and writeable by the  condor  user for HTCondor CE to\nfunction correctly.  Disable worker node proxy renewal  Worker node proxy renewal is not used by HTCondor-CE and leaving it on\nwill cause some jobs to be held. Edit  /etc/blah.config  on the\nHTCondor CE host and set the following two values:  blah_disable_wn_proxy_renewal=yes\nblah_delegate_renewed_proxies=no  NOTE : This configuration file uses bash syntax rules; there should be no whitespace around the  = .  Configuring authorization  There are two methods to manage authorization for incoming jobs, edg-mkgridmap  and GUMS.  edg-mkgridmap  is easy to set up and maintain,\nand GUMS has more features and capabilities. We recommend using edg-mkgridmap  unless you have specific needs that require the use of\nGUMS. Some examples of these specific requirements are:   You want to map users based on rules  You need to support multiple VO roles  You need to support gLExec for pilot jobs   Authorization with edg-mkgridmap  To configure your CE to use  edg-mkgridmap :   Follow the configuration instructions in  the edg-mkgridmap\n    document  to define the VOs that your site\n    accepts   Set some critical gridmap attributes by editing the\n     /etc/osg/config.d/10-misc.ini  file on the HTCondor CE\n    host:  authorization_method = gridmap    Enable edg-mkgridmap and disable GUMS in the  /etc/lcmaps.db \n    file.  In the  authorize_only  section, comment out the gumsclient  line and uncomment the  gridmapfile  line. The result\nshould be as follows:  ```\nauthorize_only:", 
            "title": "Batch System"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#gumsclient-good-bad", 
            "text": "gridmapfile -  good | bad\n```    Specify the location of your grid mapfile in\n     /etc/condor-ce/config.d/01-common-auth.conf :  GRIDMAP = /etc/grid-security/grid-mapfile  Note:  The standard location for the grid mapfile is shown\nabove. Use that location unless you have specific reasons to put the\nfile somewhere else.    Authorization with GUMS   Follow the instructions in  the GUMS installation and\n    configuration document  to prepare GUMS   Set some critical GUMS attributes by editing the\n     /etc/osg/config.d/10-misc.ini  file on the HTCondor CE\n    host:  authorization_method = xacml\ngums_host = YOUR GUMS HOSTNAME    NOTE  Once  gsi-authz.conf  is in place,\nyour local HTCondor will attempt to utilize the LCMAPS callouts if\nenabled in the  condor_mapfile . If this is not the desired behavior, set GSI_AUTHZ_CONF=/dev/null  in the local HTCondor configuration.  Configuring information systems  To split jobs between the various sites of the OSG, information about\neach site\u2019s availability is uploaded to a central collector. The job\nfactories then query the central collector for idle resources and submit\npilot jobs to the available sites. To advertise your site, you will need\nto run the Generic Information Provider and OSG Info Services.  Generic Information Provider (GIP)  The  GIP  is a service that discovers information about your site\nresources like the number of available cores and what VO's are allowed\nto run on your site. Consult the  GIP configuration\ndocument  for instructions on\nhow to set up your  GIP  service.  NOTE : If you have  gip-1.3.11-4 \ninstalled, manual intervention is required for correct reporting to\nBDII. See  3.2.20 known\nissues .  OSG Info Services  osg-info-services  takes the information collected from  GIP  and\nuploads it to OSG's central collector. For  osg-info-services  to\ncommunicate with the appropriate servers, it needs a service certificate\nand key located at  /etc/grid-security/http/httpcert.pem  and /etc/grid-security/http/httpkey.pem , respectively. Additionally, the\nservice runs as either the  tomcat  user or the account specified by the user  option in  /etc/osg/config.d/30-gip.ini , thus your service\ncertificates need to be owned by the appropriate user.    Enable osg-info-services in\n     /etc/osg/config.d/30-infoservices.ini :  enabled = *True* \n2.  Generate a  user-vo-map  file with your authorization set\nup:\ni.  If you're using edg-mkgridmap, run the following:  edg-mkgridmap \nii. If you're using GUMS, run the following:  gums-host-cron    Applying configuration settings  Making changes to the OSG configuration files in the  /etc/osg/config.d \ndirectory does not apply those settings to software automatically.\nSettings that are made outside of the OSG directory take effect\nimmediately or at least when the relevant service is restarted. For the\nOSG settings, use the  osg-configure  tool to\nvalidate (to a limited extent) and apply the settings to the relevant\nsoftware components. The  osg-configure  software is included\nautomatically in an HTCondor CE installation.    Make all changes to  .ini  files in the  /etc/osg/config.d \n    directory  Note:  This document describes the\ncritical settings for HTCondor CE and related software. You may need\nto configure other software that is installed on your HTCondor CE\nhost, too.\n2.  Validate the configuration settings  osg-configure -v \nFix any errors (at least) that  osg-configure  reports.\n3.  Once the validation command succeeds without errors, apply the\nconfiguration settings:  osg-configure -c    Optional configuration  The following configuration steps are optional and will likely not be\nrequired for setting up a small site. If you do not need any of the\nfollowing special configurations, skip to  the section on using\nHTCondor CE .   Transforming and filtering jobs  Configuring for multiple network interfaces  Limiting or disabling locally running jobs on the CE  HTCondor accounting groups  Installing the HTCondor-CE View   Transforming and filtering jobs  If you need to modify or filter jobs, more information can be found in\nthe  Job Router Recipes \ndocument.  NOTE : If you need to assign jobs to\nHTCondor accounting groups, refer to  this  section.  Configuring for multiple network interfaces  If you have multiple network interfaces with different hostnames, the\nHTCondor CE daemons need to know which hostname to use when\ncommunicating to each other. Generally, you will want to set NETWORK_HOSTNAME  to the hostname of your public interface in /etc/condor-ce/config.d/99-local.conf  directory with the line:  NETWORK_HOSTNAME=condorce.example.com  Replacing  condorce.example.com  text with your public\ninterface\u2019s hostname.  Limiting or disabling locally jobs running on the CE  If you want to limit or disable jobs running locally on your CE, you\nwill need to configure HTCondor-CE's local and scheduler universes.\nLocal and scheduler universes are HTCondor CE\u2019s analogue to GRAM\u2019s\nmanaged fork: they allow jobs to be run on the CE itself. The two\nuniverses are effectively the same (scheduler universe launches a\nstarter process for each job), so we will be configuring them in unison.    To change the default limit  on the number of locally run jobs\n    (the current default is 20), add the following to\n     /etc/condor-ce/config.d/99-local.conf :  LOCAL_JOB_LIMIT = 20\nSTART_LOCAL_UNIVERSE = TotalLocalJobsRunning + TotalSchedulerJobsRunning   $(LOCAL_JOB_LIMIT)\nSTART_SCHEDULER_UNIVERSE = $(START_LOCAL_UNIVERSE) \n(updating  LOCAL_JOB_LIMIT  as appropriate.)    To only allow a specific user  to start locally run jobs, add the\n    following to  /etc/condor-ce/config.d/99-local.conf :  ALLOWED_LOCAL_USER=alice\nSTART_LOCAL_UNIVERSE = target.Owner =?= \"$(ALLOWED_LOCAL_USER)\"\nSTART_SCHEDULER_UNIVERSE = $(START_LOCAL_UNIVERSE)    To disable  locally run jobs, add the following to\n     /etc/condor-ce/config.d/99-local.conf :  START_LOCAL_UNIVERSE = False\nSTART_SCHEDULER_UNIVERSE = $(START_LOCAL_UNIVERSE)    NOTE : RSV requires the ability to start\nlocal universe jobs so if you are using RSV, you need to allow local\nuniverse jobs from the  rsv  user.  HTCondor accounting groups  NOTE : For HTCondor batch systems only  If you want to provide fairshare on a group basis, as opposed to a Unix\nuser basis, you can use HTCondor accounting groups. They are independent\nof the Unix groups the user may already be in, and are  documented in the HTCondor manual .\nIf you are using HTCondor accounting groups, you can map jobs from the\nCE into HTCondor accounting groups based on their numeric user id, their\nDN, or their VOMS attributes.  Mapping by UID  To map UID\u2019s to an accounting group, use  /etc/osg/uid_table.txt . It is\nconsulted first and contains lines of the form:  uid GroupName  Example  uid_table.txt :  uscms02 TestGroup\nosg     other.osgedu  Mapping by DN or VOMS attribute  To map DN\u2019s or VOMS attributes to an accounting group, use /etc/osg/extattr_table.txt . This file is only consulted if the user is\nnot found in the UID file and it contains lines of the form:  SubjectOrAttribute GroupName  The SubjectOrAttribute can be a Perl regular\nexpression.  Example  extattr_table.txt :  cmsprio cms.other.prio\ncms\\/Role=production cms.prod\n.* other  Install and run the HTCondor-CE-View  The HTCondor-CE-View is an optional web interface to the status of your\nCE. To run the View,    Begin by installing the package htcondor-ce-view:  yum install htcondor-ce-view \n2.  Next, uncomment the  DAEMON_LIST  configuration located at /etc/condor-ce/config.d/05-ce-view.conf :  DAEMON_LIST = $(DAEMON_LIST), CEVIEW, GANGLIAD \n3.  Restart the CE service.  service condor-ce restart    By default, the website is served from port 80. This may be configured\nin  /etc/condor-ce/config.d/05-ce-view.conf  as well.", 
            "title": "gumsclient -&gt; good | bad"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#using-htcondor-ce", 
            "text": "As a site administrator, there are a few ways in which you might use the\nHTCondor CE:   Managing the HTCondor CE and associated services  Using HTCondor CE administrative tools to monitor and maintain the\n    job gateway  Using HTCondor CE user tools to test gateway operations   Managing HTCondor CE and associated services  In addition to the HTCondor CE job gateway service itself, there are a\nnumber of supporting services in your installation. The specific\nservices are:     Software  Service name  Notes      Fetch CRL  On EL 6:  fetch-crl-boot  and  fetch-crl-cron       On EL 5:  fetch-crl3-boot  and  fetch-crl3-cron  See  CA documentation  for more info    Gratia  gratia-probes-cron  Accounting software    Your batch system  condor  or  pbs_server  or \u2026     OSG Info Services  osg-info-services     HTCondor-CE  condor-ce      Start the services in the order listed and stop them in reverse order.\nAs a reminder, here are common service commands (all run as  root ):     To \u2026  Run the command \u2026      Start a service  service  em SERVICE-NAME /em  start    Stop a service  service  em SERVICE-NAME /em  stop    Enable a service to start during boot  chkconfig  em SERVICE-NAME /em  on    Disable a service from starting during boot  chkconfig  em SERVICE-NAME /em  off     Using HTCondor-CE tools  Some of the HTCondor CE administrative and user tools are documented in the HTCondor CE troubleshooting guide .", 
            "title": "Using HTCondor-CE"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#validating-htcondor-ce", 
            "text": "There are different ways to make sure that your HTCondor CE host is\nworking well:   Perform automated validation by running  RSV  Manually verify your HTCondor CE using  the HTCondor CE\n    troubleshooting guide ; useful tools\n    include:  condor_ce_run  condor_ce_trace  condor_submit", 
            "title": "Validating HTCondor-CE"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#troubleshooting-htcondor-ce", 
            "text": "For information on how to troubleshoot your HTCondor CE, please refer to the HTCondor CE troubleshooting guide .", 
            "title": "Troubleshooting HTCondor-CE"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#registering-the-ce", 
            "text": "To be part of the OSG Production Grid, your CE must be registered in the https://oim.grid.iu.edu/ OSG Information Management System \n(OIM). To register your resource:   Obtain, install, and verify your user\n    certificate  (which you may have\n    done already)  Register your site and CE in\n    OIM", 
            "title": "Registering the CE"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#getting-help", 
            "text": "To get assistance, please use the  this\npage .", 
            "title": "Getting Help"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#reference", 
            "text": "Here are some other HTCondor-CE documents that might be helpful:   HTCondor-CE overview and architecture  Configuring HTCondor-CE job routes  The HTCondor-CE troubleshooting guide  Submitting Jobs to HTCondor-CE   Configuration  The following directories contain the configuration for HTCondor-CE. The\ndirectories are parsed in the order presented and thus configuration\nwithin the final directory will override configuration specified in the\nprevious directories.     Location  Comment      /usr/share/condor-ce/config.d/  Configuration defaults (overwritten on package updates)    /etc/condor-ce/config.d/  Files in this directory are parsed in alphanumeric order (i.e.,  99-local.conf  will override values in  01-ce-auth.conf )     For a detailed order of the way configuration files are parsed, run the\nfollowing command:  # condor_ce_config_val -config  Users  The following users are needed by HTCondor-CE at all sites:     User  Comment      condor  The HTCondor-CE will be run as root, but perform most of its operations as the  condor  user.    gratia  Runs the Gratia probes to collect accounting data    tomcat  Default user that runs GIP     Certificates     Certificate  User that owns certificate  Path to certificate      Host certificate  root  /etc/grid-security/hostcert.pem  \\ br>  /etc/grid-security/hostkey.pem     Find instructions to request a host certificate  here .", 
            "title": "Reference"
        }, 
        {
            "location": "/Frontier_Squid/squid/", 
            "text": "Frontier Squid Caching Proxy Installation Guide\n\n\n\n\nAbout This Document\n\n\nThis document is intended for System Administrators who are installing\n\nfrontier-squid\n, the OSG distribution of the Frontier Squid software.\n\n\nApplicable Versions\n\n\nThe applicable software versions for this document are OSG Version \n= 3.1.40 or \n=\n3.2.16. The version of frontier-squid installed should be \n= 2.7.STABLE9-19.1\n\n\nAbout Frontier Squid\n\n\nFrontier Squid is a distribution of the well-known \nsquid HTTP caching\nproxy software\n that is optimized for use with\napplications on the Worldwide LHC Computing Grid (WLCG). It has \nmany\nadvantages\n\nover regular squid for common grid applications, especially Frontier and\nCVMFS.\n\n\nThe OSG distribution of frontier-squid is a straight rebuild of the\nupstream frontier-squid package for the convenience of OSG users.\n\n\nFrontier Squid is Recommended\n\n\nOSG recommends that all sites run a caching proxy for HTTP and HTTPS to\nhelp reduce bandwidth and improve throughput. To that end, Compute\nElement (CE) installations include Frontier Squid automatically. We\nencourage all sites to configure and use this service, as described\nbelow.\n\n\nFor large sites that expect heavy load on the proxy, it may be best to\nrun the proxy on its own host. In that case, the Frontier Squid software\nstill will be installed on the CE, but it need not be enabled. Instead,\ninstall your proxy service on the separate host and then configure the\nCE host to refer to the proxy on that host.\n\n\nThe \nosg-configure\n configuration tool (version 1.0.45 and later) warns\nusers who have not added the proxy location to their CE configuration.\nIn the future, a proxy will be required and osg-configure will fail if\nthe proxy location is not set.\n\n\nEngineering Considerations\n\n\nIf you will be supporting the Frontier application at your site, review\nthe \nupstream documentation Hardware considerations\nsection\n\nto determine how to size your equipment.\n\n\nRequirements\n\n\nHost and OS\n\n\n\n\nOS is Red Hat Enterprise Linux 5, 6, 7, and variants\n\n\nRoot access\n\n\n\n\nUsers The frontier-squid installation will create one user account\n\n\nunless it already exists.\n\n\n\n\n\n\n\n\nUser\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nsquid\n\n\nReduced privilege user that the squid process runs under. Set the default gid of the \u201csquid\u201d user to be a group that is also called \u201csquid\u201d.\n\n\n\n\n\n\n\n\nThe package can instead use another user name of your choice if you\ncreate a configuration file before installation. Details are in the\n\nupstream documentation Preparation\nsection\n.\n\n\nNetworking\n\n\n\n\n\n\n\n\nService Name\n\n\nProtocol\n\n\nPort Number\n\n\nInbound\n\n\nOutbound\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nSquid\n\n\ntcp\n\n\n3128\n\n\n\u2713\n\n\n\u2713\n\n\nAlso limited in squid ACLs. Both in and outbound must not be wide open to internet simultaneously\n\n\n\n\n\n\nSquid monitor\n\n\nudp\n\n\n3401\n\n\n\u2713\n\n\n\n\nAlso limited in squid ACLs. Should be limited to monitoring server addresses\n\n\n\n\n\n\n\n\nFirewalls\n \nDocumentation/Release3.FirewallInformation\n\n\\ \nFirewalls\n\n\nThe addresses of the WLCG monitoring servers for use in firewalls are\nlisted in the \nupstream documentation Enabling monitoring\nsection\n.\n\n\nInstall Instructions\n\n\nInstall the Yum Repositories required by OSG\n\n\nThe OSG RPMs currently support Red Hat Enterprise Linux 5, 6, 7, and variants\n\n\nOSG RPMs are distributed via the OSG yum repositories. Some packages depend on packages distributed via the EPEL repositories. So both repositories must be enabled.\n\n\nInstall EPEL\n\n\nInstall the EPEL repository, if not already present. Note: This enables EPEL by default. Choose the right version to match your OS version.\n\n\n# EPEL 5 (For RHEL 5, CentOS 5, and SL 5) \n[root@client ~]$ curl -O https://dl.fedoraproject.org/pub/epel/epel-release-latest-5.noarch.rpm\n[root@client ~]$ rpm -Uvh epel-release-latest-5.noarch.rpm\n# EPEL 6 (For RHEL 6, CentOS 6, and SL 6) \n[root@client ~]$ rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-6.noarch.rpm\n# EPEL 7 (For RHEL 7, CentOS 7, and SL 7) \n[root@client ~]$ rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\n\n\n\n\nInstall the Yum priorities package\n\n\nFor packages that exist in both OSG and EPEL repositories, it is important to prefer the OSG ones or else OSG software installs may fail. Installing the Yum priorities package enables the repository priority system to work.\n\n\n\n\nChoose the correct package name based on your operating system\u2019s major version:\n ..\n For EL 5 systems, use yum-priorities\n ..\n For EL 6 and EL 7 systems, use yum-plugin-priorities\n\n\nInstall the yum priorities package:\n\n\n\n\n[root@client ~]$ yum install PACKAGE\n\n\n\n\nReplace PACKAGE with the package name from the previous step.\n3. Ensure that \n/etc/yum.conf\n has the following line in the \n[main]\n section (particularly when using ROCKS), thereby enabling Yum plugins, including the priorities one:\n\n\nplugins=1\n\n\n\n\nNOTE\n: If you do not have a required key you can force the installation using \n--nogpgcheck\n; e.g., \n\n\nyum install --nogpgcheck yum-priorities\n\n\n\n\nInstall OSG Repositories\n\n\n\n\nIf you are upgrading from OSG 3.1 (or 3.2) to OSG 3.2 (or 3.3), remove the old OSG repository definition files and clean the Yum cache:\n\n\n\n\n[root@client ~]$ yum clean all\n[root@client ~]$ rpm -e osg-release\n\n\n\n\nThis step ensures that local changes to \n*.repo\n files will not block the installation of the new OSG repositories. After this step, \n*.repo\n files that have been changed will exist in \n/etc/yum.repos.d/\n with the \n*.rpmsave\n extension. After installing the new OSG repositories (the next step) you may want to apply any changes made in the \n*.rpmsave\n files to the new \n*.repo\n files.\n2. Install the OSG repositories using one of the following methods depending on your EL version:\n..1. For EL versions greater than EL5, install the files directly from \nrepo.grid.iu\n:\n\n\n[root@client ~]$ rpm -Uvh URL\n\n\n\n\nwhere URL is one of the following:\n\n\n\n\n\n\n\n\nSeries\n\n\nEL6 URL (for RHEL 6, CentOS 6, or SL 6)\n\n\nEL7 URL (for RHEL 7, CentOS 7, or SL 7)\n\n\n\n\n\n\n\n\n\n\nOSG 3.2\n\n\nhttps://repo.grid.iu.edu/osg/3.2/osg-3.2-el6-release-latest.rpm\n\n\nN/A\n\n\n\n\n\n\nOSG 3.3\n\n\nhttps://repo.grid.iu.edu/osg/3.3/osg-3.3-el6-release-latest.rpm\n\n\nhttps://repo.grid.iu.edu/osg/3.3/osg-3.3-el7-release-latest.rpm\n\n\n\n\n\n\n\n\nInstalling Frontier Squid\n\n\nAfter meeting the requirements in the previous section, install\nfrontier-squid with this command: \n\n\n[root@client ~]# yum install frontier-squid\n\n\n\n\nThen enable it to start at boot time with this command: \n\n\n[root@client ~]# chkconfig frontier-squid on\n\n\n\n\nConfiguring Frontier Squid\n\n\nConfiguring the Frontier Squid Service\n\n\nTo configure the Frontier Squid service itself:\n\n\n\n\nFollow the \noriginal Frontier Squid\n    documentation\n,\n    in \nthe Configuration\n    section\n\\\n   \nNote:\n An important difference between the standard Squid\n    software and the Frontier Squid variant is that Frontier Squid\n    changes are in \n/etc/squid/customize.sh\n instead of\n    \n/etc/squid/squid.conf\n.\n\n\nEnable, start, and test the service (as described below)\n\n\nEnable WLCG monitoring as described in the \nupstream documentation\n    on enabling\n    monitoring\n\n    and \nregister the squid in\n    OIM\n.\n\n\n\n\nConfiguring the OSG CE\n\n\nTo configure the OSG Compute Element (CE) to know about your Frontier\nSquid service:\n\n\n\n\n\n\nOn your CE host, edit \n/etc/osg/config.d/01-squid.ini\n\n\n\n\nMake sure that \nenabled\n is set to \nTrue\n\n\nSet \nlocation\n to the hostname and port of your Frontier Squid\n    service (e.g., \nmy.squid.host.edu:3128\n)\n\n\nLeave the other settings at \nDEFAULT\n unless you have specific\n    reasons to change them\n\n\n\n\n\n\n\n\nRun \nosg-configure\n to propagate the changes on your CE\n\nNote:\n You may want to finish other CE configuration tasks before running \nosg-configure\n. Just be sure to run it once before starting CE services.\n\n\n\n\n\n\nStarting and Stopping the Frontier Squid Service\n\n\nStarting frontier-squid:\n\n\n[root@client ~]# service frontier-squid start\n\n\n\n\nStopping frontier-squid:\n\n\n[root@client ~]# service frontier-squid stop\n\n\n\n\nTesting Frontier Squid\n\n\nAs any user on another computer, do the following (where\n%RED%yoursquid.your.domain\n is\nthe fully qualified domain name of your squid server):\n\n\n[user@client ~]$ export http_proxy=http://yoursquid.your.domain:3128\n[user@client ~]$ wget -qdO/dev/null http://frontier.cern.ch 2\n1|grep X-Cache\nX-Cache: MISS from yoursquid.your.domain\n[user@client ~]$ wget -qdO/dev/null http://frontier.cern.ch 2\n1|grep X-Cache\nX-Cache: HIT from yoursquid.your.domain\n\n\n\n\nIf the grep doesn\u2019t print anything, try removing it from the pipeline to\nsee if errors are obvious. If the second try says MISS again, something\nis probably wrong with the squid cache writes.\n\n\nIf your squid will be supporting the Frontier application, it is also\ngood to do the test in the \nupstream documentation Testing the\ninstallation\nsection\n.\n\n\nFrontier Squid Log Files\n\n\nLog file contents are explained in the \nupstream documentation Log file\ncontents\nsection\n.\n\n\nGetting Help\n\n\nTo get assistance please use \nHelp\nProcedure\n.", 
            "title": "Squid Install"
        }, 
        {
            "location": "/Frontier_Squid/squid/#frontier-squid-caching-proxy-installation-guide", 
            "text": "", 
            "title": "Frontier Squid Caching Proxy Installation Guide"
        }, 
        {
            "location": "/Frontier_Squid/squid/#about-this-document", 
            "text": "This document is intended for System Administrators who are installing frontier-squid , the OSG distribution of the Frontier Squid software.", 
            "title": "About This Document"
        }, 
        {
            "location": "/Frontier_Squid/squid/#applicable-versions", 
            "text": "The applicable software versions for this document are OSG Version  = 3.1.40 or  =\n3.2.16. The version of frontier-squid installed should be  = 2.7.STABLE9-19.1", 
            "title": "Applicable Versions"
        }, 
        {
            "location": "/Frontier_Squid/squid/#about-frontier-squid", 
            "text": "Frontier Squid is a distribution of the well-known  squid HTTP caching\nproxy software  that is optimized for use with\napplications on the Worldwide LHC Computing Grid (WLCG). It has  many\nadvantages \nover regular squid for common grid applications, especially Frontier and\nCVMFS.  The OSG distribution of frontier-squid is a straight rebuild of the\nupstream frontier-squid package for the convenience of OSG users.", 
            "title": "About Frontier Squid"
        }, 
        {
            "location": "/Frontier_Squid/squid/#frontier-squid-is-recommended", 
            "text": "OSG recommends that all sites run a caching proxy for HTTP and HTTPS to\nhelp reduce bandwidth and improve throughput. To that end, Compute\nElement (CE) installations include Frontier Squid automatically. We\nencourage all sites to configure and use this service, as described\nbelow.  For large sites that expect heavy load on the proxy, it may be best to\nrun the proxy on its own host. In that case, the Frontier Squid software\nstill will be installed on the CE, but it need not be enabled. Instead,\ninstall your proxy service on the separate host and then configure the\nCE host to refer to the proxy on that host.  The  osg-configure  configuration tool (version 1.0.45 and later) warns\nusers who have not added the proxy location to their CE configuration.\nIn the future, a proxy will be required and osg-configure will fail if\nthe proxy location is not set.", 
            "title": "Frontier Squid is Recommended"
        }, 
        {
            "location": "/Frontier_Squid/squid/#engineering-considerations", 
            "text": "If you will be supporting the Frontier application at your site, review\nthe  upstream documentation Hardware considerations\nsection \nto determine how to size your equipment.", 
            "title": "Engineering Considerations"
        }, 
        {
            "location": "/Frontier_Squid/squid/#requirements", 
            "text": "Host and OS   OS is Red Hat Enterprise Linux 5, 6, 7, and variants  Root access   Users The frontier-squid installation will create one user account  unless it already exists.     User  Comment      squid  Reduced privilege user that the squid process runs under. Set the default gid of the \u201csquid\u201d user to be a group that is also called \u201csquid\u201d.     The package can instead use another user name of your choice if you\ncreate a configuration file before installation. Details are in the upstream documentation Preparation\nsection .  Networking     Service Name  Protocol  Port Number  Inbound  Outbound  Comment      Squid  tcp  3128  \u2713  \u2713  Also limited in squid ACLs. Both in and outbound must not be wide open to internet simultaneously    Squid monitor  udp  3401  \u2713   Also limited in squid ACLs. Should be limited to monitoring server addresses     Firewalls   Documentation/Release3.FirewallInformation \n\\  Firewalls  The addresses of the WLCG monitoring servers for use in firewalls are\nlisted in the  upstream documentation Enabling monitoring\nsection .", 
            "title": "Requirements"
        }, 
        {
            "location": "/Frontier_Squid/squid/#install-instructions", 
            "text": "Install the Yum Repositories required by OSG  The OSG RPMs currently support Red Hat Enterprise Linux 5, 6, 7, and variants  OSG RPMs are distributed via the OSG yum repositories. Some packages depend on packages distributed via the EPEL repositories. So both repositories must be enabled.  Install EPEL  Install the EPEL repository, if not already present. Note: This enables EPEL by default. Choose the right version to match your OS version.  # EPEL 5 (For RHEL 5, CentOS 5, and SL 5) \n[root@client ~]$ curl -O https://dl.fedoraproject.org/pub/epel/epel-release-latest-5.noarch.rpm\n[root@client ~]$ rpm -Uvh epel-release-latest-5.noarch.rpm\n# EPEL 6 (For RHEL 6, CentOS 6, and SL 6) \n[root@client ~]$ rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-6.noarch.rpm\n# EPEL 7 (For RHEL 7, CentOS 7, and SL 7) \n[root@client ~]$ rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm  Install the Yum priorities package  For packages that exist in both OSG and EPEL repositories, it is important to prefer the OSG ones or else OSG software installs may fail. Installing the Yum priorities package enables the repository priority system to work.   Choose the correct package name based on your operating system\u2019s major version:\n ..  For EL 5 systems, use yum-priorities\n ..  For EL 6 and EL 7 systems, use yum-plugin-priorities  Install the yum priorities package:   [root@client ~]$ yum install PACKAGE  Replace PACKAGE with the package name from the previous step.\n3. Ensure that  /etc/yum.conf  has the following line in the  [main]  section (particularly when using ROCKS), thereby enabling Yum plugins, including the priorities one:  plugins=1  NOTE : If you do not have a required key you can force the installation using  --nogpgcheck ; e.g.,   yum install --nogpgcheck yum-priorities  Install OSG Repositories   If you are upgrading from OSG 3.1 (or 3.2) to OSG 3.2 (or 3.3), remove the old OSG repository definition files and clean the Yum cache:   [root@client ~]$ yum clean all\n[root@client ~]$ rpm -e osg-release  This step ensures that local changes to  *.repo  files will not block the installation of the new OSG repositories. After this step,  *.repo  files that have been changed will exist in  /etc/yum.repos.d/  with the  *.rpmsave  extension. After installing the new OSG repositories (the next step) you may want to apply any changes made in the  *.rpmsave  files to the new  *.repo  files.\n2. Install the OSG repositories using one of the following methods depending on your EL version:\n..1. For EL versions greater than EL5, install the files directly from  repo.grid.iu :  [root@client ~]$ rpm -Uvh URL  where URL is one of the following:     Series  EL6 URL (for RHEL 6, CentOS 6, or SL 6)  EL7 URL (for RHEL 7, CentOS 7, or SL 7)      OSG 3.2  https://repo.grid.iu.edu/osg/3.2/osg-3.2-el6-release-latest.rpm  N/A    OSG 3.3  https://repo.grid.iu.edu/osg/3.3/osg-3.3-el6-release-latest.rpm  https://repo.grid.iu.edu/osg/3.3/osg-3.3-el7-release-latest.rpm", 
            "title": "Install Instructions"
        }, 
        {
            "location": "/Frontier_Squid/squid/#installing-frontier-squid", 
            "text": "After meeting the requirements in the previous section, install\nfrontier-squid with this command:   [root@client ~]# yum install frontier-squid  Then enable it to start at boot time with this command:   [root@client ~]# chkconfig frontier-squid on", 
            "title": "Installing Frontier Squid"
        }, 
        {
            "location": "/Frontier_Squid/squid/#configuring-frontier-squid", 
            "text": "Configuring the Frontier Squid Service  To configure the Frontier Squid service itself:   Follow the  original Frontier Squid\n    documentation ,\n    in  the Configuration\n    section \\\n    Note:  An important difference between the standard Squid\n    software and the Frontier Squid variant is that Frontier Squid\n    changes are in  /etc/squid/customize.sh  instead of\n     /etc/squid/squid.conf .  Enable, start, and test the service (as described below)  Enable WLCG monitoring as described in the  upstream documentation\n    on enabling\n    monitoring \n    and  register the squid in\n    OIM .   Configuring the OSG CE  To configure the OSG Compute Element (CE) to know about your Frontier\nSquid service:    On your CE host, edit  /etc/osg/config.d/01-squid.ini   Make sure that  enabled  is set to  True  Set  location  to the hostname and port of your Frontier Squid\n    service (e.g.,  my.squid.host.edu:3128 )  Leave the other settings at  DEFAULT  unless you have specific\n    reasons to change them     Run  osg-configure  to propagate the changes on your CE Note:  You may want to finish other CE configuration tasks before running  osg-configure . Just be sure to run it once before starting CE services.", 
            "title": "Configuring Frontier Squid"
        }, 
        {
            "location": "/Frontier_Squid/squid/#starting-and-stopping-the-frontier-squid-service", 
            "text": "Starting frontier-squid:  [root@client ~]# service frontier-squid start  Stopping frontier-squid:  [root@client ~]# service frontier-squid stop", 
            "title": "Starting and Stopping the Frontier Squid Service"
        }, 
        {
            "location": "/Frontier_Squid/squid/#testing-frontier-squid", 
            "text": "As any user on another computer, do the following (where\n%RED%yoursquid.your.domain  is\nthe fully qualified domain name of your squid server):  [user@client ~]$ export http_proxy=http://yoursquid.your.domain:3128\n[user@client ~]$ wget -qdO/dev/null http://frontier.cern.ch 2 1|grep X-Cache\nX-Cache: MISS from yoursquid.your.domain\n[user@client ~]$ wget -qdO/dev/null http://frontier.cern.ch 2 1|grep X-Cache\nX-Cache: HIT from yoursquid.your.domain  If the grep doesn\u2019t print anything, try removing it from the pipeline to\nsee if errors are obvious. If the second try says MISS again, something\nis probably wrong with the squid cache writes.  If your squid will be supporting the Frontier application, it is also\ngood to do the test in the  upstream documentation Testing the\ninstallation\nsection .  Frontier Squid Log Files  Log file contents are explained in the  upstream documentation Log file\ncontents\nsection .", 
            "title": "Testing Frontier Squid"
        }, 
        {
            "location": "/Frontier_Squid/squid/#getting-help", 
            "text": "To get assistance please use  Help\nProcedure .", 
            "title": "Getting Help"
        }, 
        {
            "location": "/Common/yum/", 
            "text": "YUM Repositories\n\n\nAbout This Document\n\n\nThis document introduces YUM repositories and how OSG uses them.\n\n\nRepositories\n\n\nOSG hosts four public-facing repositories at\n\nrepo.grid.iu.edu\n:\n\n\n\n\nrelease\n: This repository contains software that we are willing\n    to support and can be used by the general community.\n\n\ncontrib\n: RPMs contributed from outside the OSG.\n\n\ntesting\n: This repository contains software ready for testing. If\n    you install packages from here, they may be buggy, but we will\n    provide limited assistance in providing a migration path to a fixed\n    version.\n\n\ndevelopment\n: This repository is the bleeding edge. Installing\n    from this repository may cause the host to stop functioning, and we\n    will not assist in undoing any damage.\n\n\n\n\nOSG\u2019s RPM packages rely also on external packages provided by supported\nOSes and EPEL. You must have the following repositories available and\nenabled:\n\n\n\n\nyour OS repositories (SL 5/6/7, CentOS 5/6/7, or RHEL 5/6/7\n    repositories)\n\n\nEPEL repositories\n\n\nthe OSG repositories you\u2019d like to use\n\n\n\n\nIf one of these repositories is missing you may have missing\ndependencies.\n\n\n We did not test other\nrepositories. If you use packages from other repositories, like\n\njpackage\n, \ndag\n, or \nrpmforge\n, you may encounter problems.\n\n\nEnabling Repositories\n\n\nIn \nour advice on using\nyum\n you will learn many\ntricks and tips on using yum.\n\n\nTo use the packages in a repository without adding special options to\nthe yum command the repository must be enabled.\n\n\nInstall the Yum Repositories required by OSG\n\n\nThe OSG RPMs currently support {{ supportedOs }}\n\n\nOSG RPMs are distributed via the OSG yum repositories. Some packages\ndepend on packages distributed via the\n\nEPEL\n repositories. So both\nrepositories must be enabled.\n\n\nInstall EPEL\n\n\n\n\n\n\nInstall the EPEL repository, if not already present. \nNote:\n This\n    enables EPEL by default. Choose the right version to match your OS\n    version.\n\n\n```\n\n\nEPEL 5 (For RHEL 5, CentOS 5, and SL 5)\n\n\n$ curl -O https://dl.fedoraproject.org/pub/epel/epel-release-latest-5.noarch.rpm\n$ rpm -Uvh epel-release-latest-5.noarch.rpm\n\n\nEPEL 6 (For RHEL 6, CentOS 6, and SL 6)\n\n\n$ rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-6.noarch.rpm\n\n\nEPEL 7 (For RHEL 7, CentOS 7, and SL 7)\n\n\n$ rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\n```\n\n\nWARNING\n: if you have your own mirror or configuration of the EPEL\nrepository, you \nMUST\n verify that the OSG repository has a better yum\npriority than EPEL (\ndetails\n).\nOtherwise, you will have strange dependency resolution (\ndepsolving\n) issues.\n\n\n\n\n\n\nInstall the Yum priorities package\n\n\nFor packages that exist in both OSG and EPEL repositories, it is\nimportant to prefer the OSG ones or else OSG software installs may fail.\nInstalling the Yum priorities package enables the repository priority\nsystem to work.\n\n\n\n\n\n\nChoose the correct package name based on your operating\n    system\u2019s major version:\n\n\n\n\nFor EL\u00a05 systems, use \nyum-priorities\n\n\nFor EL\u00a06 and EL\u00a07 systems, use \nyum-plugin-priorities\n\n\n\n\n\n\n\n\nInstall the Yum priorities package:\n    \nyum install *PACKAGE*\n\n    Replace \nPACKAGE\n with the package name\n    from the previous step.\n\n\n\n\n\n\nEnsure that \n/etc/yum.conf\n has the following line in the\n    \n[main]\n section (particularly when using ROCKS), thereby enabling\n    Yum plugins, including the priorities one:\n\n\nplugins=1\n\n\nNOTE\n: If you do not have a\nrequired key you can force the installation using\n\n--nogpgcheck=; e.g., =yum install --nogpgcheck yum-priorities\n.\n\n\n\n\n\n\nInstall OSG Repositories\n\n\n\n\nIf you are upgrading from OSG 3.1 (or 3.2) to OSG 3.2\n   (or 3.3), remove the old OSG repository definition files and clean the\n   Yum cache:\n\n\n\n\n# yum clean all\n   # rpm -e osg-release\n\n\nThis step ensures that local changes to \n*.repo\n files will not\n   block the installation of the new OSG repositories. After this step,\n   \n*.repo\n files that have been changed will exist in \n/etc/yum.repos.d/\n\n   with the \n*.rpmsave\n extension. After installing the new OSG\n   repositories (the next step) you may want to apply any changes made in\n   the \n*.rpmsave\n files to the new \n*.repo\n files.\n2. Install the OSG repositories using one of the following methods\ndepending on your EL version:\n    1. For EL versions greater than EL5, install the files directly from\n\nrepo.grid.iu\n:\n\n\n    ```\n    rpm -Uvh URL\n    ```\n\n    Where `URL` is one of the following:\n\n    | Series      | EL6 URL (for RHEL 6, CentOS 6, or SL 6) | EL7 URL (for RHEL 7, CentOS 7, or SL 7) |\n    |----------   | ----------------------------------------| --------------------------------------- | \n    | **OSG 3.2** | `https://repo.grid.iu.edu/osg/3.2/osg-3.2-el6-release-latest.rpm`  | N/A\n    | **OSG 3.3** | `https://repo.grid.iu.edu/osg/3.3/osg-3.3-el6-release-latest.rpm`  | `https://repo.grid.iu.edu/osg/3.3/osg-3.3-el7-release-latest.rpm` |\n\n2. For EL5, download the repo file and install it using the following:\n    ```\n    # curl -O https://repo.grid.iu.edu/osg/3.2/osg-3.2-el5-release-latest.rpm\n    # rpm -Uvh osg-3.2-el5-release-latest.rpm\n    ```\n\n\n\nPriorities\n\n\n Make sure you installed the Yum\npriorities plugin, as described above. Not doing so is a common mistake\nthat causes failed installations.\n\n\nThe only OSG repository enabled by default is the release one. If you\nwant to enable another one, such as \nosg-testing\n, then edit its file\n(e.g. \n/etc/yum.repos.d/osg-testing.repo\n) and change the enabled option\nfrom 0 to 1:\n\n\n[osg-testing]\nname=OSG Software for Enterprise Linux 5 - Testing - $basearch\n#baseurl=http://repo.grid.iu.edu/osg/3.2/el5/testing/$basearch\nmirrorlist=http://repo.grid.iu.edu/mirror/osg/3.2/el5/testing/$basearch\nfailovermethod=priority\npriority=98\nenabled=%RED%1%ENDCOLOR%\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\n\n\n\n\nIf you have your own mirror or\nconfiguration of the EPEL repository, you \nMUST\n verify that the OSG\nrepository has a better yum priority than EPEL. Otherwise, you will have\nstrange dependency resolution issues.\n\n\nReferences\n\n\n\n\nBasic use of Yum\n\n\nBest practices in using Yum", 
            "title": "Yum Repos"
        }, 
        {
            "location": "/Common/yum/#yum-repositories", 
            "text": "", 
            "title": "YUM Repositories"
        }, 
        {
            "location": "/Common/yum/#about-this-document", 
            "text": "This document introduces YUM repositories and how OSG uses them.", 
            "title": "About This Document"
        }, 
        {
            "location": "/Common/yum/#repositories", 
            "text": "OSG hosts four public-facing repositories at repo.grid.iu.edu :   release : This repository contains software that we are willing\n    to support and can be used by the general community.  contrib : RPMs contributed from outside the OSG.  testing : This repository contains software ready for testing. If\n    you install packages from here, they may be buggy, but we will\n    provide limited assistance in providing a migration path to a fixed\n    version.  development : This repository is the bleeding edge. Installing\n    from this repository may cause the host to stop functioning, and we\n    will not assist in undoing any damage.   OSG\u2019s RPM packages rely also on external packages provided by supported\nOSes and EPEL. You must have the following repositories available and\nenabled:   your OS repositories (SL 5/6/7, CentOS 5/6/7, or RHEL 5/6/7\n    repositories)  EPEL repositories  the OSG repositories you\u2019d like to use   If one of these repositories is missing you may have missing\ndependencies.   We did not test other\nrepositories. If you use packages from other repositories, like jpackage ,  dag , or  rpmforge , you may encounter problems.", 
            "title": "Repositories"
        }, 
        {
            "location": "/Common/yum/#enabling-repositories", 
            "text": "In  our advice on using\nyum  you will learn many\ntricks and tips on using yum.  To use the packages in a repository without adding special options to\nthe yum command the repository must be enabled.  Install the Yum Repositories required by OSG  The OSG RPMs currently support {{ supportedOs }}  OSG RPMs are distributed via the OSG yum repositories. Some packages\ndepend on packages distributed via the EPEL  repositories. So both\nrepositories must be enabled.  Install EPEL    Install the EPEL repository, if not already present.  Note:  This\n    enables EPEL by default. Choose the right version to match your OS\n    version.  ```", 
            "title": "Enabling Repositories"
        }, 
        {
            "location": "/Common/yum/#epel-5-for-rhel-5-centos-5-and-sl-5", 
            "text": "$ curl -O https://dl.fedoraproject.org/pub/epel/epel-release-latest-5.noarch.rpm\n$ rpm -Uvh epel-release-latest-5.noarch.rpm", 
            "title": "EPEL 5 (For RHEL 5, CentOS 5, and SL 5)"
        }, 
        {
            "location": "/Common/yum/#epel-6-for-rhel-6-centos-6-and-sl-6", 
            "text": "$ rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-6.noarch.rpm", 
            "title": "EPEL 6 (For RHEL 6, CentOS 6, and SL 6)"
        }, 
        {
            "location": "/Common/yum/#epel-7-for-rhel-7-centos-7-and-sl-7", 
            "text": "$ rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\n```  WARNING : if you have your own mirror or configuration of the EPEL\nrepository, you  MUST  verify that the OSG repository has a better yum\npriority than EPEL ( details ).\nOtherwise, you will have strange dependency resolution ( depsolving ) issues.    Install the Yum priorities package  For packages that exist in both OSG and EPEL repositories, it is\nimportant to prefer the OSG ones or else OSG software installs may fail.\nInstalling the Yum priorities package enables the repository priority\nsystem to work.    Choose the correct package name based on your operating\n    system\u2019s major version:   For EL\u00a05 systems, use  yum-priorities  For EL\u00a06 and EL\u00a07 systems, use  yum-plugin-priorities     Install the Yum priorities package:\n     yum install *PACKAGE* \n    Replace  PACKAGE  with the package name\n    from the previous step.    Ensure that  /etc/yum.conf  has the following line in the\n     [main]  section (particularly when using ROCKS), thereby enabling\n    Yum plugins, including the priorities one:  plugins=1  NOTE : If you do not have a\nrequired key you can force the installation using --nogpgcheck=; e.g., =yum install --nogpgcheck yum-priorities .    Install OSG Repositories   If you are upgrading from OSG 3.1 (or 3.2) to OSG 3.2\n   (or 3.3), remove the old OSG repository definition files and clean the\n   Yum cache:   # yum clean all\n   # rpm -e osg-release  This step ensures that local changes to  *.repo  files will not\n   block the installation of the new OSG repositories. After this step,\n    *.repo  files that have been changed will exist in  /etc/yum.repos.d/ \n   with the  *.rpmsave  extension. After installing the new OSG\n   repositories (the next step) you may want to apply any changes made in\n   the  *.rpmsave  files to the new  *.repo  files.\n2. Install the OSG repositories using one of the following methods\ndepending on your EL version:\n    1. For EL versions greater than EL5, install the files directly from repo.grid.iu :      ```\n    rpm -Uvh URL\n    ```\n\n    Where `URL` is one of the following:\n\n    | Series      | EL6 URL (for RHEL 6, CentOS 6, or SL 6) | EL7 URL (for RHEL 7, CentOS 7, or SL 7) |\n    |----------   | ----------------------------------------| --------------------------------------- | \n    | **OSG 3.2** | `https://repo.grid.iu.edu/osg/3.2/osg-3.2-el6-release-latest.rpm`  | N/A\n    | **OSG 3.3** | `https://repo.grid.iu.edu/osg/3.3/osg-3.3-el6-release-latest.rpm`  | `https://repo.grid.iu.edu/osg/3.3/osg-3.3-el7-release-latest.rpm` |\n\n2. For EL5, download the repo file and install it using the following:\n    ```\n    # curl -O https://repo.grid.iu.edu/osg/3.2/osg-3.2-el5-release-latest.rpm\n    # rpm -Uvh osg-3.2-el5-release-latest.rpm\n    ```", 
            "title": "EPEL 7 (For RHEL 7, CentOS 7, and SL 7)"
        }, 
        {
            "location": "/Common/yum/#priorities", 
            "text": "Make sure you installed the Yum\npriorities plugin, as described above. Not doing so is a common mistake\nthat causes failed installations.  The only OSG repository enabled by default is the release one. If you\nwant to enable another one, such as  osg-testing , then edit its file\n(e.g.  /etc/yum.repos.d/osg-testing.repo ) and change the enabled option\nfrom 0 to 1:  [osg-testing]\nname=OSG Software for Enterprise Linux 5 - Testing - $basearch\n#baseurl=http://repo.grid.iu.edu/osg/3.2/el5/testing/$basearch\nmirrorlist=http://repo.grid.iu.edu/mirror/osg/3.2/el5/testing/$basearch\nfailovermethod=priority\npriority=98\nenabled=%RED%1%ENDCOLOR%\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG  If you have your own mirror or\nconfiguration of the EPEL repository, you  MUST  verify that the OSG\nrepository has a better yum priority than EPEL. Otherwise, you will have\nstrange dependency resolution issues.", 
            "title": "Priorities"
        }, 
        {
            "location": "/Common/yum/#references", 
            "text": "Basic use of Yum  Best practices in using Yum", 
            "title": "References"
        }
    ]
}