{
    "docs": [
        {
            "location": "/", 
            "text": "Installation Guides\n\n\nThe Open Science Grid consists primarily of a fabric of services at\nparticipating sites.\n\n\nOne of the most common ways of participating in the OSG is to install\none of our software services at your site and provide computational\npower opportunistically to the grid.\n\n\nOur most common software products include:\n\n\n\n\nHTCondor CE Installation\n: Provides a \ngateway\n\n  between the grid and your batch system.\n\n\nHTTP Proxy\n: Caches the most commonly-used files at your\n  site to preserve bandwidth (a custom packaging of the venerable \nsquid2\n software).\n\n\nCVMFS\n The CernVM File System (CVMFS) is a global-scale, read-only,\n  hierarchical filesystem.  CVMFS volumes distribute the majority of the scientific\n  software used on OSG in addition to the OSG worker node client.\n\n\nWorker node client\n and \nglexec\n: An RPM-based install\n  of the worker node software; this includes the \nglexec\n binary (not provided by the\n  CVMFS install) which allows pilots to securely isolate payload jobs.", 
            "title": "Home"
        }, 
        {
            "location": "/#installation-guides", 
            "text": "The Open Science Grid consists primarily of a fabric of services at\nparticipating sites.  One of the most common ways of participating in the OSG is to install\none of our software services at your site and provide computational\npower opportunistically to the grid.  Our most common software products include:   HTCondor CE Installation : Provides a  gateway \n  between the grid and your batch system.  HTTP Proxy : Caches the most commonly-used files at your\n  site to preserve bandwidth (a custom packaging of the venerable  squid2  software).  CVMFS  The CernVM File System (CVMFS) is a global-scale, read-only,\n  hierarchical filesystem.  CVMFS volumes distribute the majority of the scientific\n  software used on OSG in addition to the OSG worker node client.  Worker node client  and  glexec : An RPM-based install\n  of the worker node software; this includes the  glexec  binary (not provided by the\n  CVMFS install) which allows pilots to securely isolate payload jobs.", 
            "title": "Installation Guides"
        }, 
        {
            "location": "/Common/yum/", 
            "text": "YUM Repositories\n\n\nAbout This Document\n\n\nThis document introduces YUM repositories and how OSG uses them.\n\n\nRepositories\n\n\nOSG hosts four public-facing repositories at\n\nrepo.grid.iu.edu\n:\n\n\n\n\nrelease\n: This repository contains software that we are willing\n    to support and can be used by the general community.\n\n\ncontrib\n: RPMs contributed from outside the OSG.\n\n\ntesting\n: This repository contains software ready for testing. If\n    you install packages from here, they may be buggy, but we will\n    provide limited assistance in providing a migration path to a fixed\n    version.\n\n\ndevelopment\n: This repository is the bleeding edge. Installing\n    from this repository may cause the host to stop functioning, and we\n    will not assist in undoing any damage.\n\n\n\n\nOSG\u2019s RPM packages rely also on external packages provided by supported\nOSes and EPEL. You must have the following repositories available and\nenabled:\n\n\n\n\nyour OS repositories (SL 5/6/7, CentOS 5/6/7, or RHEL 5/6/7\n    repositories)\n\n\nEPEL repositories\n\n\nthe OSG repositories you\u2019d like to use\n\n\n\n\nIf one of these repositories is missing you may have missing\ndependencies.\n\n\n We did not test other\nrepositories. If you use packages from other repositories, like\n\njpackage\n, \ndag\n, or \nrpmforge\n, you may encounter problems.\n\n\nEnabling Repositories\n\n\nIn \nour advice on using\nyum\n you will learn many\ntricks and tips on using yum.\n\n\nTo use the packages in a repository without adding special options to\nthe yum command the repository must be enabled.\n\n\nInstall the Yum Repositories required by OSG\n\n\nThe OSG RPMs currently support {{ supportedOs }}\n\n\nOSG RPMs are distributed via the OSG yum repositories. Some packages\ndepend on packages distributed via the\n\nEPEL\n repositories. So both\nrepositories must be enabled.\n\n\nInstall EPEL\n\n\nInstall the EPEL repository, if not already present. \nNote:\n This\nenables EPEL by default. Choose the right version to match your OS\nversion.\n\n\n# EPEL 5 (For RHEL 5, CentOS 5, and SL 5)\n$ curl -O https://dl.fedoraproject.org/pub/epel/epel-release-latest-5.noarch.rpm\n$ rpm -Uvh epel-release-latest-5.noarch.rpm\n# EPEL 6 (For RHEL 6, CentOS 6, and SL 6)\n$ rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-6.noarch.rpm\n# EPEL 7 (For RHEL 7, CentOS 7, and SL 7)\n$ rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\n\n\n\n\nWARNING\n: if you have your own mirror or configuration of the EPEL\nrepository, you \nMUST\n verify that the OSG repository has a better yum\npriority than EPEL (\ndetails\n).\nOtherwise, you will have strange dependency resolution (\ndepsolving\n) issues.\n\n\nInstall the Yum priorities package\n\n\nFor packages that exist in both OSG and EPEL repositories, it is\nimportant to prefer the OSG ones or else OSG software installs may fail.\nInstalling the Yum priorities package enables the repository priority\nsystem to work.\n\n\n\n\n\n\nChoose the correct package name based on your operating\n    system\u2019s major version:\n\n\n\n\nFor EL\u00a05 systems, use \nyum-priorities\n\n\nFor EL\u00a06 and EL\u00a07 systems, use \nyum-plugin-priorities\n\n\n\n\n\n\n\n\nInstall the Yum priorities package:\n    \nyum install *PACKAGE*\n\n    Replace \nPACKAGE\n with the package name\n    from the previous step.\n\n\n\n\n\n\nEnsure that \n/etc/yum.conf\n has the following line in the\n    \n[main]\n section (particularly when using ROCKS), thereby enabling\n    Yum plugins, including the priorities one:\n\n\nplugins=1\n\n\nNOTE\n: If you do not have a\nrequired key you can force the installation using\n\n--nogpgcheck=; e.g., =yum install --nogpgcheck yum-priorities\n.\n\n\n\n\n\n\nInstall OSG Repositories\n\n\n\n\nIf you are upgrading from OSG 3.1 (or 3.2) to OSG 3.2\n   (or 3.3), remove the old OSG repository definition files and clean the\n   Yum cache:\n\n\n\n\n# yum clean all\n   # rpm -e osg-release\n\n\nThis step ensures that local changes to \n*.repo\n files will not\n   block the installation of the new OSG repositories. After this step,\n   \n*.repo\n files that have been changed will exist in \n/etc/yum.repos.d/\n\n   with the \n*.rpmsave\n extension. After installing the new OSG\n   repositories (the next step) you may want to apply any changes made in\n   the \n*.rpmsave\n files to the new \n*.repo\n files.\n2. Install the OSG repositories using one of the following methods\ndepending on your EL version:\n    1. For EL versions greater than EL5, install the files directly from\n\nrepo.grid.iu\n:\n\n\n    ```\n    rpm -Uvh URL\n    ```\n\n    Where `URL` is one of the following:\n\n    | Series      | EL6 URL (for RHEL 6, CentOS 6, or SL 6) | EL7 URL (for RHEL 7, CentOS 7, or SL 7) |\n    |----------   | ----------------------------------------| --------------------------------------- | \n    | **OSG 3.2** | `https://repo.grid.iu.edu/osg/3.2/osg-3.2-el6-release-latest.rpm`  | N/A\n    | **OSG 3.3** | `https://repo.grid.iu.edu/osg/3.3/osg-3.3-el6-release-latest.rpm`  | `https://repo.grid.iu.edu/osg/3.3/osg-3.3-el7-release-latest.rpm` |\n\n2. For EL5, download the repo file and install it using the following:\n    ```\n    # curl -O https://repo.grid.iu.edu/osg/3.2/osg-3.2-el5-release-latest.rpm\n    # rpm -Uvh osg-3.2-el5-release-latest.rpm\n    ```\n\n\n\nPriorities\n\n\n Make sure you installed the Yum\npriorities plugin, as described above. Not doing so is a common mistake\nthat causes failed installations.\n\n\nThe only OSG repository enabled by default is the release one. If you\nwant to enable another one, such as \nosg-testing\n, then edit its file\n(e.g. \n/etc/yum.repos.d/osg-testing.repo\n) and change the enabled option\nfrom 0 to 1:\n\n\n[osg-testing]\nname=OSG Software for Enterprise Linux 5 - Testing - $basearch\n#baseurl=http://repo.grid.iu.edu/osg/3.2/el5/testing/$basearch\nmirrorlist=http://repo.grid.iu.edu/mirror/osg/3.2/el5/testing/$basearch\nfailovermethod=priority\npriority=98\nenabled=%RED%1%ENDCOLOR%\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\n\n\n\n\nIf you have your own mirror or\nconfiguration of the EPEL repository, you \nMUST\n verify that the OSG\nrepository has a better yum priority than EPEL. Otherwise, you will have\nstrange dependency resolution issues.\n\n\nReferences\n\n\n\n\nBasic use of Yum\n\n\nBest practices in using Yum", 
            "title": "Yum Repos"
        }, 
        {
            "location": "/Common/yum/#yum-repositories", 
            "text": "", 
            "title": "YUM Repositories"
        }, 
        {
            "location": "/Common/yum/#about-this-document", 
            "text": "This document introduces YUM repositories and how OSG uses them.", 
            "title": "About This Document"
        }, 
        {
            "location": "/Common/yum/#repositories", 
            "text": "OSG hosts four public-facing repositories at repo.grid.iu.edu :   release : This repository contains software that we are willing\n    to support and can be used by the general community.  contrib : RPMs contributed from outside the OSG.  testing : This repository contains software ready for testing. If\n    you install packages from here, they may be buggy, but we will\n    provide limited assistance in providing a migration path to a fixed\n    version.  development : This repository is the bleeding edge. Installing\n    from this repository may cause the host to stop functioning, and we\n    will not assist in undoing any damage.   OSG\u2019s RPM packages rely also on external packages provided by supported\nOSes and EPEL. You must have the following repositories available and\nenabled:   your OS repositories (SL 5/6/7, CentOS 5/6/7, or RHEL 5/6/7\n    repositories)  EPEL repositories  the OSG repositories you\u2019d like to use   If one of these repositories is missing you may have missing\ndependencies.   We did not test other\nrepositories. If you use packages from other repositories, like jpackage ,  dag , or  rpmforge , you may encounter problems.", 
            "title": "Repositories"
        }, 
        {
            "location": "/Common/yum/#enabling-repositories", 
            "text": "In  our advice on using\nyum  you will learn many\ntricks and tips on using yum.  To use the packages in a repository without adding special options to\nthe yum command the repository must be enabled.", 
            "title": "Enabling Repositories"
        }, 
        {
            "location": "/Common/yum/#install-the-yum-repositories-required-by-osg", 
            "text": "The OSG RPMs currently support {{ supportedOs }}  OSG RPMs are distributed via the OSG yum repositories. Some packages\ndepend on packages distributed via the EPEL  repositories. So both\nrepositories must be enabled.", 
            "title": "Install the Yum Repositories required by OSG"
        }, 
        {
            "location": "/Common/yum/#install-epel", 
            "text": "Install the EPEL repository, if not already present.  Note:  This\nenables EPEL by default. Choose the right version to match your OS\nversion.  # EPEL 5 (For RHEL 5, CentOS 5, and SL 5)\n$ curl -O https://dl.fedoraproject.org/pub/epel/epel-release-latest-5.noarch.rpm\n$ rpm -Uvh epel-release-latest-5.noarch.rpm\n# EPEL 6 (For RHEL 6, CentOS 6, and SL 6)\n$ rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-6.noarch.rpm\n# EPEL 7 (For RHEL 7, CentOS 7, and SL 7)\n$ rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm  WARNING : if you have your own mirror or configuration of the EPEL\nrepository, you  MUST  verify that the OSG repository has a better yum\npriority than EPEL ( details ).\nOtherwise, you will have strange dependency resolution ( depsolving ) issues.", 
            "title": "Install EPEL"
        }, 
        {
            "location": "/Common/yum/#install-the-yum-priorities-package", 
            "text": "For packages that exist in both OSG and EPEL repositories, it is\nimportant to prefer the OSG ones or else OSG software installs may fail.\nInstalling the Yum priorities package enables the repository priority\nsystem to work.    Choose the correct package name based on your operating\n    system\u2019s major version:   For EL\u00a05 systems, use  yum-priorities  For EL\u00a06 and EL\u00a07 systems, use  yum-plugin-priorities     Install the Yum priorities package:\n     yum install *PACKAGE* \n    Replace  PACKAGE  with the package name\n    from the previous step.    Ensure that  /etc/yum.conf  has the following line in the\n     [main]  section (particularly when using ROCKS), thereby enabling\n    Yum plugins, including the priorities one:  plugins=1  NOTE : If you do not have a\nrequired key you can force the installation using --nogpgcheck=; e.g., =yum install --nogpgcheck yum-priorities .", 
            "title": "Install the Yum priorities package"
        }, 
        {
            "location": "/Common/yum/#install-osg-repositories", 
            "text": "If you are upgrading from OSG 3.1 (or 3.2) to OSG 3.2\n   (or 3.3), remove the old OSG repository definition files and clean the\n   Yum cache:   # yum clean all\n   # rpm -e osg-release  This step ensures that local changes to  *.repo  files will not\n   block the installation of the new OSG repositories. After this step,\n    *.repo  files that have been changed will exist in  /etc/yum.repos.d/ \n   with the  *.rpmsave  extension. After installing the new OSG\n   repositories (the next step) you may want to apply any changes made in\n   the  *.rpmsave  files to the new  *.repo  files.\n2. Install the OSG repositories using one of the following methods\ndepending on your EL version:\n    1. For EL versions greater than EL5, install the files directly from repo.grid.iu :      ```\n    rpm -Uvh URL\n    ```\n\n    Where `URL` is one of the following:\n\n    | Series      | EL6 URL (for RHEL 6, CentOS 6, or SL 6) | EL7 URL (for RHEL 7, CentOS 7, or SL 7) |\n    |----------   | ----------------------------------------| --------------------------------------- | \n    | **OSG 3.2** | `https://repo.grid.iu.edu/osg/3.2/osg-3.2-el6-release-latest.rpm`  | N/A\n    | **OSG 3.3** | `https://repo.grid.iu.edu/osg/3.3/osg-3.3-el6-release-latest.rpm`  | `https://repo.grid.iu.edu/osg/3.3/osg-3.3-el7-release-latest.rpm` |\n\n2. For EL5, download the repo file and install it using the following:\n    ```\n    # curl -O https://repo.grid.iu.edu/osg/3.2/osg-3.2-el5-release-latest.rpm\n    # rpm -Uvh osg-3.2-el5-release-latest.rpm\n    ```", 
            "title": "Install OSG Repositories"
        }, 
        {
            "location": "/Common/yum/#priorities", 
            "text": "Make sure you installed the Yum\npriorities plugin, as described above. Not doing so is a common mistake\nthat causes failed installations.  The only OSG repository enabled by default is the release one. If you\nwant to enable another one, such as  osg-testing , then edit its file\n(e.g.  /etc/yum.repos.d/osg-testing.repo ) and change the enabled option\nfrom 0 to 1:  [osg-testing]\nname=OSG Software for Enterprise Linux 5 - Testing - $basearch\n#baseurl=http://repo.grid.iu.edu/osg/3.2/el5/testing/$basearch\nmirrorlist=http://repo.grid.iu.edu/mirror/osg/3.2/el5/testing/$basearch\nfailovermethod=priority\npriority=98\nenabled=%RED%1%ENDCOLOR%\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG  If you have your own mirror or\nconfiguration of the EPEL repository, you  MUST  verify that the OSG\nrepository has a better yum priority than EPEL. Otherwise, you will have\nstrange dependency resolution issues.", 
            "title": "Priorities"
        }, 
        {
            "location": "/Common/yum/#references", 
            "text": "Basic use of Yum  Best practices in using Yum", 
            "title": "References"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/", 
            "text": "Installing and Maintaining HTCondor-CE\n\n\nAbout This Guide\n\n\nThe HTCondor-CE software is a \njob gateway\n for an OSG Compute Element\n(CE). As such, HTCondor-CE is the entry point for jobs coming from the\nOSG \u2014 it handles authorization and delegation of jobs to your local\nbatch system. In OSG today, most CEs accept \npilot jobs\n from a factory,\nwhich in turn are able to accept and run end-user jobs.\n\n\nUse this page to learn how to install, configure, run, test, and\ntroubleshoot HTCondor-CE from the OSG software repositories.\n\n\nBefore Starting\n\n\nBefore starting the installation process, consider the following points\n(consulting \nthe Reference section below\n as needed):\n\n\n\n\nUser IDs:\n If they do not exist already, the installation will\n    create the Linux user IDs \ncondor\n (UID 4716), \ntomcat\n (UID 91) and\n    \ngratia\n (UID 42401)\n\n\nService certificate:\n The HTCondor-CE service uses a host\n    certificate at \n/etc/grid-security/host*.pem\n\n\nNetwork ports:\n The pilot factories must be able to contact your\n    HTCondor-CE service on ports 9619 and 9620 (TCP)\n\n\nHost choice:\n HTCondor-CE should be installed on a host that\n    already has the ability to submit jobs into your local cluster\n\n\n\n\nAs with all OSG software installations, there are some one-time (per\nhost) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating\n    system\n\n\nObtain root access to the host\n\n\nPrepare \nthe required Yum repositories\n\n\nInstall \nCA certificates\n\n\n\n\nInstalling HTCondor-CE\n\n\nAn HTCondor-CE installation consists of the job gateway (i.e., the\nHTCondor CE job router) and other support software (e.g., GridFTP, a\nGratia probe, authorization software). To simplify installation, OSG\nprovides convenience RPMs that install all required software with a\nsingle command.\n\n\n\n\nIf your batch system is already installed via non-RPM means and is\n   in the following list, install the appropriate 'empty' RPM. Otherwise,\n   skip to the next step.\n\n\n\n\n\n\n\n\n\n\nIf your batch system is\u2026\n\n\nThen run the following command\u2026\n\n\n\n\n\n\n\n\n\n\nHTCondor\n\n\nyum install empty-condor --enablerepo=osg-empty\n\n\n\n\n\n\nPBS\n\n\nyum install empty-torque --enablerepo=osg-empty\n\n\n\n\n\n\nSGE\n\n\nyum install empty-gridengine --enablerepo=osg-empty\n\n\n\n\n\n\n\n\n\n\nSelect the appropriate convenience RPM(s):\n\n\n\n\n\n\n\n\n\n\nIf your batch system is\u2026\n\n\nThen use the following package(s)\u2026\n\n\n\n\n\n\n\n\n\n\nHTCondor\n\n\nosg-ce-condor\n\n\n\n\n\n\nLSF\n\n\nosg-ce-lsf\n\n\n\n\n\n\nPBS\n\n\nosg-ce-pbs\n\n\n\n\n\n\nSGE\n\n\nosg-ce-sge\n\n\n\n\n\n\nSLURM\n\n\nosg-ce-slurm\n\n\n\n\n\n\n\n\n\n\nInstall the CE software:\n\n\n\n\nyum install *PACKAGE(S)*\n\n\nTo ease the transition from GRAM\nto HTCondor-CEs, the convenience RPMs install both types of job gateway\nsoftware. By default, the HTCondor gateway is enabled and the GRAM\ngateway is disabled, which is the correct configuration for most\nHTCondor-CE-based sites (but see the gateway configuration section below\nfor more options).\n\n\n\n\nNote\n\n\nHTCondor CE version 1.6 or later\nis required to send site resource information to OSG for matching jobs\nto resources.\n\n\n\n\nConfiguring HTCondor-CE\n\n\nThere are a few required configuration steps to connect HTCondor CE with\nyour batch system and authorization method. For more advanced\nconfiguration, see the section on \noptional\nconfigurations\n.\n\n\nEnabling HTCondor-CE\n\n\nIf you are installing HTCondor CE on a new host, the default\nconfiguration is correct and you can \nskip\n this step!\nHowever, if you are updating a host that used a Globus GRAM job gateway\n(aka the Globus gatekeeper), you must enable the HTCondor job gateway.\n\n\n\n\nDecide whether to disable GRAM (the preferred option) or run\n    both HTCondor and GRAM CEs\n\n\n\n\nEdit the gateway configuration file\n    \n/etc/osg/config.d/10-gateway.ini\n to reflect your choice\n    To enable HTCondor CE and disable GRAM CE:\n\n\ngram_gateway_enabled = False\nhtcondor_gateway_enabled = True\n\n\nTo enable both HTCondor and GRAM CEs:\n\n\ngram_gateway_enabled = True\nhtcondor_gateway_enabled = True\n\n\n\n\n\n\nMore information about the Globus GRAM CE can be found \nhere\n.\n\n\nBatch System\n\n\nConfiguring the batch system\n\n\nEnable your batch system by editing the \nenabled\n field in the\n\n/etc/osg/config.d/20-YOUR-BATCH-SYSTEM.ini\n\nfile:\n\n\nenabled = True\n\n\n\n\nBatch systems other than HTCondor\n\n\nIf you are using HTCondor as your \nlocal batch system\n (i.e., in\naddition to your HTCondor CE), skip to the \nconfiguring\nauthorization\n section. For other batch\nsystems (e.g., PBS, LSF, SGE, SLURM), keep reading.\n\n\nSharing the spool directory\n\n\nTo transfer files between the CE and the batch system, HTCondor CE\nrequires a shared file system. The current recommendation is to run a\ndedicated NFS server (whose installation is beyond the scope of this\ndocument) on the \nCE host\n. In this setup, HTCondor-CE writes to the\nlocal spool directory, the NFS server exports the it, and the NFS server\nshares the it with all of the worker nodes.\n\n\nNOTE\n: If you choose not to host the NFS\nserver on your CE, you will need to turn off root squash so that the\nHTCondor-CE daemons can write to the spool directory.\n\n\nBy default, the spool directory is \n/var/lib/condor-ce\n but you can\ncontrol this by setting \nSPOOL\n in\n\n/etc/condor-ce/config.d/99-local.conf\n. For example, the following sets\nthe \nSPOOL\n directory to \n/home/condor\n:\n\n\nSPOOL=/home/condor\n\n\n\n\nNOTE\n: The shared spool directory must\nbe readable and writeable by the \ncondor\n user for HTCondor CE to\nfunction correctly.\n\n\nDisable worker node proxy renewal\n\n\nWorker node proxy renewal is not used by HTCondor-CE and leaving it on\nwill cause some jobs to be held. Edit \n/etc/blah.config\n on the\nHTCondor CE host and set the following two values:\n\n\nblah_disable_wn_proxy_renewal=yes\nblah_delegate_renewed_proxies=no\n\n\n\n\nNOTE\n: This configuration file uses bash syntax rules; there should be no whitespace around the \n=\n.\n\n\nConfiguring authorization\n\n\nThere are two methods to manage authorization for incoming jobs,\n\nedg-mkgridmap\n and GUMS. \nedg-mkgridmap\n is easy to set up and maintain,\nand GUMS has more features and capabilities. We recommend using\n\nedg-mkgridmap\n unless you have specific needs that require the use of\nGUMS. Some examples of these specific requirements are:\n\n\n\n\nYou want to map users based on rules\n\n\nYou need to support multiple VO roles\n\n\nYou need to support gLExec for pilot jobs\n\n\n\n\nAuthorization with edg-mkgridmap\n\n\nTo configure your CE to use \nedg-mkgridmap\n:\n\n\n\n\nFollow the configuration instructions in \nthe edg-mkgridmap\n    document\n to define the VOs that your site\n    accepts\n\n\n\n\nSet some critical gridmap attributes by editing the\n    \n/etc/osg/config.d/10-misc.ini\n file on the HTCondor CE\n    host:\n\n\nauthorization_method = gridmap\n\n\n\n\n\n\nEnable edg-mkgridmap and disable GUMS in the \n/etc/lcmaps.db\n\n    file.\n\n\nIn the \nauthorize_only\n section, comment out the\n\ngumsclient\n line and uncomment the \ngridmapfile\n line. The result\nshould be as follows:\n\n\nauthorize_only:\n # gumsclient -\n good | bad\n gridmapfile -\n good | bad\n\n\n\n\n\n\nSpecify the location of your grid mapfile in\n    \n/etc/condor-ce/config.d/01-common-auth.conf\n:\n\n\nGRIDMAP = /etc/grid-security/grid-mapfile\n\n\nNote:\n The standard location for the grid mapfile is shown\nabove. Use that location unless you have specific reasons to put the\nfile somewhere else.\n\n\n\n\n\n\nAuthorization with GUMS\n\n\n\n\nFollow the instructions in \nthe GUMS installation and\n    configuration document\n to prepare GUMS\n\n\n\n\nSet some critical GUMS attributes by editing the\n    \n/etc/osg/config.d/10-misc.ini\n file on the HTCondor CE\n    host:\n\n\nauthorization_method = xacml\ngums_host = YOUR GUMS HOSTNAME\n\n\n\n\n\n\n\n\nNote\n\n\nOnce \ngsi-authz.conf\n is in place,\nyour local HTCondor will attempt to utilize the LCMAPS callouts if\nenabled in the \ncondor_mapfile\n. If this is not the desired behavior, set\n\nGSI_AUTHZ_CONF=/dev/null\n in the local HTCondor configuration.\n\n\n\n\nConfiguring information systems\n\n\nTo split jobs between the various sites of the OSG, information about\neach site\u2019s availability is uploaded to a central collector. The job\nfactories then query the central collector for idle resources and submit\npilot jobs to the available sites. To advertise your site, you will need\nto run the Generic Information Provider and OSG Info Services.\n\n\nGeneric Information Provider (GIP)\n\n\nThe \nGIP\n is a service that discovers information about your site\nresources like the number of available cores and what VO's are allowed\nto run on your site. Consult the \nGIP configuration\ndocument\n for instructions on\nhow to set up your \nGIP\n service.\n\n\nNOTE\n: If you have \ngip-1.3.11-4\n\ninstalled, manual intervention is required for correct reporting to\nBDII. See \n3.2.20 known\nissues\n.\n\n\nOSG Info Services\n\n\nosg-info-services\n takes the information collected from \nGIP\n and\nuploads it to OSG's central collector. For \nosg-info-services\n to\ncommunicate with the appropriate servers, it needs a service certificate\nand key located at \n/etc/grid-security/http/httpcert.pem\n and\n\n/etc/grid-security/http/httpkey.pem\n, respectively. Additionally, the\nservice runs as either the \ntomcat\n user or the account specified by the\n\nuser\n option in \n/etc/osg/config.d/30-gip.ini\n, thus your service\ncertificates need to be owned by the appropriate user.\n\n\n\n\n\n\nEnable osg-info-services in\n    \n/etc/osg/config.d/30-infoservices.ini\n:\n\n\nenabled = *True*\n\n2.  Generate a \nuser-vo-map\n file with your authorization set\nup:\ni.  If you're using edg-mkgridmap, run the following:\n\n\nedg-mkgridmap\n\nii. If you're using GUMS, run the following:\n\n\ngums-host-cron\n\n\n\n\n\n\nApplying configuration settings\n\n\nMaking changes to the OSG configuration files in the \n/etc/osg/config.d\n\ndirectory does not apply those settings to software automatically.\nSettings that are made outside of the OSG directory take effect\nimmediately or at least when the relevant service is restarted. For the\nOSG settings, use the \nosg-configure\n tool to\nvalidate (to a limited extent) and apply the settings to the relevant\nsoftware components. The \nosg-configure\n software is included\nautomatically in an HTCondor CE installation.\n\n\n\n\n\n\nMake all changes to \n.ini\n files in the \n/etc/osg/config.d\n\n    directory\n\n\n\n\nNote\n\n\nThis document describes the\ncritical settings for HTCondor CE and related software. You may need\nto configure other software that is installed on your HTCondor CE\nhost, too.\n\n\n\n\n\n\n\n\nValidate the configuration settings\n\n\nosg-configure -v\n\n\nFix any errors (at least) that \nosg-configure\n reports.\n\n\n\n\n\n\nOnce the validation command succeeds without errors, apply the\n    configuration settings:\n\n\nosg-configure -c\n\n\n\n\n\n\nOptional configuration\n\n\nThe following configuration steps are optional and will likely not be\nrequired for setting up a small site. If you do not need any of the\nfollowing special configurations, skip to \nthe section on using\nHTCondor CE\n.\n\n\n\n\nTransforming and filtering jobs\n\n\nConfiguring for multiple network interfaces\n\n\nLimiting or disabling locally running jobs on the CE\n\n\nHTCondor accounting groups\n\n\nInstalling the HTCondor-CE View\n\n\n\n\nTransforming and filtering jobs\n\n\nIf you need to modify or filter jobs, more information can be found in\nthe \nJob Router Recipes\n\ndocument.\n\n\nNOTE\n: If you need to assign jobs to\nHTCondor accounting groups, refer to \nthis\n section.\n\n\nConfiguring for multiple network interfaces\n\n\nIf you have multiple network interfaces with different hostnames, the\nHTCondor CE daemons need to know which hostname to use when\ncommunicating to each other. Generally, you will want to set\n\nNETWORK_HOSTNAME\n to the hostname of your public interface in\n\n/etc/condor-ce/config.d/99-local.conf\n directory with the line:\n\n\nNETWORK_HOSTNAME=condorce.example.com\n\n\n\n\nReplacing \ncondorce.example.com\n text with your public\ninterface\u2019s hostname.\n\n\nLimiting or disabling locally jobs running on the CE\n\n\nIf you want to limit or disable jobs running locally on your CE, you\nwill need to configure HTCondor-CE's local and scheduler universes.\nLocal and scheduler universes are HTCondor CE\u2019s analogue to GRAM\u2019s\nmanaged fork: they allow jobs to be run on the CE itself. The two\nuniverses are effectively the same (scheduler universe launches a\nstarter process for each job), so we will be configuring them in unison.\n\n\n\n\n\n\nTo change the default limit\n on the number of locally run jobs\n    (the current default is 20), add the following to\n    \n/etc/condor-ce/config.d/99-local.conf\n:\n\n\nLOCAL_JOB_LIMIT = 20\nSTART_LOCAL_UNIVERSE = TotalLocalJobsRunning + TotalSchedulerJobsRunning \n $(LOCAL_JOB_LIMIT)\nSTART_SCHEDULER_UNIVERSE = $(START_LOCAL_UNIVERSE)\n\n\n(updating \nLOCAL_JOB_LIMIT\n as appropriate.)\n\n\n\n\n\n\nTo only allow a specific user\n to start locally run jobs, add the\n    following to \n/etc/condor-ce/config.d/99-local.conf\n:\n\n\nALLOWED_LOCAL_USER=alice\nSTART_LOCAL_UNIVERSE = target.Owner =?= \"$(ALLOWED_LOCAL_USER)\"\nSTART_SCHEDULER_UNIVERSE = $(START_LOCAL_UNIVERSE)\n\n\n\n\n\n\nTo disable\n locally run jobs, add the following to\n    \n/etc/condor-ce/config.d/99-local.conf\n:\n\n\nSTART_LOCAL_UNIVERSE = False\nSTART_SCHEDULER_UNIVERSE = $(START_LOCAL_UNIVERSE)\n\n\n\n\n\n\nNOTE\n: RSV requires the ability to start\nlocal universe jobs so if you are using RSV, you need to allow local\nuniverse jobs from the \nrsv\n user.\n\n\nHTCondor accounting groups\n\n\nNOTE\n: For HTCondor batch systems only\n\n\nIf you want to provide fairshare on a group basis, as opposed to a Unix\nuser basis, you can use HTCondor accounting groups. They are independent\nof the Unix groups the user may already be in, and are \ndocumented in the HTCondor manual\n.\nIf you are using HTCondor accounting groups, you can map jobs from the\nCE into HTCondor accounting groups based on their numeric user id, their\nDN, or their VOMS attributes.\n\n\nMapping by UID\n\n\nTo map UID\u2019s to an accounting group, use \n/etc/osg/uid_table.txt\n. It is\nconsulted first and contains lines of the form:\n\n\nuid GroupName\n\n\n\n\nExample \nuid_table.txt\n:\n\n\nuscms02 TestGroup\nosg     other.osgedu\n\n\n\n\nMapping by DN or VOMS attribute\n\n\nTo map DN\u2019s or VOMS attributes to an accounting group, use\n\n/etc/osg/extattr_table.txt\n. This file is only consulted if the user is\nnot found in the UID file and it contains lines of the form:\n\n\nSubjectOrAttribute GroupName\n\n\n\n\nThe SubjectOrAttribute can be a Perl regular\nexpression.\n\n\nExample \nextattr_table.txt\n:\n\n\ncmsprio cms.other.prio\ncms\\/Role=production cms.prod\n.* other\n\n\n\n\nInstall and run the HTCondor-CE-View\n\n\nThe HTCondor-CE-View is an optional web interface to the status of your\nCE. To run the View,\n\n\n\n\n\n\nBegin by installing the package htcondor-ce-view:\n\n\nyum install htcondor-ce-view\n\n\n\n\n\n\nNext, uncomment the \nDAEMON_LIST\n configuration located at\n    \n/etc/condor-ce/config.d/05-ce-view.conf\n:\n\n\nDAEMON_LIST = $(DAEMON_LIST), CEVIEW, GANGLIAD\n]]]]\n\n\n\n\n\n\nRestart the CE service.\n\n\nservice condor-ce restart\n\n\n\n\n\n\nBy default, the website is served from port 80. This may be configured\nin \n/etc/condor-ce/config.d/05-ce-view.conf\n as well.\n\n\nUsing HTCondor-CE\n\n\nAs a site administrator, there are a few ways in which you might use the\nHTCondor CE:\n\n\n\n\nManaging the HTCondor CE and associated services\n\n\nUsing HTCondor CE administrative tools to monitor and maintain the\n    job gateway\n\n\nUsing HTCondor CE user tools to test gateway operations\n\n\n\n\nManaging HTCondor CE and associated services\n\n\nIn addition to the HTCondor CE job gateway service itself, there are a\nnumber of supporting services in your installation. The specific\nservices are:\n\n\n\n\n\n\n\n\nSoftware\n\n\nService name\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nFetch CRL\n\n\nOn EL 6: \nfetch-crl-boot\n and \nfetch-crl-cron\n  \n   On EL 5: \nfetch-crl3-boot\n and \nfetch-crl3-cron\n\n\nSee \nCA documentation\n for more info\n\n\n\n\n\n\nGratia\n\n\ngratia-probes-cron\n\n\nAccounting software\n\n\n\n\n\n\nYour batch system\n\n\ncondor\n or \npbs_server\n or \u2026\n\n\n\n\n\n\n\n\nOSG Info Services\n\n\nosg-info-services\n\n\n\n\n\n\n\n\nHTCondor-CE\n\n\ncondor-ce\n\n\n\n\n\n\n\n\n\n\nStart the services in the order listed and stop them in reverse order.\nAs a reminder, here are common service commands (all run as \nroot\n):\n\n\n\n\n\n\n\n\nTo \u2026\n\n\nRun the command \u2026\n\n\n\n\n\n\n\n\n\n\nStart a service\n\n\nservice \nem\nSERVICE-NAME\n/em\n start\n\n\n\n\n\n\nStop a service\n\n\nservice \nem\nSERVICE-NAME\n/em\n stop\n\n\n\n\n\n\nEnable a service to start during boot\n\n\nchkconfig \nem\nSERVICE-NAME\n/em\n on\n\n\n\n\n\n\nDisable a service from starting during boot\n\n\nchkconfig \nem\nSERVICE-NAME\n/em\n off\n\n\n\n\n\n\n\n\nUsing HTCondor-CE tools\n\n\nSome of the HTCondor CE administrative and user tools are documented in\n\nthe HTCondor CE troubleshooting guide\n.\n\n\nValidating HTCondor-CE\n\n\nThere are different ways to make sure that your HTCondor CE host is\nworking well:\n\n\n\n\nPerform automated validation by running \nRSV\n\n\nManually verify your HTCondor CE using \nthe HTCondor CE\n    troubleshooting guide\n; useful tools\n    include:\n\n\ncondor_ce_run\n\n\ncondor_ce_trace\n\n\ncondor_submit\n\n\n\n\n\n\n\n\nTroubleshooting HTCondor-CE\n\n\nFor information on how to troubleshoot your HTCondor CE, please refer to\n\nthe HTCondor CE troubleshooting guide\n.\n\n\nRegistering the CE\n\n\nTo be part of the OSG Production Grid, your CE must be registered in the\n\nhttps://oim.grid.iu.edu/ OSG Information Management System\n\n(OIM). To register your resource:\n\n\n\n\nObtain, install, and verify your user\n    certificate\n (which you may have\n    done already)\n\n\nRegister your site and CE in\n    OIM\n\n\n\n\nGetting Help\n\n\nTo get assistance, please use the \nthis\npage\n.\n\n\nReference\n\n\nHere are some other HTCondor-CE documents that might be helpful:\n\n\n\n\nHTCondor-CE overview and architecture\n\n\nConfiguring HTCondor-CE job routes\n\n\nThe HTCondor-CE troubleshooting guide\n\n\nSubmitting Jobs to HTCondor-CE\n\n\n\n\nConfiguration\n\n\nThe following directories contain the configuration for HTCondor-CE. The\ndirectories are parsed in the order presented and thus configuration\nwithin the final directory will override configuration specified in the\nprevious directories.\n\n\n\n\n\n\n\n\nLocation\n\n\nComment\n\n\n\n\n\n\n\n\n\n\n/usr/share/condor-ce/config.d/\n\n\nConfiguration defaults (overwritten on package updates)\n\n\n\n\n\n\n/etc/condor-ce/config.d/\n\n\nFiles in this directory are parsed in alphanumeric order (i.e., \n99-local.conf\n will override values in \n01-ce-auth.conf\n)\n\n\n\n\n\n\n\n\nFor a detailed order of the way configuration files are parsed, run the\nfollowing command:\n\n\n# condor_ce_config_val -config\n\n\n\n\nUsers\n\n\nThe following users are needed by HTCondor-CE at all sites:\n\n\n\n\n\n\n\n\nUser\n\n\nComment\n\n\n\n\n\n\n\n\n\n\ncondor\n\n\nThe HTCondor-CE will be run as root, but perform most of its operations as the \ncondor\n user.\n\n\n\n\n\n\ngratia\n\n\nRuns the Gratia probes to collect accounting data\n\n\n\n\n\n\ntomcat\n\n\nDefault user that runs GIP\n\n\n\n\n\n\n\n\nCertificates\n\n\n\n\n\n\n\n\nCertificate\n\n\nUser that owns certificate\n\n\nPath to certificate\n\n\n\n\n\n\n\n\n\n\nHost certificate\n\n\nroot\n\n\n/etc/grid-security/hostcert.pem\n \nbr>  \n/etc/grid-security/hostkey.pem\n\n\n\n\n\n\n\n\nFind instructions to request a host certificate \nhere\n.", 
            "title": "HTCondor-CE"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#installing-and-maintaining-htcondor-ce", 
            "text": "", 
            "title": "Installing and Maintaining HTCondor-CE"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#about-this-guide", 
            "text": "The HTCondor-CE software is a  job gateway  for an OSG Compute Element\n(CE). As such, HTCondor-CE is the entry point for jobs coming from the\nOSG \u2014 it handles authorization and delegation of jobs to your local\nbatch system. In OSG today, most CEs accept  pilot jobs  from a factory,\nwhich in turn are able to accept and run end-user jobs.  Use this page to learn how to install, configure, run, test, and\ntroubleshoot HTCondor-CE from the OSG software repositories.", 
            "title": "About This Guide"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#before-starting", 
            "text": "Before starting the installation process, consider the following points\n(consulting  the Reference section below  as needed):   User IDs:  If they do not exist already, the installation will\n    create the Linux user IDs  condor  (UID 4716),  tomcat  (UID 91) and\n     gratia  (UID 42401)  Service certificate:  The HTCondor-CE service uses a host\n    certificate at  /etc/grid-security/host*.pem  Network ports:  The pilot factories must be able to contact your\n    HTCondor-CE service on ports 9619 and 9620 (TCP)  Host choice:  HTCondor-CE should be installed on a host that\n    already has the ability to submit jobs into your local cluster   As with all OSG software installations, there are some one-time (per\nhost) steps to prepare in advance:   Ensure the host has  a supported operating\n    system  Obtain root access to the host  Prepare  the required Yum repositories  Install  CA certificates", 
            "title": "Before Starting"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#installing-htcondor-ce", 
            "text": "An HTCondor-CE installation consists of the job gateway (i.e., the\nHTCondor CE job router) and other support software (e.g., GridFTP, a\nGratia probe, authorization software). To simplify installation, OSG\nprovides convenience RPMs that install all required software with a\nsingle command.   If your batch system is already installed via non-RPM means and is\n   in the following list, install the appropriate 'empty' RPM. Otherwise,\n   skip to the next step.      If your batch system is\u2026  Then run the following command\u2026      HTCondor  yum install empty-condor --enablerepo=osg-empty    PBS  yum install empty-torque --enablerepo=osg-empty    SGE  yum install empty-gridengine --enablerepo=osg-empty      Select the appropriate convenience RPM(s):      If your batch system is\u2026  Then use the following package(s)\u2026      HTCondor  osg-ce-condor    LSF  osg-ce-lsf    PBS  osg-ce-pbs    SGE  osg-ce-sge    SLURM  osg-ce-slurm      Install the CE software:   yum install *PACKAGE(S)*  To ease the transition from GRAM\nto HTCondor-CEs, the convenience RPMs install both types of job gateway\nsoftware. By default, the HTCondor gateway is enabled and the GRAM\ngateway is disabled, which is the correct configuration for most\nHTCondor-CE-based sites (but see the gateway configuration section below\nfor more options).   Note  HTCondor CE version 1.6 or later\nis required to send site resource information to OSG for matching jobs\nto resources.", 
            "title": "Installing HTCondor-CE"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#configuring-htcondor-ce", 
            "text": "There are a few required configuration steps to connect HTCondor CE with\nyour batch system and authorization method. For more advanced\nconfiguration, see the section on  optional\nconfigurations .", 
            "title": "Configuring HTCondor-CE"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#enabling-htcondor-ce", 
            "text": "If you are installing HTCondor CE on a new host, the default\nconfiguration is correct and you can  skip  this step!\nHowever, if you are updating a host that used a Globus GRAM job gateway\n(aka the Globus gatekeeper), you must enable the HTCondor job gateway.   Decide whether to disable GRAM (the preferred option) or run\n    both HTCondor and GRAM CEs   Edit the gateway configuration file\n     /etc/osg/config.d/10-gateway.ini  to reflect your choice\n    To enable HTCondor CE and disable GRAM CE:  gram_gateway_enabled = False\nhtcondor_gateway_enabled = True  To enable both HTCondor and GRAM CEs:  gram_gateway_enabled = True\nhtcondor_gateway_enabled = True    More information about the Globus GRAM CE can be found  here .", 
            "title": "Enabling HTCondor-CE"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#batch-system", 
            "text": "", 
            "title": "Batch System"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#configuring-the-batch-system", 
            "text": "Enable your batch system by editing the  enabled  field in the /etc/osg/config.d/20-YOUR-BATCH-SYSTEM.ini \nfile:  enabled = True", 
            "title": "Configuring the batch system"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#batch-systems-other-than-htcondor", 
            "text": "If you are using HTCondor as your  local batch system  (i.e., in\naddition to your HTCondor CE), skip to the  configuring\nauthorization  section. For other batch\nsystems (e.g., PBS, LSF, SGE, SLURM), keep reading.", 
            "title": "Batch systems other than HTCondor"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#sharing-the-spool-directory", 
            "text": "To transfer files between the CE and the batch system, HTCondor CE\nrequires a shared file system. The current recommendation is to run a\ndedicated NFS server (whose installation is beyond the scope of this\ndocument) on the  CE host . In this setup, HTCondor-CE writes to the\nlocal spool directory, the NFS server exports the it, and the NFS server\nshares the it with all of the worker nodes.  NOTE : If you choose not to host the NFS\nserver on your CE, you will need to turn off root squash so that the\nHTCondor-CE daemons can write to the spool directory.  By default, the spool directory is  /var/lib/condor-ce  but you can\ncontrol this by setting  SPOOL  in /etc/condor-ce/config.d/99-local.conf . For example, the following sets\nthe  SPOOL  directory to  /home/condor :  SPOOL=/home/condor  NOTE : The shared spool directory must\nbe readable and writeable by the  condor  user for HTCondor CE to\nfunction correctly.", 
            "title": "Sharing the spool directory"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#disable-worker-node-proxy-renewal", 
            "text": "Worker node proxy renewal is not used by HTCondor-CE and leaving it on\nwill cause some jobs to be held. Edit  /etc/blah.config  on the\nHTCondor CE host and set the following two values:  blah_disable_wn_proxy_renewal=yes\nblah_delegate_renewed_proxies=no  NOTE : This configuration file uses bash syntax rules; there should be no whitespace around the  = .", 
            "title": "Disable worker node proxy renewal"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#configuring-authorization", 
            "text": "There are two methods to manage authorization for incoming jobs, edg-mkgridmap  and GUMS.  edg-mkgridmap  is easy to set up and maintain,\nand GUMS has more features and capabilities. We recommend using edg-mkgridmap  unless you have specific needs that require the use of\nGUMS. Some examples of these specific requirements are:   You want to map users based on rules  You need to support multiple VO roles  You need to support gLExec for pilot jobs", 
            "title": "Configuring authorization"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#authorization-with-edg-mkgridmap", 
            "text": "To configure your CE to use  edg-mkgridmap :   Follow the configuration instructions in  the edg-mkgridmap\n    document  to define the VOs that your site\n    accepts   Set some critical gridmap attributes by editing the\n     /etc/osg/config.d/10-misc.ini  file on the HTCondor CE\n    host:  authorization_method = gridmap    Enable edg-mkgridmap and disable GUMS in the  /etc/lcmaps.db \n    file.  In the  authorize_only  section, comment out the gumsclient  line and uncomment the  gridmapfile  line. The result\nshould be as follows:  authorize_only:\n # gumsclient -  good | bad\n gridmapfile -  good | bad    Specify the location of your grid mapfile in\n     /etc/condor-ce/config.d/01-common-auth.conf :  GRIDMAP = /etc/grid-security/grid-mapfile  Note:  The standard location for the grid mapfile is shown\nabove. Use that location unless you have specific reasons to put the\nfile somewhere else.", 
            "title": "Authorization with edg-mkgridmap"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#authorization-with-gums", 
            "text": "Follow the instructions in  the GUMS installation and\n    configuration document  to prepare GUMS   Set some critical GUMS attributes by editing the\n     /etc/osg/config.d/10-misc.ini  file on the HTCondor CE\n    host:  authorization_method = xacml\ngums_host = YOUR GUMS HOSTNAME     Note  Once  gsi-authz.conf  is in place,\nyour local HTCondor will attempt to utilize the LCMAPS callouts if\nenabled in the  condor_mapfile . If this is not the desired behavior, set GSI_AUTHZ_CONF=/dev/null  in the local HTCondor configuration.", 
            "title": "Authorization with GUMS"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#configuring-information-systems", 
            "text": "To split jobs between the various sites of the OSG, information about\neach site\u2019s availability is uploaded to a central collector. The job\nfactories then query the central collector for idle resources and submit\npilot jobs to the available sites. To advertise your site, you will need\nto run the Generic Information Provider and OSG Info Services.", 
            "title": "Configuring information systems"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#generic-information-provider-gip", 
            "text": "The  GIP  is a service that discovers information about your site\nresources like the number of available cores and what VO's are allowed\nto run on your site. Consult the  GIP configuration\ndocument  for instructions on\nhow to set up your  GIP  service.  NOTE : If you have  gip-1.3.11-4 \ninstalled, manual intervention is required for correct reporting to\nBDII. See  3.2.20 known\nissues .", 
            "title": "Generic Information Provider (GIP)"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#osg-info-services", 
            "text": "osg-info-services  takes the information collected from  GIP  and\nuploads it to OSG's central collector. For  osg-info-services  to\ncommunicate with the appropriate servers, it needs a service certificate\nand key located at  /etc/grid-security/http/httpcert.pem  and /etc/grid-security/http/httpkey.pem , respectively. Additionally, the\nservice runs as either the  tomcat  user or the account specified by the user  option in  /etc/osg/config.d/30-gip.ini , thus your service\ncertificates need to be owned by the appropriate user.    Enable osg-info-services in\n     /etc/osg/config.d/30-infoservices.ini :  enabled = *True* \n2.  Generate a  user-vo-map  file with your authorization set\nup:\ni.  If you're using edg-mkgridmap, run the following:  edg-mkgridmap \nii. If you're using GUMS, run the following:  gums-host-cron", 
            "title": "OSG Info Services"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#applying-configuration-settings", 
            "text": "Making changes to the OSG configuration files in the  /etc/osg/config.d \ndirectory does not apply those settings to software automatically.\nSettings that are made outside of the OSG directory take effect\nimmediately or at least when the relevant service is restarted. For the\nOSG settings, use the  osg-configure  tool to\nvalidate (to a limited extent) and apply the settings to the relevant\nsoftware components. The  osg-configure  software is included\nautomatically in an HTCondor CE installation.    Make all changes to  .ini  files in the  /etc/osg/config.d \n    directory   Note  This document describes the\ncritical settings for HTCondor CE and related software. You may need\nto configure other software that is installed on your HTCondor CE\nhost, too.     Validate the configuration settings  osg-configure -v  Fix any errors (at least) that  osg-configure  reports.    Once the validation command succeeds without errors, apply the\n    configuration settings:  osg-configure -c", 
            "title": "Applying configuration settings"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#optional-configuration", 
            "text": "The following configuration steps are optional and will likely not be\nrequired for setting up a small site. If you do not need any of the\nfollowing special configurations, skip to  the section on using\nHTCondor CE .   Transforming and filtering jobs  Configuring for multiple network interfaces  Limiting or disabling locally running jobs on the CE  HTCondor accounting groups  Installing the HTCondor-CE View", 
            "title": "Optional configuration"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#transforming-and-filtering-jobs", 
            "text": "If you need to modify or filter jobs, more information can be found in\nthe  Job Router Recipes \ndocument.  NOTE : If you need to assign jobs to\nHTCondor accounting groups, refer to  this  section.", 
            "title": "Transforming and filtering jobs"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#configuring-for-multiple-network-interfaces", 
            "text": "If you have multiple network interfaces with different hostnames, the\nHTCondor CE daemons need to know which hostname to use when\ncommunicating to each other. Generally, you will want to set NETWORK_HOSTNAME  to the hostname of your public interface in /etc/condor-ce/config.d/99-local.conf  directory with the line:  NETWORK_HOSTNAME=condorce.example.com  Replacing  condorce.example.com  text with your public\ninterface\u2019s hostname.", 
            "title": "Configuring for multiple network interfaces"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#limiting-or-disabling-locally-jobs-running-on-the-ce", 
            "text": "If you want to limit or disable jobs running locally on your CE, you\nwill need to configure HTCondor-CE's local and scheduler universes.\nLocal and scheduler universes are HTCondor CE\u2019s analogue to GRAM\u2019s\nmanaged fork: they allow jobs to be run on the CE itself. The two\nuniverses are effectively the same (scheduler universe launches a\nstarter process for each job), so we will be configuring them in unison.    To change the default limit  on the number of locally run jobs\n    (the current default is 20), add the following to\n     /etc/condor-ce/config.d/99-local.conf :  LOCAL_JOB_LIMIT = 20\nSTART_LOCAL_UNIVERSE = TotalLocalJobsRunning + TotalSchedulerJobsRunning   $(LOCAL_JOB_LIMIT)\nSTART_SCHEDULER_UNIVERSE = $(START_LOCAL_UNIVERSE)  (updating  LOCAL_JOB_LIMIT  as appropriate.)    To only allow a specific user  to start locally run jobs, add the\n    following to  /etc/condor-ce/config.d/99-local.conf :  ALLOWED_LOCAL_USER=alice\nSTART_LOCAL_UNIVERSE = target.Owner =?= \"$(ALLOWED_LOCAL_USER)\"\nSTART_SCHEDULER_UNIVERSE = $(START_LOCAL_UNIVERSE)    To disable  locally run jobs, add the following to\n     /etc/condor-ce/config.d/99-local.conf :  START_LOCAL_UNIVERSE = False\nSTART_SCHEDULER_UNIVERSE = $(START_LOCAL_UNIVERSE)    NOTE : RSV requires the ability to start\nlocal universe jobs so if you are using RSV, you need to allow local\nuniverse jobs from the  rsv  user.", 
            "title": "Limiting or disabling locally jobs running on the CE"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#htcondor-accounting-groups", 
            "text": "NOTE : For HTCondor batch systems only  If you want to provide fairshare on a group basis, as opposed to a Unix\nuser basis, you can use HTCondor accounting groups. They are independent\nof the Unix groups the user may already be in, and are  documented in the HTCondor manual .\nIf you are using HTCondor accounting groups, you can map jobs from the\nCE into HTCondor accounting groups based on their numeric user id, their\nDN, or their VOMS attributes.", 
            "title": "HTCondor accounting groups"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#mapping-by-uid", 
            "text": "To map UID\u2019s to an accounting group, use  /etc/osg/uid_table.txt . It is\nconsulted first and contains lines of the form:  uid GroupName  Example  uid_table.txt :  uscms02 TestGroup\nosg     other.osgedu", 
            "title": "Mapping by UID"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#mapping-by-dn-or-voms-attribute", 
            "text": "To map DN\u2019s or VOMS attributes to an accounting group, use /etc/osg/extattr_table.txt . This file is only consulted if the user is\nnot found in the UID file and it contains lines of the form:  SubjectOrAttribute GroupName  The SubjectOrAttribute can be a Perl regular\nexpression.  Example  extattr_table.txt :  cmsprio cms.other.prio\ncms\\/Role=production cms.prod\n.* other", 
            "title": "Mapping by DN or VOMS attribute"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#install-and-run-the-htcondor-ce-view", 
            "text": "The HTCondor-CE-View is an optional web interface to the status of your\nCE. To run the View,    Begin by installing the package htcondor-ce-view:  yum install htcondor-ce-view    Next, uncomment the  DAEMON_LIST  configuration located at\n     /etc/condor-ce/config.d/05-ce-view.conf :  DAEMON_LIST = $(DAEMON_LIST), CEVIEW, GANGLIAD\n]]]]    Restart the CE service.  service condor-ce restart    By default, the website is served from port 80. This may be configured\nin  /etc/condor-ce/config.d/05-ce-view.conf  as well.", 
            "title": "Install and run the HTCondor-CE-View"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#using-htcondor-ce", 
            "text": "As a site administrator, there are a few ways in which you might use the\nHTCondor CE:   Managing the HTCondor CE and associated services  Using HTCondor CE administrative tools to monitor and maintain the\n    job gateway  Using HTCondor CE user tools to test gateway operations", 
            "title": "Using HTCondor-CE"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#managing-htcondor-ce-and-associated-services", 
            "text": "In addition to the HTCondor CE job gateway service itself, there are a\nnumber of supporting services in your installation. The specific\nservices are:     Software  Service name  Notes      Fetch CRL  On EL 6:  fetch-crl-boot  and  fetch-crl-cron       On EL 5:  fetch-crl3-boot  and  fetch-crl3-cron  See  CA documentation  for more info    Gratia  gratia-probes-cron  Accounting software    Your batch system  condor  or  pbs_server  or \u2026     OSG Info Services  osg-info-services     HTCondor-CE  condor-ce      Start the services in the order listed and stop them in reverse order.\nAs a reminder, here are common service commands (all run as  root ):     To \u2026  Run the command \u2026      Start a service  service  em SERVICE-NAME /em  start    Stop a service  service  em SERVICE-NAME /em  stop    Enable a service to start during boot  chkconfig  em SERVICE-NAME /em  on    Disable a service from starting during boot  chkconfig  em SERVICE-NAME /em  off", 
            "title": "Managing HTCondor CE and associated services"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#using-htcondor-ce-tools", 
            "text": "Some of the HTCondor CE administrative and user tools are documented in the HTCondor CE troubleshooting guide .", 
            "title": "Using HTCondor-CE tools"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#validating-htcondor-ce", 
            "text": "There are different ways to make sure that your HTCondor CE host is\nworking well:   Perform automated validation by running  RSV  Manually verify your HTCondor CE using  the HTCondor CE\n    troubleshooting guide ; useful tools\n    include:  condor_ce_run  condor_ce_trace  condor_submit", 
            "title": "Validating HTCondor-CE"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#troubleshooting-htcondor-ce", 
            "text": "For information on how to troubleshoot your HTCondor CE, please refer to the HTCondor CE troubleshooting guide .", 
            "title": "Troubleshooting HTCondor-CE"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#registering-the-ce", 
            "text": "To be part of the OSG Production Grid, your CE must be registered in the https://oim.grid.iu.edu/ OSG Information Management System \n(OIM). To register your resource:   Obtain, install, and verify your user\n    certificate  (which you may have\n    done already)  Register your site and CE in\n    OIM", 
            "title": "Registering the CE"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#getting-help", 
            "text": "To get assistance, please use the  this\npage .", 
            "title": "Getting Help"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#reference", 
            "text": "Here are some other HTCondor-CE documents that might be helpful:   HTCondor-CE overview and architecture  Configuring HTCondor-CE job routes  The HTCondor-CE troubleshooting guide  Submitting Jobs to HTCondor-CE", 
            "title": "Reference"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#configuration", 
            "text": "The following directories contain the configuration for HTCondor-CE. The\ndirectories are parsed in the order presented and thus configuration\nwithin the final directory will override configuration specified in the\nprevious directories.     Location  Comment      /usr/share/condor-ce/config.d/  Configuration defaults (overwritten on package updates)    /etc/condor-ce/config.d/  Files in this directory are parsed in alphanumeric order (i.e.,  99-local.conf  will override values in  01-ce-auth.conf )     For a detailed order of the way configuration files are parsed, run the\nfollowing command:  # condor_ce_config_val -config", 
            "title": "Configuration"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#users", 
            "text": "The following users are needed by HTCondor-CE at all sites:     User  Comment      condor  The HTCondor-CE will be run as root, but perform most of its operations as the  condor  user.    gratia  Runs the Gratia probes to collect accounting data    tomcat  Default user that runs GIP", 
            "title": "Users"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#certificates", 
            "text": "Certificate  User that owns certificate  Path to certificate      Host certificate  root  /etc/grid-security/hostcert.pem   br>   /etc/grid-security/hostkey.pem     Find instructions to request a host certificate  here .", 
            "title": "Certificates"
        }, 
        {
            "location": "/Frontier_Squid/squid/", 
            "text": "Frontier Squid Caching Proxy Installation Guide\n\n\n\n\nAbout This Document\n\n\nThis document is intended for System Administrators who are installing\n\nfrontier-squid\n, the OSG distribution of the Frontier Squid software.\n\n\nApplicable Versions\n\n\nThe applicable software versions for this document are OSG Version \n= 3.1.40 or \n=\n3.2.16. The version of frontier-squid installed should be \n= 2.7.STABLE9-19.1\n\n\nAbout Frontier Squid\n\n\nFrontier Squid is a distribution of the well-known \nsquid HTTP caching\nproxy software\n that is optimized for use with\napplications on the Worldwide LHC Computing Grid (WLCG). It has \nmany\nadvantages\n\nover regular squid for common grid applications, especially Frontier and\nCVMFS.\n\n\nThe OSG distribution of frontier-squid is a straight rebuild of the\nupstream frontier-squid package for the convenience of OSG users.\n\n\nFrontier Squid is Recommended\n\n\nOSG recommends that all sites run a caching proxy for HTTP and HTTPS to\nhelp reduce bandwidth and improve throughput. To that end, Compute\nElement (CE) installations include Frontier Squid automatically. We\nencourage all sites to configure and use this service, as described\nbelow.\n\n\nFor large sites that expect heavy load on the proxy, it may be best to\nrun the proxy on its own host. In that case, the Frontier Squid software\nstill will be installed on the CE, but it need not be enabled. Instead,\ninstall your proxy service on the separate host and then configure the\nCE host to refer to the proxy on that host.\n\n\nThe \nosg-configure\n configuration tool (version 1.0.45 and later) warns\nusers who have not added the proxy location to their CE configuration.\nIn the future, a proxy will be required and osg-configure will fail if\nthe proxy location is not set.\n\n\nEngineering Considerations\n\n\nIf you will be supporting the Frontier application at your site, review\nthe \nupstream documentation Hardware considerations\nsection\n\nto determine how to size your equipment.\n\n\nRequirements\n\n\nHost and OS\n\n\n\n\nOS is Red Hat Enterprise Linux 5, 6, 7, and variants\n\n\nRoot access\n\n\n\n\nUsers The frontier-squid installation will create one user account\n\n\nunless it already exists.\n\n\n\n\n\n\n\n\nUser\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nsquid\n\n\nReduced privilege user that the squid process runs under. Set the default gid of the \u201csquid\u201d user to be a group that is also called \u201csquid\u201d.\n\n\n\n\n\n\n\n\nThe package can instead use another user name of your choice if you\ncreate a configuration file before installation. Details are in the\n\nupstream documentation Preparation\nsection\n.\n\n\nNetworking\n\n\n\n\n\n\n\n\nService Name\n\n\nProtocol\n\n\nPort Number\n\n\nInbound\n\n\nOutbound\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nSquid\n\n\ntcp\n\n\n3128\n\n\n\u2713\n\n\n\u2713\n\n\nAlso limited in squid ACLs. Both in and outbound must not be wide open to internet simultaneously\n\n\n\n\n\n\nSquid monitor\n\n\nudp\n\n\n3401\n\n\n\u2713\n\n\n\n\nAlso limited in squid ACLs. Should be limited to monitoring server addresses\n\n\n\n\n\n\n\n\nFirewalls\n \nDocumentation/Release3.FirewallInformation\n\n\\ \nFirewalls\n\n\nThe addresses of the WLCG monitoring servers for use in firewalls are\nlisted in the \nupstream documentation Enabling monitoring\nsection\n.\n\n\nInstall Instructions\n\n\nInstall the Yum Repositories required by OSG\n\n\nThe OSG RPMs currently support Red Hat Enterprise Linux 5, 6, 7, and variants\n\n\nOSG RPMs are distributed via the OSG yum repositories. Some packages depend on packages distributed via the EPEL repositories. So both repositories must be enabled.\n\n\nInstall EPEL\n\n\nInstall the EPEL repository, if not already present. Note: This enables EPEL by default. Choose the right version to match your OS version.\n\n\n# EPEL 5 (For RHEL 5, CentOS 5, and SL 5) \n[root@client ~]$ curl -O https://dl.fedoraproject.org/pub/epel/epel-release-latest-5.noarch.rpm\n[root@client ~]$ rpm -Uvh epel-release-latest-5.noarch.rpm\n# EPEL 6 (For RHEL 6, CentOS 6, and SL 6) \n[root@client ~]$ rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-6.noarch.rpm\n# EPEL 7 (For RHEL 7, CentOS 7, and SL 7) \n[root@client ~]$ rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\n\n\n\n\nInstall the Yum priorities package\n\n\nFor packages that exist in both OSG and EPEL repositories, it is important to prefer the OSG ones or else OSG software installs may fail. Installing the Yum priorities package enables the repository priority system to work.\n\n\n\n\nChoose the correct package name based on your operating system\u2019s major version:\n ..\n For EL 5 systems, use yum-priorities\n ..\n For EL 6 and EL 7 systems, use yum-plugin-priorities\n\n\nInstall the yum priorities package:\n\n\n\n\n[root@client ~]$ yum install PACKAGE\n\n\n\n\nReplace PACKAGE with the package name from the previous step.\n3. Ensure that \n/etc/yum.conf\n has the following line in the \n[main]\n section (particularly when using ROCKS), thereby enabling Yum plugins, including the priorities one:\n\n\nplugins=1\n\n\n\n\nNOTE\n: If you do not have a required key you can force the installation using \n--nogpgcheck\n; e.g., \n\n\nyum install --nogpgcheck yum-priorities\n\n\n\n\nInstall OSG Repositories\n\n\n\n\nIf you are upgrading from OSG 3.1 (or 3.2) to OSG 3.2 (or 3.3), remove the old OSG repository definition files and clean the Yum cache:\n\n\n\n\n[root@client ~]$ yum clean all\n[root@client ~]$ rpm -e osg-release\n\n\n\n\nThis step ensures that local changes to \n*.repo\n files will not block the installation of the new OSG repositories. After this step, \n*.repo\n files that have been changed will exist in \n/etc/yum.repos.d/\n with the \n*.rpmsave\n extension. After installing the new OSG repositories (the next step) you may want to apply any changes made in the \n*.rpmsave\n files to the new \n*.repo\n files.\n2. Install the OSG repositories using one of the following methods depending on your EL version:\n..1. For EL versions greater than EL5, install the files directly from \nrepo.grid.iu\n:\n\n\n[root@client ~]$ rpm -Uvh URL\n\n\n\n\nwhere URL is one of the following:\n\n\n\n\n\n\n\n\nSeries\n\n\nEL6 URL (for RHEL 6, CentOS 6, or SL 6)\n\n\nEL7 URL (for RHEL 7, CentOS 7, or SL 7)\n\n\n\n\n\n\n\n\n\n\nOSG 3.2\n\n\nhttps://repo.grid.iu.edu/osg/3.2/osg-3.2-el6-release-latest.rpm\n\n\nN/A\n\n\n\n\n\n\nOSG 3.3\n\n\nhttps://repo.grid.iu.edu/osg/3.3/osg-3.3-el6-release-latest.rpm\n\n\nhttps://repo.grid.iu.edu/osg/3.3/osg-3.3-el7-release-latest.rpm\n\n\n\n\n\n\n\n\nInstalling Frontier Squid\n\n\nAfter meeting the requirements in the previous section, install\nfrontier-squid with this command: \n\n\n[root@client ~]# yum install frontier-squid\n\n\n\n\nThen enable it to start at boot time with this command: \n\n\n[root@client ~]# chkconfig frontier-squid on\n\n\n\n\nConfiguring Frontier Squid\n\n\nConfiguring the Frontier Squid Service\n\n\nTo configure the Frontier Squid service itself:\n\n\n\n\nFollow the \noriginal Frontier Squid\n    documentation\n,\n    in \nthe Configuration\n    section\n\\\n   \nNote:\n An important difference between the standard Squid\n    software and the Frontier Squid variant is that Frontier Squid\n    changes are in \n/etc/squid/customize.sh\n instead of\n    \n/etc/squid/squid.conf\n.\n\n\nEnable, start, and test the service (as described below)\n\n\nEnable WLCG monitoring as described in the \nupstream documentation\n    on enabling\n    monitoring\n\n    and \nregister the squid in\n    OIM\n.\n\n\n\n\nConfiguring the OSG CE\n\n\nTo configure the OSG Compute Element (CE) to know about your Frontier\nSquid service:\n\n\n\n\n\n\nOn your CE host, edit \n/etc/osg/config.d/01-squid.ini\n\n\n\n\nMake sure that \nenabled\n is set to \nTrue\n\n\nSet \nlocation\n to the hostname and port of your Frontier Squid\n    service (e.g., \nmy.squid.host.edu:3128\n)\n\n\nLeave the other settings at \nDEFAULT\n unless you have specific\n    reasons to change them\n\n\n\n\n\n\n\n\nRun \nosg-configure\n to propagate the changes on your CE\n\nNote:\n You may want to finish other CE configuration tasks before running \nosg-configure\n. Just be sure to run it once before starting CE services.\n\n\n\n\n\n\nStarting and Stopping the Frontier Squid Service\n\n\nStarting frontier-squid:\n\n\n[root@client ~]# service frontier-squid start\n\n\n\n\nStopping frontier-squid:\n\n\n[root@client ~]# service frontier-squid stop\n\n\n\n\nTesting Frontier Squid\n\n\nAs any user on another computer, do the following (where\n%RED%yoursquid.your.domain\n is\nthe fully qualified domain name of your squid server):\n\n\n[user@client ~]$ export http_proxy=http://yoursquid.your.domain:3128\n[user@client ~]$ wget -qdO/dev/null http://frontier.cern.ch 2\n1|grep X-Cache\nX-Cache: MISS from yoursquid.your.domain\n[user@client ~]$ wget -qdO/dev/null http://frontier.cern.ch 2\n1|grep X-Cache\nX-Cache: HIT from yoursquid.your.domain\n\n\n\n\nIf the grep doesn\u2019t print anything, try removing it from the pipeline to\nsee if errors are obvious. If the second try says MISS again, something\nis probably wrong with the squid cache writes.\n\n\nIf your squid will be supporting the Frontier application, it is also\ngood to do the test in the \nupstream documentation Testing the\ninstallation\nsection\n.\n\n\nFrontier Squid Log Files\n\n\nLog file contents are explained in the \nupstream documentation Log file\ncontents\nsection\n.\n\n\nGetting Help\n\n\nTo get assistance please use \nHelp\nProcedure\n.", 
            "title": "HTTP Cache"
        }, 
        {
            "location": "/Frontier_Squid/squid/#frontier-squid-caching-proxy-installation-guide", 
            "text": "", 
            "title": "Frontier Squid Caching Proxy Installation Guide"
        }, 
        {
            "location": "/Frontier_Squid/squid/#about-this-document", 
            "text": "This document is intended for System Administrators who are installing frontier-squid , the OSG distribution of the Frontier Squid software.", 
            "title": "About This Document"
        }, 
        {
            "location": "/Frontier_Squid/squid/#applicable-versions", 
            "text": "The applicable software versions for this document are OSG Version  = 3.1.40 or  =\n3.2.16. The version of frontier-squid installed should be  = 2.7.STABLE9-19.1", 
            "title": "Applicable Versions"
        }, 
        {
            "location": "/Frontier_Squid/squid/#about-frontier-squid", 
            "text": "Frontier Squid is a distribution of the well-known  squid HTTP caching\nproxy software  that is optimized for use with\napplications on the Worldwide LHC Computing Grid (WLCG). It has  many\nadvantages \nover regular squid for common grid applications, especially Frontier and\nCVMFS.  The OSG distribution of frontier-squid is a straight rebuild of the\nupstream frontier-squid package for the convenience of OSG users.", 
            "title": "About Frontier Squid"
        }, 
        {
            "location": "/Frontier_Squid/squid/#frontier-squid-is-recommended", 
            "text": "OSG recommends that all sites run a caching proxy for HTTP and HTTPS to\nhelp reduce bandwidth and improve throughput. To that end, Compute\nElement (CE) installations include Frontier Squid automatically. We\nencourage all sites to configure and use this service, as described\nbelow.  For large sites that expect heavy load on the proxy, it may be best to\nrun the proxy on its own host. In that case, the Frontier Squid software\nstill will be installed on the CE, but it need not be enabled. Instead,\ninstall your proxy service on the separate host and then configure the\nCE host to refer to the proxy on that host.  The  osg-configure  configuration tool (version 1.0.45 and later) warns\nusers who have not added the proxy location to their CE configuration.\nIn the future, a proxy will be required and osg-configure will fail if\nthe proxy location is not set.", 
            "title": "Frontier Squid is Recommended"
        }, 
        {
            "location": "/Frontier_Squid/squid/#engineering-considerations", 
            "text": "If you will be supporting the Frontier application at your site, review\nthe  upstream documentation Hardware considerations\nsection \nto determine how to size your equipment.", 
            "title": "Engineering Considerations"
        }, 
        {
            "location": "/Frontier_Squid/squid/#requirements", 
            "text": "", 
            "title": "Requirements"
        }, 
        {
            "location": "/Frontier_Squid/squid/#host-and-os", 
            "text": "OS is Red Hat Enterprise Linux 5, 6, 7, and variants  Root access", 
            "title": "Host and OS"
        }, 
        {
            "location": "/Frontier_Squid/squid/#users-the-frontier-squid-installation-will-create-one-user-account", 
            "text": "unless it already exists.     User  Comment      squid  Reduced privilege user that the squid process runs under. Set the default gid of the \u201csquid\u201d user to be a group that is also called \u201csquid\u201d.     The package can instead use another user name of your choice if you\ncreate a configuration file before installation. Details are in the upstream documentation Preparation\nsection .", 
            "title": "Users The frontier-squid installation will create one user account"
        }, 
        {
            "location": "/Frontier_Squid/squid/#networking", 
            "text": "Service Name  Protocol  Port Number  Inbound  Outbound  Comment      Squid  tcp  3128  \u2713  \u2713  Also limited in squid ACLs. Both in and outbound must not be wide open to internet simultaneously    Squid monitor  udp  3401  \u2713   Also limited in squid ACLs. Should be limited to monitoring server addresses     Firewalls   Documentation/Release3.FirewallInformation \n\\  Firewalls  The addresses of the WLCG monitoring servers for use in firewalls are\nlisted in the  upstream documentation Enabling monitoring\nsection .", 
            "title": "Networking"
        }, 
        {
            "location": "/Frontier_Squid/squid/#install-instructions", 
            "text": "", 
            "title": "Install Instructions"
        }, 
        {
            "location": "/Frontier_Squid/squid/#install-the-yum-repositories-required-by-osg", 
            "text": "The OSG RPMs currently support Red Hat Enterprise Linux 5, 6, 7, and variants  OSG RPMs are distributed via the OSG yum repositories. Some packages depend on packages distributed via the EPEL repositories. So both repositories must be enabled.", 
            "title": "Install the Yum Repositories required by OSG"
        }, 
        {
            "location": "/Frontier_Squid/squid/#install-epel", 
            "text": "Install the EPEL repository, if not already present. Note: This enables EPEL by default. Choose the right version to match your OS version.  # EPEL 5 (For RHEL 5, CentOS 5, and SL 5) \n[root@client ~]$ curl -O https://dl.fedoraproject.org/pub/epel/epel-release-latest-5.noarch.rpm\n[root@client ~]$ rpm -Uvh epel-release-latest-5.noarch.rpm\n# EPEL 6 (For RHEL 6, CentOS 6, and SL 6) \n[root@client ~]$ rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-6.noarch.rpm\n# EPEL 7 (For RHEL 7, CentOS 7, and SL 7) \n[root@client ~]$ rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm", 
            "title": "Install EPEL"
        }, 
        {
            "location": "/Frontier_Squid/squid/#install-the-yum-priorities-package", 
            "text": "For packages that exist in both OSG and EPEL repositories, it is important to prefer the OSG ones or else OSG software installs may fail. Installing the Yum priorities package enables the repository priority system to work.   Choose the correct package name based on your operating system\u2019s major version:\n ..  For EL 5 systems, use yum-priorities\n ..  For EL 6 and EL 7 systems, use yum-plugin-priorities  Install the yum priorities package:   [root@client ~]$ yum install PACKAGE  Replace PACKAGE with the package name from the previous step.\n3. Ensure that  /etc/yum.conf  has the following line in the  [main]  section (particularly when using ROCKS), thereby enabling Yum plugins, including the priorities one:  plugins=1  NOTE : If you do not have a required key you can force the installation using  --nogpgcheck ; e.g.,   yum install --nogpgcheck yum-priorities", 
            "title": "Install the Yum priorities package"
        }, 
        {
            "location": "/Frontier_Squid/squid/#install-osg-repositories", 
            "text": "If you are upgrading from OSG 3.1 (or 3.2) to OSG 3.2 (or 3.3), remove the old OSG repository definition files and clean the Yum cache:   [root@client ~]$ yum clean all\n[root@client ~]$ rpm -e osg-release  This step ensures that local changes to  *.repo  files will not block the installation of the new OSG repositories. After this step,  *.repo  files that have been changed will exist in  /etc/yum.repos.d/  with the  *.rpmsave  extension. After installing the new OSG repositories (the next step) you may want to apply any changes made in the  *.rpmsave  files to the new  *.repo  files.\n2. Install the OSG repositories using one of the following methods depending on your EL version:\n..1. For EL versions greater than EL5, install the files directly from  repo.grid.iu :  [root@client ~]$ rpm -Uvh URL  where URL is one of the following:     Series  EL6 URL (for RHEL 6, CentOS 6, or SL 6)  EL7 URL (for RHEL 7, CentOS 7, or SL 7)      OSG 3.2  https://repo.grid.iu.edu/osg/3.2/osg-3.2-el6-release-latest.rpm  N/A    OSG 3.3  https://repo.grid.iu.edu/osg/3.3/osg-3.3-el6-release-latest.rpm  https://repo.grid.iu.edu/osg/3.3/osg-3.3-el7-release-latest.rpm", 
            "title": "Install OSG Repositories"
        }, 
        {
            "location": "/Frontier_Squid/squid/#installing-frontier-squid", 
            "text": "After meeting the requirements in the previous section, install\nfrontier-squid with this command:   [root@client ~]# yum install frontier-squid  Then enable it to start at boot time with this command:   [root@client ~]# chkconfig frontier-squid on", 
            "title": "Installing Frontier Squid"
        }, 
        {
            "location": "/Frontier_Squid/squid/#configuring-frontier-squid", 
            "text": "", 
            "title": "Configuring Frontier Squid"
        }, 
        {
            "location": "/Frontier_Squid/squid/#configuring-the-frontier-squid-service", 
            "text": "To configure the Frontier Squid service itself:   Follow the  original Frontier Squid\n    documentation ,\n    in  the Configuration\n    section \\\n    Note:  An important difference between the standard Squid\n    software and the Frontier Squid variant is that Frontier Squid\n    changes are in  /etc/squid/customize.sh  instead of\n     /etc/squid/squid.conf .  Enable, start, and test the service (as described below)  Enable WLCG monitoring as described in the  upstream documentation\n    on enabling\n    monitoring \n    and  register the squid in\n    OIM .", 
            "title": "Configuring the Frontier Squid Service"
        }, 
        {
            "location": "/Frontier_Squid/squid/#configuring-the-osg-ce", 
            "text": "To configure the OSG Compute Element (CE) to know about your Frontier\nSquid service:    On your CE host, edit  /etc/osg/config.d/01-squid.ini   Make sure that  enabled  is set to  True  Set  location  to the hostname and port of your Frontier Squid\n    service (e.g.,  my.squid.host.edu:3128 )  Leave the other settings at  DEFAULT  unless you have specific\n    reasons to change them     Run  osg-configure  to propagate the changes on your CE Note:  You may want to finish other CE configuration tasks before running  osg-configure . Just be sure to run it once before starting CE services.", 
            "title": "Configuring the OSG CE"
        }, 
        {
            "location": "/Frontier_Squid/squid/#starting-and-stopping-the-frontier-squid-service", 
            "text": "Starting frontier-squid:  [root@client ~]# service frontier-squid start  Stopping frontier-squid:  [root@client ~]# service frontier-squid stop", 
            "title": "Starting and Stopping the Frontier Squid Service"
        }, 
        {
            "location": "/Frontier_Squid/squid/#testing-frontier-squid", 
            "text": "As any user on another computer, do the following (where\n%RED%yoursquid.your.domain  is\nthe fully qualified domain name of your squid server):  [user@client ~]$ export http_proxy=http://yoursquid.your.domain:3128\n[user@client ~]$ wget -qdO/dev/null http://frontier.cern.ch 2 1|grep X-Cache\nX-Cache: MISS from yoursquid.your.domain\n[user@client ~]$ wget -qdO/dev/null http://frontier.cern.ch 2 1|grep X-Cache\nX-Cache: HIT from yoursquid.your.domain  If the grep doesn\u2019t print anything, try removing it from the pipeline to\nsee if errors are obvious. If the second try says MISS again, something\nis probably wrong with the squid cache writes.  If your squid will be supporting the Frontier application, it is also\ngood to do the test in the  upstream documentation Testing the\ninstallation\nsection .", 
            "title": "Testing Frontier Squid"
        }, 
        {
            "location": "/Frontier_Squid/squid/#frontier-squid-log-files", 
            "text": "Log file contents are explained in the  upstream documentation Log file\ncontents\nsection .", 
            "title": "Frontier Squid Log Files"
        }, 
        {
            "location": "/Frontier_Squid/squid/#getting-help", 
            "text": "To get assistance please use  Help\nProcedure .", 
            "title": "Getting Help"
        }, 
        {
            "location": "/Other/cvmfs/", 
            "text": "Install CVMFS\n\n\nHere we describe how to install the \nCVMFS\n (Cern-VM file system) client.\nThis document is intended for system administrators who wish to install this client to have access to files distributed\nby cvmfs servers via HTTP.\n\n\n\n\nApplicable versions\n\n\nThe applicable software versions for this document are OSG Version \n= 3.2.22.\nThe version of cvmfs installed should be \n= 2.1.20-1.osg or greater.\n\n\n\n\nRequirements\n\n\nHost and OS\n\n\n\n\nOS is RedHat 5, 6, 7 or variants\n\n\nroot\n access\n\n\nautofs\n should be installed\n\n\nfuse\n should be installed (or will be as part of the installation)\n\n\nSufficient (~20GB+20%) cache space reserved, preferably in a separate filesystem (details \nbelow\n)\n\n\n\n\nUsers and Groups\n\n\nThis installation will create one user unless it already exists:\n\n\n\n\n\n\n\n\nUser\n\n\nComment\n\n\n\n\n\n\n\n\n\n\ncvmfs\n\n\nCernVM-FS service account\n\n\n\n\n\n\n\n\nThe installation will also create a cvmfs group and default the cvmfs user to that group. In addition, if the fuse rpm is not for some reason already installed, installing cvmfs will also install fuse and that will create another group:\n\n\n\n\n\n\n\n\nGroup\n\n\nComment\n\n\nGroup members\n\n\n\n\n\n\n\n\n\n\ncvmfs\n\n\nCernVM-FS service account\n\n\nnone\n\n\n\n\n\n\nfuse\n\n\nFUSE service account\n\n\ncvmfs\n\n\n\n\n\n\n\n\nNetworking\n\n\nYou will need network access to a local squid server such as the \nsquid distributed by OSG\n. The squid will need out-bound access to cvmfs stratum 1 servers.\n\n\nUpgrading\n\n\nWhen upgrading to cvmfs 2.1.20, delete the setting of \nCVMFS_SERVER_URL\n in \n/etc/cvmfs/domain.d/cern.ch.local\n. If that's the only thing in the file (which is likely) then delete the whole file.\n\n\nWhen upgrading from a cvmfs 2.0.X version, all \n/cvmfs\n repositories must be unmounted in order to upgrade. When upgrading between 2.1.X versions, repositories may be mounted.\n\n\nNote that version 2.1 removed the \n/etc/init.d/cvmfs\n script. Starting it didn't actually do anything anyway (it is automatically starts when mounted), and the stop function has been moved to \ncvmfs_config umount\n.\nThe \nrestartautofs\n function is instead done by \nservice autofs restart\n. The \nprobe\n function has also been moved to \ncvmfs_config probe\n. Since 2.1.X enables shared cache by default, so in order to reclaim the previous cache space when upgrading from 2.0.X you must manually remove the old caches. For example\n\n\nrm -rf /var/cache/cvmfs/*.*\n\n\n\n\nInstall Instructions\n\n\nPrior to installing CVMFS, make sure the \nyum repositories\n are correctly configured for OSG.\n\n\nThe following will install cvmfs from the OSG repository. It will also install cern public keys as well as fuse and autofs if you do not have them, and it will install the configuration for the OSG CVMFS distribution, OASIS.\n\n\nyum install osg-oasis\n\n\n\n\nCreate or edit \n/etc/fuse.conf\n. It should contain the following in order to allow fuse to do proper file ownership:\n\n\nuser_allow_other\n\n\n\n\nCreate or edit \n/etc/auto.master\n. It should contain the following in order to allow cvmfs to automount:\n\n\n/cvmfs /etc/auto.cvmfs\n\n\n\n\nRestart autofs to make the change take effect:\n\n\nservice autofs restart\nStopping automount:                       [  OK  ]\nStarting automount:                       [  OK  ]\n\n\n\n\nCreate or edit \n/etc/cvmfs/default.local\n, a file that controls the cvmfs configuration.\nBelow is a sample configuration, \nbut please note\n that you will need to customize this for your site. (In particular, the \nCVMFS_HTTP_PROXY\n line below only works within the .fnal.gov domain.)\n\n\nCVMFS_REPOSITORIES=\n`echo $((echo oasis.opensciencegrid.org;echo cms.cern.ch;ls /cvmfs)|sort -u)|tr ' ' ,`\n\nCVMFS_CACHE_BASE=/var/cache/cvmfs\nCVMFS_QUOTA_LIMIT=20000\nCVMFS_HTTP_PROXY=\nhttp://squid.fnal.gov:3128\n\n\n\n\n\nCVMFS 2.1 by default allows any repository to be mounted. The recommended CVMFS_REPOSITORIES setting is what it is above so that tools such as \ncvmfs_config\n and \ncvmfs_talk\n that use known repositories will use two common repositories plus any additional that have been mounted. You may want to choose a different set of always-known repositories. A full list of cern.ch repositories is found at \nhttp://cernvm.cern.ch/portal/cvmfs/examples\n. The only opensciencegrid.org repository is currently oasis.\n\n\nSet up a list of cvmfs HTTP proxies to retrieve from in CVMFS_HTTP_PROXY. Vertical bars separating proxies means to load balance between them and try them all before continuing. A semicolon between proxies means to try that one only after the previous ones have failed. A special proxy called DIRECT can be placed last in the list to indicate directly connecting to servers if all other proxies fail. This is acceptable for small sites but discouraged for large sites because of the potential load that could be put upon the stratum one servers.\n\n\nSet up the cache limit in \nCVMFS_QUOTA_LIMIT\n (in MB). Recommended for most applications is 20GB. This is the combined limit for all repositories. This cache will be stored in \n$CVMFS_CACHE_BASE\n. Make sure that at least 20% more than that amount of space stays available for cvmfs in that filesystem. This is very important, since if that space is not available it can cause many I/O errors and application crashes. Many system administrators choose to put the cache space in a separate filesystem.\n\n\nVerifying cvmfs\n\n\nAfter CVMFS is installed, you should be able to see the \n/cvmfs\n directory. But note that it will initially appear to be empty:\n\n\n$ ls /cvmfs\n\n\n\n\n\nDirectories within \n/cvmfs\n will not be mounted until you examine them. For instance:\n\n\n# ls -l /cvmfs/atlas.cern.ch\n/cvmfs/atlas.cern.ch:\ntotal 5\ndrwxr-xr-x 1 cvmfs cvmfs 4096 Mar  5  2012 repo\n# ls -l /cvmfs/oasis.opensciencegrid.org/cmssoft\n/cvmfs/oasis.opensciencegrid.org/cmssoft:\ntotal 1\nlrwxrwxrwx 1 cvmfs cvmfs 18 May 28 10:33 cms -\n /cvmfs/cms.cern.ch\n# ls -l /cvmfs/glast.egi.eu\ntotal 5\ndrwxr-xr-x 9 cvmfs cvmfs 4096 Feb  7  2014 glast\n# ls /cvmfs\natlas.cern.ch  cms.cern.ch  glast.egi.eu  oasis.opensciencegrid.org\n\n\n\n\nTroubleshooting problems\n\n\nIf no directories exist under \n/cvmfs/\n, you can try the following steps to debug:\n\n\n\n\nMount it manually \nmkdir /mnt/cvmfs\n then \nmount -t cvmfs REPOSITORYNAME /mnt/cvmfs\n where \nREPOSITORYNAME\n is the repository, for example \noasis.opensciencegrid.org\n. If this works, then cvmfs is working, but there is a problem with automount.\n\n\nIf that doesn't work and doesn't give any explanatory errors, try \ncvmfs_config chksetup\n or \ncvmfs_config showconfig REPOSITORYNAME\n to verify your setup.\n\n\nIf chksetup reports access problems to proxies, it may be caused by access control settings in the squids.\n\n\nIf you have changed settings in \n/etc/cvmfs/default.local\n, and they do not seem to be taking effect, note that there are other configuration files that can override the settings. See the comments at the beginning of \n/etc/cvmfs/default.conf\n regarding the order in which configuration files are evaluated and look for old files that may have been left from a previous installation.\n\n\nMore things to try are in the \nupstream documentation\n.\n\n\n\n\nStarting and Stopping services\n\n\nOnce it is set up, cvmfs is always automatically started when one of the repositories are accessed.\n\n\ncvmfs can be stopped via:\n\n\n# cvmfs_config umount\nUnmounting /cvmfs/atlas.cern.ch: OK\nUnmounting /cvmfs/oasis.opensciencegrid.org: OK\nUnmounting /cvmfs/cms.cern.ch: OK\nUnmounting /cvmfs/glast.egi.eu: OK\n\n\n\n\nScreendump of Install\n\n\n[root@fermicloud044 ~]# rpm -Uvh http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm\nRetrieving http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm\nPreparing...                ########################################### [100%]\n   1:epel-release           ########################################### [100%]\n[root@fermicloud044 ~]# yum install yum-priorities\nLoaded plugins: security\nepel/metalink                                            |  15 kB     00:00     \nepel                                                     | 4.4 kB     00:00     \nepel/primary_db                                          | 6.5 MB     00:02     \nSetting up Install Process\nResolving Dependencies\n--\n Running transaction check\n---\n Package yum-plugin-priorities.noarch 0:1.1.30-14.el6 will be installed\n--\n Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package                     Arch         Version               Repository\n                                                                           Size\n================================================================================\nInstalling:\n yum-plugin-priorities       noarch       1.1.30-14.el6         slf        21 k\n\nTransaction Summary\n================================================================================\nInstall       1 Package(s)\n\nTotal download size: 21 k\nInstalled size: 28 k\nIs this ok [y/N]: y\nDownloading Packages:\nyum-plugin-priorities-1.1.30-14.el6.noarch.rpm           |  21 kB     00:00     \nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n  Installing : yum-plugin-priorities-1.1.30-14.el6.noarch                   1/1 \n  Verifying  : yum-plugin-priorities-1.1.30-14.el6.noarch                   1/1 \n\nInstalled:\n  yum-plugin-priorities.noarch 0:1.1.30-14.el6                                  \n\nComplete!\n[root@fermicloud044 ~]# grep plugins /etc/yum.conf\nplugins=1\n[root@fermicloud044 ~]# rpm -Uvh http://repo.grid.iu.edu/osg/3.2/osg-3.2-el6-release-latest.rpm\nRetrieving http://repo.grid.iu.edu/osg/3.2/osg-3.2-el6-release-latest.rpm\nwarning: /var/tmp/rpm-tmp.C1YSbQ: Header V3 DSA/SHA1 Signature, key ID 824b8603: NOKEY\nPreparing...                ########################################### [100%]\n   1:osg-release            ########################################### [100%]\n[root@fermicloud044 ~]# yum install osg-oasis\nLoaded plugins: priorities, security\nosg                                                      | 1.9 kB     00:00     \nosg/primary_db                                           | 1.9 MB     00:00     \n342 packages excluded due to repository priority protections\nSetting up Install Process\nResolving Dependencies\n--\n Running transaction check\n---\n Package osg-oasis.noarch 0:5-1.osg32.el6 will be installed\n--\n Processing Dependency: cvmfs-config-osg \n= 1.1 for package: osg-oasis-5-1.osg32.el6.noarch\n--\n Processing Dependency: cvmfs \n= 2.1.20 for package: osg-oasis-5-1.osg32.el6.noarch\n--\n Running transaction check\n---\n Package cvmfs.x86_64 0:2.1.20-1.osg32.el6 will be installed\n--\n Processing Dependency: libfuse.so.2(FUSE_2.4)(64bit) for package: cvmfs-2.1.20-1.osg32.el6.x86_64\n--\n Processing Dependency: /usr/sbin/semanage for package: cvmfs-2.1.20-1.osg32.el6.x86_64\n--\n Processing Dependency: libfuse.so.2(FUSE_2.6)(64bit) for package: cvmfs-2.1.20-1.osg32.el6.x86_64\n--\n Processing Dependency: fuse-libs for package: cvmfs-2.1.20-1.osg32.el6.x86_64\n--\n Processing Dependency: gdb for package: cvmfs-2.1.20-1.osg32.el6.x86_64\n--\n Processing Dependency: libfuse.so.2(FUSE_2.5)(64bit) for package: cvmfs-2.1.20-1.osg32.el6.x86_64\n--\n Processing Dependency: fuse for package: cvmfs-2.1.20-1.osg32.el6.x86_64\n--\n Processing Dependency: libfuse.so.2()(64bit) for package: cvmfs-2.1.20-1.osg32.el6.x86_64\n---\n Package cvmfs-config-osg.noarch 0:1.1-5.osg32.el6 will be installed\n--\n Running transaction check\n---\n Package fuse.x86_64 0:2.8.3-4.el6 will be installed\n---\n Package fuse-libs.x86_64 0:2.8.3-4.el6 will be installed\n---\n Package gdb.x86_64 0:7.2-60.el6 will be installed\n---\n Package policycoreutils-python.x86_64 0:2.0.83-19.30.el6 will be installed\n--\n Processing Dependency: libsemanage-python \n= 2.0.43-4 for package: policycoreutils-python-2.0.83-19.30.el6.x86_64\n--\n Processing Dependency: audit-libs-python \n= 1.4.2-1 for package: policycoreutils-python-2.0.83-19.30.el6.x86_64\n--\n Processing Dependency: setools-libs-python for package: policycoreutils-python-2.0.83-19.30.el6.x86_64\n--\n Processing Dependency: libselinux-python for package: policycoreutils-python-2.0.83-19.30.el6.x86_64\n--\n Processing Dependency: libcgroup for package: policycoreutils-python-2.0.83-19.30.el6.x86_64\n--\n Running transaction check\n---\n Package audit-libs-python.x86_64 0:2.2-2.el6 will be installed\n---\n Package libcgroup.x86_64 0:0.37-7.el6 will be installed\n---\n Package libselinux-python.x86_64 0:2.0.94-5.3.el6 will be installed\n---\n Package libsemanage-python.x86_64 0:2.0.43-4.2.el6 will be installed\n---\n Package setools-libs-python.x86_64 0:3.3.7-4.el6 will be installed\n--\n Processing Dependency: setools-libs = 3.3.7-4.el6 for package: setools-libs-python-3.3.7-4.el6.x86_64\n--\n Processing Dependency: libqpol.so.1(VERS_1.3)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--\n Processing Dependency: libpoldiff.so.1(VERS_1.3)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--\n Processing Dependency: libapol.so.4(VERS_4.1)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--\n Processing Dependency: libqpol.so.1(VERS_1.2)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--\n Processing Dependency: libqpol.so.1(VERS_1.4)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--\n Processing Dependency: libseaudit.so.4(VERS_4.2)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--\n Processing Dependency: libpoldiff.so.1(VERS_1.2)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--\n Processing Dependency: libsefs.so.4(VERS_4.0)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--\n Processing Dependency: libapol.so.4(VERS_4.0)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--\n Processing Dependency: libseaudit.so.4(VERS_4.1)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--\n Processing Dependency: libseaudit.so.4()(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--\n Processing Dependency: libqpol.so.1()(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--\n Processing Dependency: libapol.so.4()(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--\n Processing Dependency: libpoldiff.so.1()(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--\n Processing Dependency: libsefs.so.4()(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--\n Running transaction check\n---\n Package setools-libs.x86_64 0:3.3.7-4.el6 will be installed\n--\n Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package                     Arch        Version                 Repository\n                                                                           Size\n================================================================================\nInstalling:\n osg-oasis                   noarch      5-1.osg32.el6           osg      2.4 k\nInstalling for dependencies:\n audit-libs-python           x86_64      2.2-2.el6               slf       58 k\n cvmfs                       x86_64      2.1.20-1.osg32.el6      osg      5.9 M\n cvmfs-config-osg            noarch      1.1-5.osg32.el6         osg      8.0 k\n fuse                        x86_64      2.8.3-4.el6             slf       70 k\n fuse-libs                   x86_64      2.8.3-4.el6             slf       73 k\n gdb                         x86_64      7.2-60.el6              slf      2.3 M\n libcgroup                   x86_64      0.37-7.el6              slf      110 k\n libselinux-python           x86_64      2.0.94-5.3.el6          slf      201 k\n libsemanage-python          x86_64      2.0.43-4.2.el6          slf       80 k\n policycoreutils-python      x86_64      2.0.83-19.30.el6        slf      341 k\n setools-libs                x86_64      3.3.7-4.el6             slf      399 k\n setools-libs-python         x86_64      3.3.7-4.el6             slf      221 k\n\nTransaction Summary\n================================================================================\nInstall      13 Package(s)\n\nTotal download size: 9.8 M\nInstalled size: 35 M\nIs this ok [y/N]: y\nDownloading Packages:\n(1/13): audit-libs-python-2.2-2.el6.x86_64.rpm           |  58 kB     00:00     \n(2/13): cvmfs-2.1.20-1.osg32.el6.x86_64.rpm              | 5.9 MB     00:00     \n(3/13): cvmfs-config-osg-1.1-5.osg32.el6.noarch.rpm      | 8.0 kB     00:00     \n(4/13): fuse-2.8.3-4.el6.x86_64.rpm                      |  70 kB     00:00     \n(5/13): fuse-libs-2.8.3-4.el6.x86_64.rpm                 |  73 kB     00:00     \n(6/13): gdb-7.2-60.el6.x86_64.rpm                        | 2.3 MB     00:00     \n(7/13): libcgroup-0.37-7.el6.x86_64.rpm                  | 110 kB     00:00     \n(8/13): libselinux-python-2.0.94-5.3.el6.x86_64.rpm      | 201 kB     00:00     \n(9/13): libsemanage-python-2.0.43-4.2.el6.x86_64.rpm     |  80 kB     00:00     \n(10/13): osg-oasis-5-1.osg32.el6.noarch.rpm              | 2.4 kB     00:00     \n(11/13): policycoreutils-python-2.0.83-19.30.el6.x86_64. | 341 kB     00:00     \n(12/13): setools-libs-3.3.7-4.el6.x86_64.rpm             | 399 kB     00:00     \n(13/13): setools-libs-python-3.3.7-4.el6.x86_64.rpm      | 221 kB     00:00     \n--------------------------------------------------------------------------------\nTotal                                           9.7 MB/s | 9.8 MB     00:01     \nwarning: rpmts_HdrFromFdno: Header V4 DSA/SHA1 Signature, key ID 824b8603: NOKEY\nRetrieving key from file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\nImporting GPG key 0x824B8603:\n Userid : OSG Software Team (RPM Signing Key for Koji Packages) \nvdt-support@opensciencegrid.org\n\n Package: osg-release-3.2-7.osg32.el6.noarch (installed)\n From   : /etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\nIs this ok [y/N]: y\nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\nWarning: RPMDB altered outside of yum.\n  Installing : cvmfs-config-osg-1.1-5.osg32.el6.noarch                     1/13 \n  Installing : setools-libs-3.3.7-4.el6.x86_64                             2/13 \n  Installing : setools-libs-python-3.3.7-4.el6.x86_64                      3/13 \n  Installing : fuse-libs-2.8.3-4.el6.x86_64                                4/13 \n  Installing : libsemanage-python-2.0.43-4.2.el6.x86_64                    5/13 \n  Installing : gdb-7.2-60.el6.x86_64                                       6/13 \n  Installing : fuse-2.8.3-4.el6.x86_64                                     7/13 \n  Installing : audit-libs-python-2.2-2.el6.x86_64                          8/13 \n  Installing : libselinux-python-2.0.94-5.3.el6.x86_64                     9/13 \n  Installing : libcgroup-0.37-7.el6.x86_64                                10/13 \n  Installing : policycoreutils-python-2.0.83-19.30.el6.x86_64             11/13 \n  Installing : cvmfs-2.1.20-1.osg32.el6.x86_64                            12/13 \n  Installing : osg-oasis-5-1.osg32.el6.noarch                             13/13 \n  Verifying  : libcgroup-0.37-7.el6.x86_64                                 1/13 \n  Verifying  : libselinux-python-2.0.94-5.3.el6.x86_64                     2/13 \n  Verifying  : audit-libs-python-2.2-2.el6.x86_64                          3/13 \n  Verifying  : policycoreutils-python-2.0.83-19.30.el6.x86_64              4/13 \n  Verifying  : fuse-2.8.3-4.el6.x86_64                                     5/13 \n  Verifying  : cvmfs-config-osg-1.1-5.osg32.el6.noarch                     6/13 \n  Verifying  : setools-libs-python-3.3.7-4.el6.x86_64                      7/13 \n  Verifying  : gdb-7.2-60.el6.x86_64                                       8/13 \n  Verifying  : osg-oasis-5-1.osg32.el6.noarch                              9/13 \n  Verifying  : libsemanage-python-2.0.43-4.2.el6.x86_64                   10/13 \n  Verifying  : cvmfs-2.1.20-1.osg32.el6.x86_64                            11/13 \n  Verifying  : fuse-libs-2.8.3-4.el6.x86_64                               12/13 \n  Verifying  : setools-libs-3.3.7-4.el6.x86_64                            13/13 \n\nInstalled:\n  osg-oasis.noarch 0:5-1.osg32.el6                                              \n\nDependency Installed:\n  audit-libs-python.x86_64 0:2.2-2.el6                                          \n  cvmfs.x86_64 0:2.1.20-1.osg32.el6                                             \n  cvmfs-config-osg.noarch 0:1.1-5.osg32.el6                                     \n  fuse.x86_64 0:2.8.3-4.el6                                                     \n  fuse-libs.x86_64 0:2.8.3-4.el6                                                \n  gdb.x86_64 0:7.2-60.el6                                                       \n  libcgroup.x86_64 0:0.37-7.el6                                                 \n  libselinux-python.x86_64 0:2.0.94-5.3.el6                                     \n  libsemanage-python.x86_64 0:2.0.43-4.2.el6                                    \n  policycoreutils-python.x86_64 0:2.0.83-19.30.el6                              \n  setools-libs.x86_64 0:3.3.7-4.el6                                             \n  setools-libs-python.x86_64 0:3.3.7-4.el6                                      \n\nComplete!\n[root@fermicloud044 ~]# echo user_allow_other \n/etc/fuse.conf\n[root@fermicloud044 ~]# echo \n/cvmfs /etc/auto.cvmfs\n \n/etc/auto.master\n[root@fermicloud044 ~]# service autofs restart\nStopping automount:                                        [  OK  ]\nStarting automount:                                        [  OK  ]\n[root@fermicloud044 ~]# cat \n/etc/cvmfs/default.local\nCVMFS_REPOSITORIES=\n`echo $((echo oasis.opensciencegrid.org;echo cms.cern.ch;ls /cvmfs)|sort -u)|tr ' ' ,`\n\nCVMFS_CACHE_BASE=/var/cache/cvmfs\nCVMFS_QUOTA_LIMIT=20000\nCVMFS_HTTP_PROXY=\nhttp://squid.fnal.gov:3128\n\n[root@fermicloud044 ~]# ls /cvmfs\n[root@fermicloud044 ~]# ls -l /cvmfs/atlas.cern.ch\ntotal 5\ndrwxr-xr-x 6 cvmfs cvmfs 4096 Sep 12  2014 repo\n[root@fermicloud044 ~]# ls -l /cvmfs/oasis.opensciencegrid.org/cmssoft\ntotal 1\nlrwxrwxrwx 1 cvmfs cvmfs 18 Mar 11  2014 cms -\n /cvmfs/cms.cern.ch\n[root@fermicloud044 ~]# ls -l /cvmfs/glast.egi.eu\ntotal 5\ndrwxr-xr-x 9 cvmfs cvmfs 4096 Feb  7  2014 glast\n[root@fermicloud044 ~]# ls /cvmfs\natlas.cern.ch  cms.cern.ch  glast.egi.eu  oasis.opensciencegrid.org\n[root@fermicloud044 ~]# cvmfs_config umount\nUnmounting /cvmfs/atlas.cern.ch: OK\nUnmounting /cvmfs/oasis.opensciencegrid.org: OK\nUnmounting /cvmfs/cms.cern.ch: OK\nUnmounting /cvmfs/glast.egi.eu: OK\n[root@fermicloud044 ~]# \n\n\n\n\nFile Locations\n\n\n\n\n\n\n\n\nService/Process\n\n\nConfiguration File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ncvmfs\n\n\n/etc/cvmfs/default.local\n\n\ncvmfs environment settings and repository setup\n\n\n\n\n\n\nfuse\n\n\n/etc/fuse.conf\n\n\nfuse settings\n\n\n\n\n\n\nautomount\n\n\n/etc/auto.master\n\n\nautomount settings\n\n\n\n\n\n\n\n\nHow to get Help?\n\n\nIf you cannot resolve the problem, there are several ways to receive help:\n\n\n\n\nFor bug reporting and OSG-specific issues, submit a ticket to the \nGrid Operations Center\n.\n\n\nFor community support and best-effort software team support contact \n.\n\n\nFor general CERN VM FileSystem support contact \n.\n\n\n\n\nFor a full set of help options, see \nHelp Procedure\n.\n\n\nReferences\n\n\n\n\nhttp://cernvm.cern.ch/portal/filesystem/techinformation\n\n\nhttps://ecsft.cern.ch/dist/cvmfs/cvmfstech-2.1-6.pdf", 
            "title": "CVMFS"
        }, 
        {
            "location": "/Other/cvmfs/#install-cvmfs", 
            "text": "Here we describe how to install the  CVMFS  (Cern-VM file system) client.\nThis document is intended for system administrators who wish to install this client to have access to files distributed\nby cvmfs servers via HTTP.   Applicable versions  The applicable software versions for this document are OSG Version  = 3.2.22.\nThe version of cvmfs installed should be  = 2.1.20-1.osg or greater.", 
            "title": "Install CVMFS"
        }, 
        {
            "location": "/Other/cvmfs/#requirements", 
            "text": "", 
            "title": "Requirements"
        }, 
        {
            "location": "/Other/cvmfs/#host-and-os", 
            "text": "OS is RedHat 5, 6, 7 or variants  root  access  autofs  should be installed  fuse  should be installed (or will be as part of the installation)  Sufficient (~20GB+20%) cache space reserved, preferably in a separate filesystem (details  below )", 
            "title": "Host and OS"
        }, 
        {
            "location": "/Other/cvmfs/#users-and-groups", 
            "text": "This installation will create one user unless it already exists:     User  Comment      cvmfs  CernVM-FS service account     The installation will also create a cvmfs group and default the cvmfs user to that group. In addition, if the fuse rpm is not for some reason already installed, installing cvmfs will also install fuse and that will create another group:     Group  Comment  Group members      cvmfs  CernVM-FS service account  none    fuse  FUSE service account  cvmfs", 
            "title": "Users and Groups"
        }, 
        {
            "location": "/Other/cvmfs/#networking", 
            "text": "You will need network access to a local squid server such as the  squid distributed by OSG . The squid will need out-bound access to cvmfs stratum 1 servers.", 
            "title": "Networking"
        }, 
        {
            "location": "/Other/cvmfs/#upgrading", 
            "text": "When upgrading to cvmfs 2.1.20, delete the setting of  CVMFS_SERVER_URL  in  /etc/cvmfs/domain.d/cern.ch.local . If that's the only thing in the file (which is likely) then delete the whole file.  When upgrading from a cvmfs 2.0.X version, all  /cvmfs  repositories must be unmounted in order to upgrade. When upgrading between 2.1.X versions, repositories may be mounted.  Note that version 2.1 removed the  /etc/init.d/cvmfs  script. Starting it didn't actually do anything anyway (it is automatically starts when mounted), and the stop function has been moved to  cvmfs_config umount .\nThe  restartautofs  function is instead done by  service autofs restart . The  probe  function has also been moved to  cvmfs_config probe . Since 2.1.X enables shared cache by default, so in order to reclaim the previous cache space when upgrading from 2.0.X you must manually remove the old caches. For example  rm -rf /var/cache/cvmfs/*.*", 
            "title": "Upgrading"
        }, 
        {
            "location": "/Other/cvmfs/#install-instructions", 
            "text": "Prior to installing CVMFS, make sure the  yum repositories  are correctly configured for OSG.  The following will install cvmfs from the OSG repository. It will also install cern public keys as well as fuse and autofs if you do not have them, and it will install the configuration for the OSG CVMFS distribution, OASIS.  yum install osg-oasis  Create or edit  /etc/fuse.conf . It should contain the following in order to allow fuse to do proper file ownership:  user_allow_other  Create or edit  /etc/auto.master . It should contain the following in order to allow cvmfs to automount:  /cvmfs /etc/auto.cvmfs  Restart autofs to make the change take effect:  service autofs restart\nStopping automount:                       [  OK  ]\nStarting automount:                       [  OK  ]  Create or edit  /etc/cvmfs/default.local , a file that controls the cvmfs configuration.\nBelow is a sample configuration,  but please note  that you will need to customize this for your site. (In particular, the  CVMFS_HTTP_PROXY  line below only works within the .fnal.gov domain.)  CVMFS_REPOSITORIES= `echo $((echo oasis.opensciencegrid.org;echo cms.cern.ch;ls /cvmfs)|sort -u)|tr ' ' ,` \nCVMFS_CACHE_BASE=/var/cache/cvmfs\nCVMFS_QUOTA_LIMIT=20000\nCVMFS_HTTP_PROXY= http://squid.fnal.gov:3128   CVMFS 2.1 by default allows any repository to be mounted. The recommended CVMFS_REPOSITORIES setting is what it is above so that tools such as  cvmfs_config  and  cvmfs_talk  that use known repositories will use two common repositories plus any additional that have been mounted. You may want to choose a different set of always-known repositories. A full list of cern.ch repositories is found at  http://cernvm.cern.ch/portal/cvmfs/examples . The only opensciencegrid.org repository is currently oasis.  Set up a list of cvmfs HTTP proxies to retrieve from in CVMFS_HTTP_PROXY. Vertical bars separating proxies means to load balance between them and try them all before continuing. A semicolon between proxies means to try that one only after the previous ones have failed. A special proxy called DIRECT can be placed last in the list to indicate directly connecting to servers if all other proxies fail. This is acceptable for small sites but discouraged for large sites because of the potential load that could be put upon the stratum one servers.  Set up the cache limit in  CVMFS_QUOTA_LIMIT  (in MB). Recommended for most applications is 20GB. This is the combined limit for all repositories. This cache will be stored in  $CVMFS_CACHE_BASE . Make sure that at least 20% more than that amount of space stays available for cvmfs in that filesystem. This is very important, since if that space is not available it can cause many I/O errors and application crashes. Many system administrators choose to put the cache space in a separate filesystem.", 
            "title": "Install Instructions"
        }, 
        {
            "location": "/Other/cvmfs/#verifying-cvmfs", 
            "text": "After CVMFS is installed, you should be able to see the  /cvmfs  directory. But note that it will initially appear to be empty:  $ ls /cvmfs  Directories within  /cvmfs  will not be mounted until you examine them. For instance:  # ls -l /cvmfs/atlas.cern.ch\n/cvmfs/atlas.cern.ch:\ntotal 5\ndrwxr-xr-x 1 cvmfs cvmfs 4096 Mar  5  2012 repo\n# ls -l /cvmfs/oasis.opensciencegrid.org/cmssoft\n/cvmfs/oasis.opensciencegrid.org/cmssoft:\ntotal 1\nlrwxrwxrwx 1 cvmfs cvmfs 18 May 28 10:33 cms -  /cvmfs/cms.cern.ch\n# ls -l /cvmfs/glast.egi.eu\ntotal 5\ndrwxr-xr-x 9 cvmfs cvmfs 4096 Feb  7  2014 glast\n# ls /cvmfs\natlas.cern.ch  cms.cern.ch  glast.egi.eu  oasis.opensciencegrid.org", 
            "title": "Verifying cvmfs"
        }, 
        {
            "location": "/Other/cvmfs/#troubleshooting-problems", 
            "text": "If no directories exist under  /cvmfs/ , you can try the following steps to debug:   Mount it manually  mkdir /mnt/cvmfs  then  mount -t cvmfs REPOSITORYNAME /mnt/cvmfs  where  REPOSITORYNAME  is the repository, for example  oasis.opensciencegrid.org . If this works, then cvmfs is working, but there is a problem with automount.  If that doesn't work and doesn't give any explanatory errors, try  cvmfs_config chksetup  or  cvmfs_config showconfig REPOSITORYNAME  to verify your setup.  If chksetup reports access problems to proxies, it may be caused by access control settings in the squids.  If you have changed settings in  /etc/cvmfs/default.local , and they do not seem to be taking effect, note that there are other configuration files that can override the settings. See the comments at the beginning of  /etc/cvmfs/default.conf  regarding the order in which configuration files are evaluated and look for old files that may have been left from a previous installation.  More things to try are in the  upstream documentation .", 
            "title": "Troubleshooting problems"
        }, 
        {
            "location": "/Other/cvmfs/#starting-and-stopping-services", 
            "text": "Once it is set up, cvmfs is always automatically started when one of the repositories are accessed.  cvmfs can be stopped via:  # cvmfs_config umount\nUnmounting /cvmfs/atlas.cern.ch: OK\nUnmounting /cvmfs/oasis.opensciencegrid.org: OK\nUnmounting /cvmfs/cms.cern.ch: OK\nUnmounting /cvmfs/glast.egi.eu: OK", 
            "title": "Starting and Stopping services"
        }, 
        {
            "location": "/Other/cvmfs/#screendump-of-install", 
            "text": "[root@fermicloud044 ~]# rpm -Uvh http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm\nRetrieving http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm\nPreparing...                ########################################### [100%]\n   1:epel-release           ########################################### [100%]\n[root@fermicloud044 ~]# yum install yum-priorities\nLoaded plugins: security\nepel/metalink                                            |  15 kB     00:00     \nepel                                                     | 4.4 kB     00:00     \nepel/primary_db                                          | 6.5 MB     00:02     \nSetting up Install Process\nResolving Dependencies\n--  Running transaction check\n---  Package yum-plugin-priorities.noarch 0:1.1.30-14.el6 will be installed\n--  Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package                     Arch         Version               Repository\n                                                                           Size\n================================================================================\nInstalling:\n yum-plugin-priorities       noarch       1.1.30-14.el6         slf        21 k\n\nTransaction Summary\n================================================================================\nInstall       1 Package(s)\n\nTotal download size: 21 k\nInstalled size: 28 k\nIs this ok [y/N]: y\nDownloading Packages:\nyum-plugin-priorities-1.1.30-14.el6.noarch.rpm           |  21 kB     00:00     \nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n  Installing : yum-plugin-priorities-1.1.30-14.el6.noarch                   1/1 \n  Verifying  : yum-plugin-priorities-1.1.30-14.el6.noarch                   1/1 \n\nInstalled:\n  yum-plugin-priorities.noarch 0:1.1.30-14.el6                                  \n\nComplete!\n[root@fermicloud044 ~]# grep plugins /etc/yum.conf\nplugins=1\n[root@fermicloud044 ~]# rpm -Uvh http://repo.grid.iu.edu/osg/3.2/osg-3.2-el6-release-latest.rpm\nRetrieving http://repo.grid.iu.edu/osg/3.2/osg-3.2-el6-release-latest.rpm\nwarning: /var/tmp/rpm-tmp.C1YSbQ: Header V3 DSA/SHA1 Signature, key ID 824b8603: NOKEY\nPreparing...                ########################################### [100%]\n   1:osg-release            ########################################### [100%]\n[root@fermicloud044 ~]# yum install osg-oasis\nLoaded plugins: priorities, security\nosg                                                      | 1.9 kB     00:00     \nosg/primary_db                                           | 1.9 MB     00:00     \n342 packages excluded due to repository priority protections\nSetting up Install Process\nResolving Dependencies\n--  Running transaction check\n---  Package osg-oasis.noarch 0:5-1.osg32.el6 will be installed\n--  Processing Dependency: cvmfs-config-osg  = 1.1 for package: osg-oasis-5-1.osg32.el6.noarch\n--  Processing Dependency: cvmfs  = 2.1.20 for package: osg-oasis-5-1.osg32.el6.noarch\n--  Running transaction check\n---  Package cvmfs.x86_64 0:2.1.20-1.osg32.el6 will be installed\n--  Processing Dependency: libfuse.so.2(FUSE_2.4)(64bit) for package: cvmfs-2.1.20-1.osg32.el6.x86_64\n--  Processing Dependency: /usr/sbin/semanage for package: cvmfs-2.1.20-1.osg32.el6.x86_64\n--  Processing Dependency: libfuse.so.2(FUSE_2.6)(64bit) for package: cvmfs-2.1.20-1.osg32.el6.x86_64\n--  Processing Dependency: fuse-libs for package: cvmfs-2.1.20-1.osg32.el6.x86_64\n--  Processing Dependency: gdb for package: cvmfs-2.1.20-1.osg32.el6.x86_64\n--  Processing Dependency: libfuse.so.2(FUSE_2.5)(64bit) for package: cvmfs-2.1.20-1.osg32.el6.x86_64\n--  Processing Dependency: fuse for package: cvmfs-2.1.20-1.osg32.el6.x86_64\n--  Processing Dependency: libfuse.so.2()(64bit) for package: cvmfs-2.1.20-1.osg32.el6.x86_64\n---  Package cvmfs-config-osg.noarch 0:1.1-5.osg32.el6 will be installed\n--  Running transaction check\n---  Package fuse.x86_64 0:2.8.3-4.el6 will be installed\n---  Package fuse-libs.x86_64 0:2.8.3-4.el6 will be installed\n---  Package gdb.x86_64 0:7.2-60.el6 will be installed\n---  Package policycoreutils-python.x86_64 0:2.0.83-19.30.el6 will be installed\n--  Processing Dependency: libsemanage-python  = 2.0.43-4 for package: policycoreutils-python-2.0.83-19.30.el6.x86_64\n--  Processing Dependency: audit-libs-python  = 1.4.2-1 for package: policycoreutils-python-2.0.83-19.30.el6.x86_64\n--  Processing Dependency: setools-libs-python for package: policycoreutils-python-2.0.83-19.30.el6.x86_64\n--  Processing Dependency: libselinux-python for package: policycoreutils-python-2.0.83-19.30.el6.x86_64\n--  Processing Dependency: libcgroup for package: policycoreutils-python-2.0.83-19.30.el6.x86_64\n--  Running transaction check\n---  Package audit-libs-python.x86_64 0:2.2-2.el6 will be installed\n---  Package libcgroup.x86_64 0:0.37-7.el6 will be installed\n---  Package libselinux-python.x86_64 0:2.0.94-5.3.el6 will be installed\n---  Package libsemanage-python.x86_64 0:2.0.43-4.2.el6 will be installed\n---  Package setools-libs-python.x86_64 0:3.3.7-4.el6 will be installed\n--  Processing Dependency: setools-libs = 3.3.7-4.el6 for package: setools-libs-python-3.3.7-4.el6.x86_64\n--  Processing Dependency: libqpol.so.1(VERS_1.3)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--  Processing Dependency: libpoldiff.so.1(VERS_1.3)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--  Processing Dependency: libapol.so.4(VERS_4.1)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--  Processing Dependency: libqpol.so.1(VERS_1.2)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--  Processing Dependency: libqpol.so.1(VERS_1.4)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--  Processing Dependency: libseaudit.so.4(VERS_4.2)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--  Processing Dependency: libpoldiff.so.1(VERS_1.2)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--  Processing Dependency: libsefs.so.4(VERS_4.0)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--  Processing Dependency: libapol.so.4(VERS_4.0)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--  Processing Dependency: libseaudit.so.4(VERS_4.1)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--  Processing Dependency: libseaudit.so.4()(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--  Processing Dependency: libqpol.so.1()(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--  Processing Dependency: libapol.so.4()(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--  Processing Dependency: libpoldiff.so.1()(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--  Processing Dependency: libsefs.so.4()(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--  Running transaction check\n---  Package setools-libs.x86_64 0:3.3.7-4.el6 will be installed\n--  Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package                     Arch        Version                 Repository\n                                                                           Size\n================================================================================\nInstalling:\n osg-oasis                   noarch      5-1.osg32.el6           osg      2.4 k\nInstalling for dependencies:\n audit-libs-python           x86_64      2.2-2.el6               slf       58 k\n cvmfs                       x86_64      2.1.20-1.osg32.el6      osg      5.9 M\n cvmfs-config-osg            noarch      1.1-5.osg32.el6         osg      8.0 k\n fuse                        x86_64      2.8.3-4.el6             slf       70 k\n fuse-libs                   x86_64      2.8.3-4.el6             slf       73 k\n gdb                         x86_64      7.2-60.el6              slf      2.3 M\n libcgroup                   x86_64      0.37-7.el6              slf      110 k\n libselinux-python           x86_64      2.0.94-5.3.el6          slf      201 k\n libsemanage-python          x86_64      2.0.43-4.2.el6          slf       80 k\n policycoreutils-python      x86_64      2.0.83-19.30.el6        slf      341 k\n setools-libs                x86_64      3.3.7-4.el6             slf      399 k\n setools-libs-python         x86_64      3.3.7-4.el6             slf      221 k\n\nTransaction Summary\n================================================================================\nInstall      13 Package(s)\n\nTotal download size: 9.8 M\nInstalled size: 35 M\nIs this ok [y/N]: y\nDownloading Packages:\n(1/13): audit-libs-python-2.2-2.el6.x86_64.rpm           |  58 kB     00:00     \n(2/13): cvmfs-2.1.20-1.osg32.el6.x86_64.rpm              | 5.9 MB     00:00     \n(3/13): cvmfs-config-osg-1.1-5.osg32.el6.noarch.rpm      | 8.0 kB     00:00     \n(4/13): fuse-2.8.3-4.el6.x86_64.rpm                      |  70 kB     00:00     \n(5/13): fuse-libs-2.8.3-4.el6.x86_64.rpm                 |  73 kB     00:00     \n(6/13): gdb-7.2-60.el6.x86_64.rpm                        | 2.3 MB     00:00     \n(7/13): libcgroup-0.37-7.el6.x86_64.rpm                  | 110 kB     00:00     \n(8/13): libselinux-python-2.0.94-5.3.el6.x86_64.rpm      | 201 kB     00:00     \n(9/13): libsemanage-python-2.0.43-4.2.el6.x86_64.rpm     |  80 kB     00:00     \n(10/13): osg-oasis-5-1.osg32.el6.noarch.rpm              | 2.4 kB     00:00     \n(11/13): policycoreutils-python-2.0.83-19.30.el6.x86_64. | 341 kB     00:00     \n(12/13): setools-libs-3.3.7-4.el6.x86_64.rpm             | 399 kB     00:00     \n(13/13): setools-libs-python-3.3.7-4.el6.x86_64.rpm      | 221 kB     00:00     \n--------------------------------------------------------------------------------\nTotal                                           9.7 MB/s | 9.8 MB     00:01     \nwarning: rpmts_HdrFromFdno: Header V4 DSA/SHA1 Signature, key ID 824b8603: NOKEY\nRetrieving key from file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\nImporting GPG key 0x824B8603:\n Userid : OSG Software Team (RPM Signing Key for Koji Packages)  vdt-support@opensciencegrid.org \n Package: osg-release-3.2-7.osg32.el6.noarch (installed)\n From   : /etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\nIs this ok [y/N]: y\nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\nWarning: RPMDB altered outside of yum.\n  Installing : cvmfs-config-osg-1.1-5.osg32.el6.noarch                     1/13 \n  Installing : setools-libs-3.3.7-4.el6.x86_64                             2/13 \n  Installing : setools-libs-python-3.3.7-4.el6.x86_64                      3/13 \n  Installing : fuse-libs-2.8.3-4.el6.x86_64                                4/13 \n  Installing : libsemanage-python-2.0.43-4.2.el6.x86_64                    5/13 \n  Installing : gdb-7.2-60.el6.x86_64                                       6/13 \n  Installing : fuse-2.8.3-4.el6.x86_64                                     7/13 \n  Installing : audit-libs-python-2.2-2.el6.x86_64                          8/13 \n  Installing : libselinux-python-2.0.94-5.3.el6.x86_64                     9/13 \n  Installing : libcgroup-0.37-7.el6.x86_64                                10/13 \n  Installing : policycoreutils-python-2.0.83-19.30.el6.x86_64             11/13 \n  Installing : cvmfs-2.1.20-1.osg32.el6.x86_64                            12/13 \n  Installing : osg-oasis-5-1.osg32.el6.noarch                             13/13 \n  Verifying  : libcgroup-0.37-7.el6.x86_64                                 1/13 \n  Verifying  : libselinux-python-2.0.94-5.3.el6.x86_64                     2/13 \n  Verifying  : audit-libs-python-2.2-2.el6.x86_64                          3/13 \n  Verifying  : policycoreutils-python-2.0.83-19.30.el6.x86_64              4/13 \n  Verifying  : fuse-2.8.3-4.el6.x86_64                                     5/13 \n  Verifying  : cvmfs-config-osg-1.1-5.osg32.el6.noarch                     6/13 \n  Verifying  : setools-libs-python-3.3.7-4.el6.x86_64                      7/13 \n  Verifying  : gdb-7.2-60.el6.x86_64                                       8/13 \n  Verifying  : osg-oasis-5-1.osg32.el6.noarch                              9/13 \n  Verifying  : libsemanage-python-2.0.43-4.2.el6.x86_64                   10/13 \n  Verifying  : cvmfs-2.1.20-1.osg32.el6.x86_64                            11/13 \n  Verifying  : fuse-libs-2.8.3-4.el6.x86_64                               12/13 \n  Verifying  : setools-libs-3.3.7-4.el6.x86_64                            13/13 \n\nInstalled:\n  osg-oasis.noarch 0:5-1.osg32.el6                                              \n\nDependency Installed:\n  audit-libs-python.x86_64 0:2.2-2.el6                                          \n  cvmfs.x86_64 0:2.1.20-1.osg32.el6                                             \n  cvmfs-config-osg.noarch 0:1.1-5.osg32.el6                                     \n  fuse.x86_64 0:2.8.3-4.el6                                                     \n  fuse-libs.x86_64 0:2.8.3-4.el6                                                \n  gdb.x86_64 0:7.2-60.el6                                                       \n  libcgroup.x86_64 0:0.37-7.el6                                                 \n  libselinux-python.x86_64 0:2.0.94-5.3.el6                                     \n  libsemanage-python.x86_64 0:2.0.43-4.2.el6                                    \n  policycoreutils-python.x86_64 0:2.0.83-19.30.el6                              \n  setools-libs.x86_64 0:3.3.7-4.el6                                             \n  setools-libs-python.x86_64 0:3.3.7-4.el6                                      \n\nComplete!\n[root@fermicloud044 ~]# echo user_allow_other  /etc/fuse.conf\n[root@fermicloud044 ~]# echo  /cvmfs /etc/auto.cvmfs   /etc/auto.master\n[root@fermicloud044 ~]# service autofs restart\nStopping automount:                                        [  OK  ]\nStarting automount:                                        [  OK  ]\n[root@fermicloud044 ~]# cat  /etc/cvmfs/default.local\nCVMFS_REPOSITORIES= `echo $((echo oasis.opensciencegrid.org;echo cms.cern.ch;ls /cvmfs)|sort -u)|tr ' ' ,` \nCVMFS_CACHE_BASE=/var/cache/cvmfs\nCVMFS_QUOTA_LIMIT=20000\nCVMFS_HTTP_PROXY= http://squid.fnal.gov:3128 \n[root@fermicloud044 ~]# ls /cvmfs\n[root@fermicloud044 ~]# ls -l /cvmfs/atlas.cern.ch\ntotal 5\ndrwxr-xr-x 6 cvmfs cvmfs 4096 Sep 12  2014 repo\n[root@fermicloud044 ~]# ls -l /cvmfs/oasis.opensciencegrid.org/cmssoft\ntotal 1\nlrwxrwxrwx 1 cvmfs cvmfs 18 Mar 11  2014 cms -  /cvmfs/cms.cern.ch\n[root@fermicloud044 ~]# ls -l /cvmfs/glast.egi.eu\ntotal 5\ndrwxr-xr-x 9 cvmfs cvmfs 4096 Feb  7  2014 glast\n[root@fermicloud044 ~]# ls /cvmfs\natlas.cern.ch  cms.cern.ch  glast.egi.eu  oasis.opensciencegrid.org\n[root@fermicloud044 ~]# cvmfs_config umount\nUnmounting /cvmfs/atlas.cern.ch: OK\nUnmounting /cvmfs/oasis.opensciencegrid.org: OK\nUnmounting /cvmfs/cms.cern.ch: OK\nUnmounting /cvmfs/glast.egi.eu: OK\n[root@fermicloud044 ~]#", 
            "title": "Screendump of Install"
        }, 
        {
            "location": "/Other/cvmfs/#file-locations", 
            "text": "Service/Process  Configuration File  Description      cvmfs  /etc/cvmfs/default.local  cvmfs environment settings and repository setup    fuse  /etc/fuse.conf  fuse settings    automount  /etc/auto.master  automount settings", 
            "title": "File Locations"
        }, 
        {
            "location": "/Other/cvmfs/#how-to-get-help", 
            "text": "If you cannot resolve the problem, there are several ways to receive help:   For bug reporting and OSG-specific issues, submit a ticket to the  Grid Operations Center .  For community support and best-effort software team support contact  .  For general CERN VM FileSystem support contact  .   For a full set of help options, see  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/Other/cvmfs/#references", 
            "text": "http://cernvm.cern.ch/portal/filesystem/techinformation  https://ecsft.cern.ch/dist/cvmfs/cvmfstech-2.1-6.pdf", 
            "title": "References"
        }, 
        {
            "location": "/Other/gsissh/", 
            "text": "Installing GSI OpenSSH\n\n\nThis document gives instructions on installing and using the GSI OpenSSH server available in the OSG repository and configuring it so that you can use on your cluster.\n\n\nRequirements\n\n\nHost and OS\n\n\nThe GSI OpenSSH rpms will require an user account and group in order for the privilege separation to work.\n\n\nUsers and Groups\n\n\nThe RPM installation will try to create the \ngsisshd\n user and group and the \n/var/empty/gsisshd\n directory with the correct ownership if they are not present. If you are using a configuration management system or ROCKS, you should make sure that these users and groups are created before installing the RPMs to avoid potential issues. The gsisshd user should have an empty home directory. By default, this is home directory set to \n/var/empty/gsisshd\n and belongs to the \ngsisshd\n user and group. You may change it if needed to something else as long as the ownerships remain the same.\n\n\nNetworking\n\n\nYou'll find more client specific details also in the \nFirewall section\n of this document.\n\n\nInstallation procedure\n\n\nPrior to install, make sure you have:\n\n \nYum repositories correctly configured\n for OSG.\n\n \nCA certificates installed\n\n\nGSI OpenSSH Installation\n\n\nStart with installing GSI OpenSSH from the repository\n\n\nyum install gsi-openssh-server gsi-openssh-clients\n\n\n\n\nIn addition, you'll need to install CA certificates in order for GSIOpenSSH to work. You can follow the instructions below in order to install them:\n\n\nConfiguration and Operations\n\n\nUseful configuration and log files\n\n\nConfiguration Files\n\n\n\n\n\n\n\n\nService or Process\n\n\nConfiguration File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ngsisshd\n\n\n/etc/gsissh/sshd_config\n\n\nConfiguration file\n\n\n\n\n\n\ngsisshd\n\n\n/etc/sysconfig/gsisshd\n\n\nEnvironment variables for gsisshd\n\n\n\n\n\n\ngsisshd\n\n\n/etc/lcmaps.db\n\n\nLCMAPS configuration\n\n\n\n\n\n\n\n\nLog Files\n\n\n\n\n\n\n\n\nService or Process\n\n\nLog File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ngsisshd\n\n\n/var/log/messages\n\n\nAll log messages\n\n\n\n\n\n\n\n\nOther Files\n\n\n\n\n\n\n\n\nService or Process\n\n\nFile\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ngsisshd\n\n\n/etc/grid-security/hostcert.pem\n\n\nHost certificate\n\n\n\n\n\n\ngsisshd\n\n\n/etc/grid-security/hostcert.pem\n\n\nKey certificate\n\n\n\n\n\n\ngsisshd\n\n\n/etc/gsissh/ssh_host_rsa_key\n\n\nRSA Host key\n\n\n\n\n\n\n\n\nConfiguration\n\n\nConfiguration\n\n\nIn order to get a running instance of the GSI OpenSSH server, you'll\nneed to change the default configuration. However, before you go any\nfurther, you'll need to decide whether you want GSI OpenSSH to be your \nprimary ssh service or not (e.g. whether the GSI OpenSSH service will \nreplace your existing SSH service). If you choose not to replace your \nexisting service, you'll need to change the port setting in the GSI \nOpenSSH configuration to another port (e.g. 2222) so that you can run \nboth SSH services at the same time. Regardless of your choice, you \nshould probably have both services use the same host key. In order \nto do this, symlink \n/etc/gsissh/ssh_host_dsa_key\n and \n/etc/gsissh/ssh_host_rsa_key\n \nto \n/etc/ssh/ssh_host_dsa_key\n and \n/etc/ssh/ssh_host_rsa_key\n respectively. \n\n\n\n\nNote\n\n\nRegardless of the authorization method used for the user, any \naccount that will be used with GSI OpenSSH must have a shell \nassigned to it and not be locked (have ! in the password field of \n/etc/shadow\n).\n\n\n\n\nUsing a gridmap file for authorization\n\n\nIn order to use gsissh, you'll need to create mappings in your \n\n/etc/grid-security/grid-mapfile\n for the DNs that you will \nallow to login. The mappings should be entered one to a line, \nwith each line consisting of DN followed by the account the DN \nshould map to. Also, you should ensure that the \n\n/etc/grid-security/gsi-authz.conf\n file is empty or that all \nof the lines in the file are commented out using a \n#\n at the beginning of the line.\n\n\n\n\nNote\n\n\nThe mappings will not consider VOMS extensions so the first mapping that matches will be used regardless of the VO role or VO present in the users proxy\n\n\n\n\nAn example of the \n/etc/grid-security/grid-mapfile\n follows:\n\n\n/DC=org/DC=doegrids/OU=People/CN=USER NAME 123456\n useraccount\n\n\n\n\nUsing LCMAPS and GUMS for authorization\n\n\nIn order to use LCMAPS callouts with GSI OpenSSH, you'll first need to edit \n/etc/grid-security/gsi-authz.conf\n to indicate that Globus should do a GSI callout for authorization. The file should contain the following:\n\n\nglobus_mapping liblcas_lcmaps_gt4_mapping.so lcmaps_callout\n\n\n\n\nso that LCMAPS is used. Next, install the lcmaps rpms:\n\n\nyum install lcmaps lcas-lcmaps-gt4-interface\n\n\n\n\nFinally, you'll need to modify \n/etc/lcmaps.db\n so that the \ngumsclient\n entry has the correct endpoint for your gums server.\n\n\nStarting and Enabling Services\n\n\nTo start the services:\n\n\n\n\n\n\nTo start GSI OpenSSH you can use the service command, e.g.:\n\n\nservice gsisshd start\n\n\n\n\n\n\nYou should also enable the appropriate services so that they are automatically started when your system is powered on:\n\n\n\n\n\n\nTo enable OpenSSH by default on the node:\n\n\nchkconfig gsisshd on\n\n\n\n\n\n\nStopping and Disabling Services\n\n\nTo stop the services:\n\n\n\n\n\n\nTo stop OpenSSH you can use: \\\npre class=\u201crootscreen\u201d>\n\n\nservice gsisshd stop\n\n\n\n\n\n\nIn addition, you can disable services by running the following commands. However, you don't need to do this normally.\n\n\n\n\n\n\nOptionally, to disable OpenSSH:\n\n\nchkconfig gsisshd off\n\n\n\n\n\n\nTroubleshooting\n\n\nYou can get information on troubleshooting errors on the \nNCSA page\n.\n\n\nTo troubleshoot LCMAPS authorization, you can add the following to \n/etc/sysconfig/gsisshd\n and choose a higher debug level:\n\n\n# level 0: no messages, 1: errors, 2: also warnings, 3: also notices,\n#  4: also info, 5: maximum debug\nLCMAPS_DEBUG_LEVEL=2\n\n\n\n\nOutput goes to \n/var/log/messages\n by default.\n\n\nTest GSI OpenSSH\n\n\nAfter starting the \ngsisshd\n service you can check if it is running correctly\n\n\n$ grid-proxy-init\nYour identity: /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=User Name\nEnter GRID pass phrase for this identity:\nCreating proxy ............................................................................................... Done\nYour proxy is valid until: Sat Apr 23 08:18:27 2016\n$ gsissh localhost -p 2222\nLast login: Tue Sep 18 16:08:03 2012 from itb4.uchicago.edu\n$\n\n\n\n\nHow to get Help?\n\n\nTo get assistance please use this \nHelp Procedure\n.", 
            "title": "GSI-enabled SSH"
        }, 
        {
            "location": "/Other/gsissh/#installing-gsi-openssh", 
            "text": "This document gives instructions on installing and using the GSI OpenSSH server available in the OSG repository and configuring it so that you can use on your cluster.", 
            "title": "Installing GSI OpenSSH"
        }, 
        {
            "location": "/Other/gsissh/#requirements", 
            "text": "", 
            "title": "Requirements"
        }, 
        {
            "location": "/Other/gsissh/#host-and-os", 
            "text": "The GSI OpenSSH rpms will require an user account and group in order for the privilege separation to work.", 
            "title": "Host and OS"
        }, 
        {
            "location": "/Other/gsissh/#users-and-groups", 
            "text": "The RPM installation will try to create the  gsisshd  user and group and the  /var/empty/gsisshd  directory with the correct ownership if they are not present. If you are using a configuration management system or ROCKS, you should make sure that these users and groups are created before installing the RPMs to avoid potential issues. The gsisshd user should have an empty home directory. By default, this is home directory set to  /var/empty/gsisshd  and belongs to the  gsisshd  user and group. You may change it if needed to something else as long as the ownerships remain the same.", 
            "title": "Users and Groups"
        }, 
        {
            "location": "/Other/gsissh/#networking", 
            "text": "You'll find more client specific details also in the  Firewall section  of this document.", 
            "title": "Networking"
        }, 
        {
            "location": "/Other/gsissh/#installation-procedure", 
            "text": "Prior to install, make sure you have:   Yum repositories correctly configured  for OSG.   CA certificates installed", 
            "title": "Installation procedure"
        }, 
        {
            "location": "/Other/gsissh/#gsi-openssh-installation", 
            "text": "Start with installing GSI OpenSSH from the repository  yum install gsi-openssh-server gsi-openssh-clients  In addition, you'll need to install CA certificates in order for GSIOpenSSH to work. You can follow the instructions below in order to install them:", 
            "title": "GSI OpenSSH Installation"
        }, 
        {
            "location": "/Other/gsissh/#configuration-and-operations", 
            "text": "", 
            "title": "Configuration and Operations"
        }, 
        {
            "location": "/Other/gsissh/#useful-configuration-and-log-files", 
            "text": "Configuration Files     Service or Process  Configuration File  Description      gsisshd  /etc/gsissh/sshd_config  Configuration file    gsisshd  /etc/sysconfig/gsisshd  Environment variables for gsisshd    gsisshd  /etc/lcmaps.db  LCMAPS configuration     Log Files     Service or Process  Log File  Description      gsisshd  /var/log/messages  All log messages     Other Files     Service or Process  File  Description      gsisshd  /etc/grid-security/hostcert.pem  Host certificate    gsisshd  /etc/grid-security/hostcert.pem  Key certificate    gsisshd  /etc/gsissh/ssh_host_rsa_key  RSA Host key", 
            "title": "Useful configuration and log files"
        }, 
        {
            "location": "/Other/gsissh/#configuration", 
            "text": "", 
            "title": "Configuration"
        }, 
        {
            "location": "/Other/gsissh/#configuration_1", 
            "text": "In order to get a running instance of the GSI OpenSSH server, you'll\nneed to change the default configuration. However, before you go any\nfurther, you'll need to decide whether you want GSI OpenSSH to be your \nprimary ssh service or not (e.g. whether the GSI OpenSSH service will \nreplace your existing SSH service). If you choose not to replace your \nexisting service, you'll need to change the port setting in the GSI \nOpenSSH configuration to another port (e.g. 2222) so that you can run \nboth SSH services at the same time. Regardless of your choice, you \nshould probably have both services use the same host key. In order \nto do this, symlink  /etc/gsissh/ssh_host_dsa_key  and  /etc/gsissh/ssh_host_rsa_key  \nto  /etc/ssh/ssh_host_dsa_key  and  /etc/ssh/ssh_host_rsa_key  respectively.    Note  Regardless of the authorization method used for the user, any \naccount that will be used with GSI OpenSSH must have a shell \nassigned to it and not be locked (have ! in the password field of  /etc/shadow ).", 
            "title": "Configuration"
        }, 
        {
            "location": "/Other/gsissh/#using-a-gridmap-file-for-authorization", 
            "text": "In order to use gsissh, you'll need to create mappings in your  /etc/grid-security/grid-mapfile  for the DNs that you will \nallow to login. The mappings should be entered one to a line, \nwith each line consisting of DN followed by the account the DN \nshould map to. Also, you should ensure that the  /etc/grid-security/gsi-authz.conf  file is empty or that all \nof the lines in the file are commented out using a  #  at the beginning of the line.   Note  The mappings will not consider VOMS extensions so the first mapping that matches will be used regardless of the VO role or VO present in the users proxy   An example of the  /etc/grid-security/grid-mapfile  follows:  /DC=org/DC=doegrids/OU=People/CN=USER NAME 123456  useraccount", 
            "title": "Using a gridmap file for authorization"
        }, 
        {
            "location": "/Other/gsissh/#using-lcmaps-and-gums-for-authorization", 
            "text": "In order to use LCMAPS callouts with GSI OpenSSH, you'll first need to edit  /etc/grid-security/gsi-authz.conf  to indicate that Globus should do a GSI callout for authorization. The file should contain the following:  globus_mapping liblcas_lcmaps_gt4_mapping.so lcmaps_callout  so that LCMAPS is used. Next, install the lcmaps rpms:  yum install lcmaps lcas-lcmaps-gt4-interface  Finally, you'll need to modify  /etc/lcmaps.db  so that the  gumsclient  entry has the correct endpoint for your gums server.", 
            "title": "Using LCMAPS and GUMS for authorization"
        }, 
        {
            "location": "/Other/gsissh/#starting-and-enabling-services", 
            "text": "To start the services:    To start GSI OpenSSH you can use the service command, e.g.:  service gsisshd start    You should also enable the appropriate services so that they are automatically started when your system is powered on:    To enable OpenSSH by default on the node:  chkconfig gsisshd on", 
            "title": "Starting and Enabling Services"
        }, 
        {
            "location": "/Other/gsissh/#stopping-and-disabling-services", 
            "text": "To stop the services:    To stop OpenSSH you can use: \\ pre class=\u201crootscreen\u201d>  service gsisshd stop    In addition, you can disable services by running the following commands. However, you don't need to do this normally.    Optionally, to disable OpenSSH:  chkconfig gsisshd off", 
            "title": "Stopping and Disabling Services"
        }, 
        {
            "location": "/Other/gsissh/#troubleshooting", 
            "text": "You can get information on troubleshooting errors on the  NCSA page .  To troubleshoot LCMAPS authorization, you can add the following to  /etc/sysconfig/gsisshd  and choose a higher debug level:  # level 0: no messages, 1: errors, 2: also warnings, 3: also notices,\n#  4: also info, 5: maximum debug\nLCMAPS_DEBUG_LEVEL=2  Output goes to  /var/log/messages  by default.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/Other/gsissh/#test-gsi-openssh", 
            "text": "After starting the  gsisshd  service you can check if it is running correctly  $ grid-proxy-init\nYour identity: /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=User Name\nEnter GRID pass phrase for this identity:\nCreating proxy ............................................................................................... Done\nYour proxy is valid until: Sat Apr 23 08:18:27 2016\n$ gsissh localhost -p 2222\nLast login: Tue Sep 18 16:08:03 2012 from itb4.uchicago.edu\n$", 
            "title": "Test GSI OpenSSH"
        }, 
        {
            "location": "/Other/gsissh/#how-to-get-help", 
            "text": "To get assistance please use this  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/Other/wn/", 
            "text": "Installing and Using the Worker Node Client From RPMs\n\n\nThe \nOSG Worker Node Client\n is a collection of software components that is expected to be added to every worker node that can run OSG jobs. It provides a common environment and a minimal set of common tools that all OSG jobs can expect to use. See the \nreference section\n below for contents of the Worker Node Client.\n\n\nIt is possible to install the Worker Node Client software in a variety of ways, depending on what works best for distributing and managing software at your site:\n\n\n\n\nInstall using RPMs and \nyum\n (this guide) - useful when managing your worker nodes with a tool (e.g., Puppet, Chef) that can automate RPM installs\n\n\nInstall using a tarball\n - useful when installing onto a shared filesystem for distribution to worker nodes\n\n\nUse from OASIS\n - useful when worker nodes already mount \nOASIS\n on your worker nodes\n\n\n\n\nThis document is intended to guide system administrators through the process of configuring a site to make the Worker Node Client software available from an RPM.\n\n\nBefore Starting\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to the host\n\n\nPrepare \nthe required Yum repositories\n\n\nInstall \nCA certificates\n\n\n\n\nInstall the Worker Node Client\n\n\nInstall the Worker Node Client RPM:\n\n\nyum install osg-wn-client\n\n\n\n\nServices\n\n\nFetch-CRL is the only service required to support the WN Client.\n\n\n\n\n\n\n\n\nSoftware\n\n\nService name\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nFetch CRL\n\n\nOn EL6 and EL7: \nfetch-crl-boot\n and \nfetch-crl-cron\n\n\n\n\n\n\n\n\nOn EL5: \nfetch-crl3-boot\n and \nfetch-crl3-cron\n\n\nSee \nCA documentation\n for more info\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nfetch-crl-boot\n will begin fetching CRLS, which can take a few minutes\nand fail on transient errors. You can add configuration to ignore these\ntransient errors in \n/etc/fetch-crl.conf\n for EL6 or EL7 machines or\n\n/etc/fetch-crl3.conf\n for EL5 machines:\n\n\nnoerrors\n\n\n\n\nAs a reminder, here are common service commands (all run as \nroot\n):\n\n\n\n\n\n\n\n\nTo \u2026\n\n\nRun the command \u2026\n\n\n\n\n\n\n\n\n\n\nStart a service\n\n\nservice SERVICE-NAME start\n\n\n\n\n\n\nStop a service\n\n\nservice SERVICE-NAME stop\n\n\n\n\n\n\nEnable a service to start during boot\n\n\nchkconfig SERVICE-NAME on\n\n\n\n\n\n\nDisable a service from starting during boot\n\n\nchkconfig SERVICE-NAME off\n\n\n\n\n\n\n\n\nValiding the Worker Node Client\n\n\nTo verify functionality of the worker node client, you will need to submit a test job against your CE and verify the job\u2019s output.\n\n\n\n\nSubmit a job that executes the \nenv\n command (e.g. Run \ncondor_ce_trace\n with the \n-d\n flag from your HTCondor CE)\n\n\nVerify that the value of \nOSG_GRID\n is set to \n/etc/osg/wn-client\n\n\n\n\nHow to get Help?\n\n\nTo get assistance please use this \nHelp Procedure\n.\n\n\nReference\n\n\nPlease see the documentation on using \nyum and RPM\n, \nthe best practices\n for using yum to install software, and using \nyum repositories\n.\n\n\nTo see the currently installed version of the worker node package, run the following command:\n\n\nrpm -q --requires osg-wn-client\n\n\n\n\nSee \nour yum basics guide\n for more details on using RPM to see what was installed.", 
            "title": "RPM-based Worker Node"
        }, 
        {
            "location": "/Other/wn/#installing-and-using-the-worker-node-client-from-rpms", 
            "text": "The  OSG Worker Node Client  is a collection of software components that is expected to be added to every worker node that can run OSG jobs. It provides a common environment and a minimal set of common tools that all OSG jobs can expect to use. See the  reference section  below for contents of the Worker Node Client.  It is possible to install the Worker Node Client software in a variety of ways, depending on what works best for distributing and managing software at your site:   Install using RPMs and  yum  (this guide) - useful when managing your worker nodes with a tool (e.g., Puppet, Chef) that can automate RPM installs  Install using a tarball  - useful when installing onto a shared filesystem for distribution to worker nodes  Use from OASIS  - useful when worker nodes already mount  OASIS  on your worker nodes   This document is intended to guide system administrators through the process of configuring a site to make the Worker Node Client software available from an RPM.", 
            "title": "Installing and Using the Worker Node Client From RPMs"
        }, 
        {
            "location": "/Other/wn/#before-starting", 
            "text": "As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system  Obtain root access to the host  Prepare  the required Yum repositories  Install  CA certificates", 
            "title": "Before Starting"
        }, 
        {
            "location": "/Other/wn/#install-the-worker-node-client", 
            "text": "Install the Worker Node Client RPM:  yum install osg-wn-client", 
            "title": "Install the Worker Node Client"
        }, 
        {
            "location": "/Other/wn/#services", 
            "text": "Fetch-CRL is the only service required to support the WN Client.     Software  Service name  Notes      Fetch CRL  On EL6 and EL7:  fetch-crl-boot  and  fetch-crl-cron     On EL5:  fetch-crl3-boot  and  fetch-crl3-cron  See  CA documentation  for more info       Note  fetch-crl-boot  will begin fetching CRLS, which can take a few minutes\nand fail on transient errors. You can add configuration to ignore these\ntransient errors in  /etc/fetch-crl.conf  for EL6 or EL7 machines or /etc/fetch-crl3.conf  for EL5 machines:  noerrors   As a reminder, here are common service commands (all run as  root ):     To \u2026  Run the command \u2026      Start a service  service SERVICE-NAME start    Stop a service  service SERVICE-NAME stop    Enable a service to start during boot  chkconfig SERVICE-NAME on    Disable a service from starting during boot  chkconfig SERVICE-NAME off", 
            "title": "Services"
        }, 
        {
            "location": "/Other/wn/#validing-the-worker-node-client", 
            "text": "To verify functionality of the worker node client, you will need to submit a test job against your CE and verify the job\u2019s output.   Submit a job that executes the  env  command (e.g. Run  condor_ce_trace  with the  -d  flag from your HTCondor CE)  Verify that the value of  OSG_GRID  is set to  /etc/osg/wn-client", 
            "title": "Validing the Worker Node Client"
        }, 
        {
            "location": "/Other/wn/#how-to-get-help", 
            "text": "To get assistance please use this  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/Other/wn/#reference", 
            "text": "Please see the documentation on using  yum and RPM ,  the best practices  for using yum to install software, and using  yum repositories .  To see the currently installed version of the worker node package, run the following command:  rpm -q --requires osg-wn-client  See  our yum basics guide  for more details on using RPM to see what was installed.", 
            "title": "Reference"
        }, 
        {
            "location": "/Other/glexec/", 
            "text": "Glexec Installation Guide\n\n\nThis document is intended for System Administrators that are installing the OSG version of glexec.\n\n\nGlexec is commonly used for what are referred to as \u201cpilot\u201d or \u201cglidein\u201d jobs.\n\n\nTraditionally, users submitted their jobs directly to a remote site (or compute element gatekeeper). The user job was authenticated/authorized to run at that site based on the user\u2019s proxy credentials and run under the local unix account assigned.\n\n\nIn a pilot-based infrastructure, users submit their jobs to a centralized site (or queue). The pilot/glidein software at the centralized site then recognizes there is a demand for computing resources. It will then submit what is called a pilot/glidein job to a remote site. This pilot job gets authenticated/authorized to run on a worker node in that site\u2019s cluster. It will then \u201cpull\u201d down user jobs from the centralized queue and execute them. Both the pilot and the user job are run under the pilot job\u2019s proxy certificate credentials and local unix account. This represents a security problem in pilot-based systems as there is no authentication/authorization of the individual user\u2019s proxy credentials and, thus, the user\u2019s jobs do not run using it\u2019s own local unix account.\n\n\nGlexec is a security tool that can be used to resolve this problem. It is meant to be used by VOs that run these pilot-based jobs. It has a number of authentication plugins and can be used both by European grid and by OSG.\n\n\nThe pilot job will \u201cpull\u201d user jobs down from the central queue and invoke glexec which will then\n1. authenticate the user job\u2019s proxy,\n2. perform an authorization callout (to GUMS in the case of OSG, or possibly a gridmapfile) similar to that done by the gatekeeper,\n3. and then run the user job under the local account assigned by the authorization service for that user.\n\n\nIn effect, glexec functions much the same as a compute element gatekeeper, except these functions are now performed on the individual worker node. The pilot jobs authentication/authorization is done by the gatekeeper and the individual user jobs are now done by glexec on the individual worker node.\n\n\nMany worker node clusters use shared file systems like NFS for much of their software and user home accounts. Since glexec is an suid program, it must be installed on every single worker node individually. Most shared file systems do not handle this correctly so it cannot and must not be NFS-exported.\n\n\nFor more information regarding pilot-based systems and glexec:\n1. \nglideinWMS - The glidein based WMS\n\n2. \nAddressing the pilot security problem with gLExec (pdf)\n\n\nEngineering Considerations\n\n\nThis section describes any prerequisite software/considerations that must be taken into account before the glexec software installation is performed. It should be reviewed completely before starting the installation process.\n\n\nA large number of batch slots using glexec can occasionally put an enormous strain on GUMS servers and cause overloading and client timeouts. In order to survive peak loads, the sysctl parameter \u2018net.core.somaxconn\u2019 on a GUMS server machine should be set at least as high as the maximum number of job slots that might attempt to contact the server at about the same time. (For example, Fermilab set the value to 4096 on each of two servers and tested with a continuous load from 5000 job slots). At the same time, the Apache parameter \u2018ListenBacklog\u2019 must be changed to the same value. Also note that Fermilab determined that for best performance, the Apache parameter \u2018MaxClients\u2019 on GUMS servers (at least on their dual-core Virtual Machines) should be set to a value of 32. For details on these and other parameters on the GUMS server see GumsScalability.\n\n\nRequirements\n\n\nThese are the requirements that must be met to install glexec.\n\n\n\n\nNote\n\n\nNormally you will install the \nOSG worker node\n first. Technically, installing the \nosg-wn-client-glexec\n package will also install the worker node, but we do not duplicate instructions specific to the worker node here, so refer to the \nOSG worker node\n for details about the worker node installation.\n\n\n\n\nHost and OS\n\n\n\n\nOS is Red Hat Enterprise Linux 5, 6, 7, and variants.\n\n\nRoot access\n\n\n\n\nUsers\n\n\nThe glexec installation will create two users unless they are already created.\n\n\n\n\n\n\n\n\nUser\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nglexec\n\n\nReduced privilege separate id used to improve security. Set the default gid of the \u201cglexec\u201d user to be a group that is also called \u201cglexec\u201d.\n\n\n\n\n\n\ngratia\n\n\nNeeded for the glexec gratia probe which is also automatically installed.\n\n\n\n\n\n\n\n\nIn addition, OSG glexec requires a range of \ngroup ids\n for tracking purposes. You don\u2018t actually have to create the group entries but it is recommended to do so in order to reserve the gids and so they can be associated with names in the \n/usr/bin/id\n command. The recommended names are \u2019glexecNN\u2019 where NN is a number starting from 00.\n\n\n\n\nDefine at least 4 group ids per batch slot per worker node. A conservative way to handle this is to multiply the number of batch slots on the largest worker node by 6 and then share the group ids between all the worker nodes.\n\n\nThey must be consecutive and in any range (default range is 65000-65049, configured in the \nConfiguring glexec\n section below).\n\n\nThe same group ids can be used on every worker node.\n\n\n\n\n\n\n\nInstall Instructions\n\n\nPrior to installing \nglexec\n, verify the \nyum repositories\n are correctly configured.\n\n\nSome of the worker node client software verifies proxies or certificates. In order to do this, they will need the \nCA certificates\n used to sign the proxies.\n\n\nInstall glexec\n\n\n\n\nNote\n\n\nThe glexec tracking function requires a part of HTCondor. There are multiple ways to install HTCondor, for details see \nthese instructions\n. If you want a minimal install, you can run just this command to install the needed piece from the OSG distribution:\n\n\nyum install condor-procd\n\n\n\n\nAfter meeting all the requirements in the previous section, install glexec with this command:\n\n\nyum install osg-wn-client-glexec\n\n\n\n\nConfiguring glexec\n\n\nThe following steps need to be done after the glexec installation is complete.\n\n\n\n\nFirst, review the contents of \n/etc/glexec.conf\n. All of the defaults should be fine, but if you want to change the behavior, the parameters are described in \nman glexec.conf\n.\n\n\n\n\nNext, review all of the contents of \n/etc/lcmaps.db\n and in particular update the following pieces.\n\n\n\n\n\n\nIf you have GUMS, change the yourgums.yourdomain in the following line to the fully qualified domain name of your GUMS server:\n\n\n\"\u2013endpoint \nhttps://yourgums.yourdomain:8443/gums/services/GUMSXACMLAuthorizationServicePort\n\"\n\n\n\n\n\n\nIf you want to use a range of tracking group ids other than the default as described in the \nRequirements\n section above, uncomment and change the \n-min-gid\n and \n-max-gid\n lines to your chosen values:\n\n\n\"-min-gid 65000\u201d \u201c-max-gid 65049\"\n\n\n\n\n\n\nUncomment the following two lines:\n\n\nglexectracking = \"lcmaps_glexec_tracking.mod\"\n                   \"-exec /usr/sbin/glexec_monitor\"\n\n\n\n\n\n\nIf you have GUMS, uncomment the following policy toward the end of the file:\n\n\nverifyproxy -\n gumsclient\n gumsclient -\n glexectracking\n\n\nor if you have do not have GUMS and want to use a gridmapfile, uncomment the following policy:\n\n\nverifyproxy -\n gridmapfile\n gridmapfile -\n glexectracking\n\n\n\n\n\n\nTesting the Installation of glexec\n\n\nNow, \nas a non-privileged user (not root)\n , do the following (where \nyourvo\n is your VO, and \nNNN\n is your uid as reported by \n/usr/bin/id\n):\n\n\nvoms-proxy-init -voms yourvo:/yourvo\nexport GLEXEC_CLIENT_CERT=/tmp/x509up_uNNN\n/usr/sbin/glexec /usr/bin/id\nuid=13160(fnalgrid) gid=9767(fnalgrid) groups=65000(glexec00)\n\n\n\n\nIf your \nlcmaps.db\n is set up to not use a host certificate as described in GlexecPilotCert, you should also set\n\n\n``\nexport X509_USER_PROXY=/tmp/x509up_uNNN\n\n\n\n(substitute `NNN` for your UID) before running glexec.\n\nIf `glexec` is successful, it will print out the uid and gid that your proxy would normally be mapped to by your GUMS server, plus a supplementary tracking group. (The actual names and numbers will be different from what you see above.)\n\nIf you have problems, please read about [troubleshooting glexec](TroubleshootingGlexecLcmaps).\n\nGlexec log files\n----------------\n\n`Glexec` sends all its log information by default to syslog. Where it goes from there depends on your syslog configuration, but by default they go to `/var/log/messages`. Here are some sample messages:\n\n\n\n\n\nApr 25 16:36:16 fermicloud053 glexec[2867]: lcmaps: lcmaps.mod-PluginInit(): plugin glexectracking not found (arguments: )\nApr 25 16:36:16 fermicloud053 glexec[2867]: lcmaps: lcmaps.mod-lcmaps_startPluginManager(): error initializing plugin: glexectracking\nApr 25 16:36:16 fermicloud053 glexec[2867]: lcmaps: lcmaps_init() error: could not start plugin manager\nApr 25 16:36:16 fermicloud053 glexec[2867]: Initialisation of LCMAPS failed.\n\n\n\nThese particular messages are pretty common, caused by forgetting to uncomment the beginning of the glexectracking rule in `/etc/lcmaps.db`.\n\nIt is possible to redirect glexec log messages to a different file with standard syslog. To do that, choose one of the `LOG_LOCAL[0-7]` log facilities that are unused, for example `LOG_LOCAL1`. Then set the following in `/etc/glexec.conf`:\n\n\n\n\n\nsyslog_facility = LOG_LOCAL1\n\n\n\nand add a corresponding parameter to the `lcmaps_glexec_tracking.mod` entry in `/etc/lcmaps.db`:\n\n\n\n\n\n\"-log-facility LOG_LOCAL1\"\n\n\n\nThen in `/etc/syslog.conf` on el5 or `/etc/rsyslog.conf` on el6 add a line like this\n\n\n\n\n\nlocal1.* /var/log/glexec.log\n\n\n\nand also exclude those messages from `/var/log/messages` by adding `local1.none` after other wildcards on the existing `/var/log/messages` line, for example:\n\n\n\n\n\n*.info;local1.none;mail.none;authpriv.none;cron.none /var/log/messages\n\n\n\nBe sure to notify the system logger to re-read the configuration file with `service syslog reload` on el5, or with `service rsyslog restart` on el6.\n\n`rsyslog`, by default, limits the rate at which messages may be logged, and if maximum debugging is enabled in glexec this limit is reached. To avoid that, you can add the following to `/etc/rsyslog.conf` after the line \n$ModLoad imuxsock.so\n:\n\n\n\n\n\n$SystemLogRateLimitInterval 0\n$SystemLogRateLimitBurst 0\n\n\n\nand of course do `service rsyslog restart`.\n\nAlternatively, `syslog-ng` (available in the EPEL repository) can do the same job by matching all the messages that have the string \nglexec\n in the name.\nThese rules in `/etc/syslog-ng/syslog-ng.conf` will separate the glexec messages into `/var/log/glexec.log`:\n\n\n\n\n\ndestination d_glexec { file(\"/var/log/glexec.log\"); };\nfilter f_glexec { program(\"^glexec\"); };\nfilter f_notglexec { not program(\"^glexec\"); };\nlog { source(s_sys); filter(f_glexec); destination(d_glexec); };\n\n\n\nThen later, in the log rule writing sending to `d_mesg`, add a `filter(f_notglexec);` before the destination rule to keep glexec messages out of `/var/log/messages`:\n\n\n\n\n\nlog { source(s_sys); filter(f_filter1); filter(f_notglexec); destination(d_mesg); };\n```\n\n\nHow to get Help?\n\n\nTo get assistance please use \nHelp Procedure\n.", 
            "title": "Worker node glexec"
        }, 
        {
            "location": "/Other/glexec/#glexec-installation-guide", 
            "text": "This document is intended for System Administrators that are installing the OSG version of glexec.  Glexec is commonly used for what are referred to as \u201cpilot\u201d or \u201cglidein\u201d jobs.  Traditionally, users submitted their jobs directly to a remote site (or compute element gatekeeper). The user job was authenticated/authorized to run at that site based on the user\u2019s proxy credentials and run under the local unix account assigned.  In a pilot-based infrastructure, users submit their jobs to a centralized site (or queue). The pilot/glidein software at the centralized site then recognizes there is a demand for computing resources. It will then submit what is called a pilot/glidein job to a remote site. This pilot job gets authenticated/authorized to run on a worker node in that site\u2019s cluster. It will then \u201cpull\u201d down user jobs from the centralized queue and execute them. Both the pilot and the user job are run under the pilot job\u2019s proxy certificate credentials and local unix account. This represents a security problem in pilot-based systems as there is no authentication/authorization of the individual user\u2019s proxy credentials and, thus, the user\u2019s jobs do not run using it\u2019s own local unix account.  Glexec is a security tool that can be used to resolve this problem. It is meant to be used by VOs that run these pilot-based jobs. It has a number of authentication plugins and can be used both by European grid and by OSG.  The pilot job will \u201cpull\u201d user jobs down from the central queue and invoke glexec which will then\n1. authenticate the user job\u2019s proxy,\n2. perform an authorization callout (to GUMS in the case of OSG, or possibly a gridmapfile) similar to that done by the gatekeeper,\n3. and then run the user job under the local account assigned by the authorization service for that user.  In effect, glexec functions much the same as a compute element gatekeeper, except these functions are now performed on the individual worker node. The pilot jobs authentication/authorization is done by the gatekeeper and the individual user jobs are now done by glexec on the individual worker node.  Many worker node clusters use shared file systems like NFS for much of their software and user home accounts. Since glexec is an suid program, it must be installed on every single worker node individually. Most shared file systems do not handle this correctly so it cannot and must not be NFS-exported.  For more information regarding pilot-based systems and glexec:\n1.  glideinWMS - The glidein based WMS \n2.  Addressing the pilot security problem with gLExec (pdf)", 
            "title": "Glexec Installation Guide"
        }, 
        {
            "location": "/Other/glexec/#engineering-considerations", 
            "text": "This section describes any prerequisite software/considerations that must be taken into account before the glexec software installation is performed. It should be reviewed completely before starting the installation process.  A large number of batch slots using glexec can occasionally put an enormous strain on GUMS servers and cause overloading and client timeouts. In order to survive peak loads, the sysctl parameter \u2018net.core.somaxconn\u2019 on a GUMS server machine should be set at least as high as the maximum number of job slots that might attempt to contact the server at about the same time. (For example, Fermilab set the value to 4096 on each of two servers and tested with a continuous load from 5000 job slots). At the same time, the Apache parameter \u2018ListenBacklog\u2019 must be changed to the same value. Also note that Fermilab determined that for best performance, the Apache parameter \u2018MaxClients\u2019 on GUMS servers (at least on their dual-core Virtual Machines) should be set to a value of 32. For details on these and other parameters on the GUMS server see GumsScalability.", 
            "title": "Engineering Considerations"
        }, 
        {
            "location": "/Other/glexec/#requirements", 
            "text": "These are the requirements that must be met to install glexec.   Note  Normally you will install the  OSG worker node  first. Technically, installing the  osg-wn-client-glexec  package will also install the worker node, but we do not duplicate instructions specific to the worker node here, so refer to the  OSG worker node  for details about the worker node installation.", 
            "title": "Requirements"
        }, 
        {
            "location": "/Other/glexec/#host-and-os", 
            "text": "OS is Red Hat Enterprise Linux 5, 6, 7, and variants.  Root access", 
            "title": "Host and OS"
        }, 
        {
            "location": "/Other/glexec/#users", 
            "text": "The glexec installation will create two users unless they are already created.     User  Comment      glexec  Reduced privilege separate id used to improve security. Set the default gid of the \u201cglexec\u201d user to be a group that is also called \u201cglexec\u201d.    gratia  Needed for the glexec gratia probe which is also automatically installed.     In addition, OSG glexec requires a range of  group ids  for tracking purposes. You don\u2018t actually have to create the group entries but it is recommended to do so in order to reserve the gids and so they can be associated with names in the  /usr/bin/id  command. The recommended names are \u2019glexecNN\u2019 where NN is a number starting from 00.   Define at least 4 group ids per batch slot per worker node. A conservative way to handle this is to multiply the number of batch slots on the largest worker node by 6 and then share the group ids between all the worker nodes.  They must be consecutive and in any range (default range is 65000-65049, configured in the  Configuring glexec  section below).  The same group ids can be used on every worker node.", 
            "title": "Users"
        }, 
        {
            "location": "/Other/glexec/#install-instructions", 
            "text": "Prior to installing  glexec , verify the  yum repositories  are correctly configured.  Some of the worker node client software verifies proxies or certificates. In order to do this, they will need the  CA certificates  used to sign the proxies.", 
            "title": "Install Instructions"
        }, 
        {
            "location": "/Other/glexec/#install-glexec", 
            "text": "Note  The glexec tracking function requires a part of HTCondor. There are multiple ways to install HTCondor, for details see  these instructions . If you want a minimal install, you can run just this command to install the needed piece from the OSG distribution:  yum install condor-procd   After meeting all the requirements in the previous section, install glexec with this command:  yum install osg-wn-client-glexec", 
            "title": "Install glexec"
        }, 
        {
            "location": "/Other/glexec/#configuring-glexec", 
            "text": "The following steps need to be done after the glexec installation is complete.   First, review the contents of  /etc/glexec.conf . All of the defaults should be fine, but if you want to change the behavior, the parameters are described in  man glexec.conf .   Next, review all of the contents of  /etc/lcmaps.db  and in particular update the following pieces.    If you have GUMS, change the yourgums.yourdomain in the following line to the fully qualified domain name of your GUMS server:  \"\u2013endpoint  https://yourgums.yourdomain:8443/gums/services/GUMSXACMLAuthorizationServicePort \"    If you want to use a range of tracking group ids other than the default as described in the  Requirements  section above, uncomment and change the  -min-gid  and  -max-gid  lines to your chosen values:  \"-min-gid 65000\u201d \u201c-max-gid 65049\"    Uncomment the following two lines:  glexectracking = \"lcmaps_glexec_tracking.mod\"\n                   \"-exec /usr/sbin/glexec_monitor\"    If you have GUMS, uncomment the following policy toward the end of the file:  verifyproxy -  gumsclient\n gumsclient -  glexectracking  or if you have do not have GUMS and want to use a gridmapfile, uncomment the following policy:  verifyproxy -  gridmapfile\n gridmapfile -  glexectracking", 
            "title": "Configuring glexec"
        }, 
        {
            "location": "/Other/glexec/#testing-the-installation-of-glexec", 
            "text": "Now,  as a non-privileged user (not root)  , do the following (where  yourvo  is your VO, and  NNN  is your uid as reported by  /usr/bin/id ):  voms-proxy-init -voms yourvo:/yourvo\nexport GLEXEC_CLIENT_CERT=/tmp/x509up_uNNN\n/usr/sbin/glexec /usr/bin/id\nuid=13160(fnalgrid) gid=9767(fnalgrid) groups=65000(glexec00)  If your  lcmaps.db  is set up to not use a host certificate as described in GlexecPilotCert, you should also set  ``\nexport X509_USER_PROXY=/tmp/x509up_uNNN  \n(substitute `NNN` for your UID) before running glexec.\n\nIf `glexec` is successful, it will print out the uid and gid that your proxy would normally be mapped to by your GUMS server, plus a supplementary tracking group. (The actual names and numbers will be different from what you see above.)\n\nIf you have problems, please read about [troubleshooting glexec](TroubleshootingGlexecLcmaps).\n\nGlexec log files\n----------------\n\n`Glexec` sends all its log information by default to syslog. Where it goes from there depends on your syslog configuration, but by default they go to `/var/log/messages`. Here are some sample messages:  Apr 25 16:36:16 fermicloud053 glexec[2867]: lcmaps: lcmaps.mod-PluginInit(): plugin glexectracking not found (arguments: )\nApr 25 16:36:16 fermicloud053 glexec[2867]: lcmaps: lcmaps.mod-lcmaps_startPluginManager(): error initializing plugin: glexectracking\nApr 25 16:36:16 fermicloud053 glexec[2867]: lcmaps: lcmaps_init() error: could not start plugin manager\nApr 25 16:36:16 fermicloud053 glexec[2867]: Initialisation of LCMAPS failed.  \nThese particular messages are pretty common, caused by forgetting to uncomment the beginning of the glexectracking rule in `/etc/lcmaps.db`.\n\nIt is possible to redirect glexec log messages to a different file with standard syslog. To do that, choose one of the `LOG_LOCAL[0-7]` log facilities that are unused, for example `LOG_LOCAL1`. Then set the following in `/etc/glexec.conf`:  syslog_facility = LOG_LOCAL1  \nand add a corresponding parameter to the `lcmaps_glexec_tracking.mod` entry in `/etc/lcmaps.db`:  \"-log-facility LOG_LOCAL1\"  \nThen in `/etc/syslog.conf` on el5 or `/etc/rsyslog.conf` on el6 add a line like this  local1.* /var/log/glexec.log  \nand also exclude those messages from `/var/log/messages` by adding `local1.none` after other wildcards on the existing `/var/log/messages` line, for example:  *.info;local1.none;mail.none;authpriv.none;cron.none /var/log/messages  \nBe sure to notify the system logger to re-read the configuration file with `service syslog reload` on el5, or with `service rsyslog restart` on el6.\n\n`rsyslog`, by default, limits the rate at which messages may be logged, and if maximum debugging is enabled in glexec this limit is reached. To avoid that, you can add the following to `/etc/rsyslog.conf` after the line  $ModLoad imuxsock.so :  $SystemLogRateLimitInterval 0\n$SystemLogRateLimitBurst 0  \nand of course do `service rsyslog restart`.\n\nAlternatively, `syslog-ng` (available in the EPEL repository) can do the same job by matching all the messages that have the string  glexec  in the name.\nThese rules in `/etc/syslog-ng/syslog-ng.conf` will separate the glexec messages into `/var/log/glexec.log`:  destination d_glexec { file(\"/var/log/glexec.log\"); };\nfilter f_glexec { program(\"^glexec\"); };\nfilter f_notglexec { not program(\"^glexec\"); };\nlog { source(s_sys); filter(f_glexec); destination(d_glexec); };  \nThen later, in the log rule writing sending to `d_mesg`, add a `filter(f_notglexec);` before the destination rule to keep glexec messages out of `/var/log/messages`:  log { source(s_sys); filter(f_filter1); filter(f_notglexec); destination(d_mesg); };\n```", 
            "title": "Testing the Installation of glexec"
        }, 
        {
            "location": "/Other/glexec/#how-to-get-help", 
            "text": "To get assistance please use  Help Procedure .", 
            "title": "How to get Help?"
        }
    ]
}